{"file_contents":{"app.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom database.db_manager import DatabaseManager\n\n# Configuração da página\nst.set_page_config(\n    page_title=\"Insight Hub - Monitoramento de Frotas\",\n    page_icon=\"🚛\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# CSS customizado para melhor aparência\nst.markdown(\"\"\"\n    <style>\n    .main-header {\n        font-size: 2.5rem;\n        font-weight: bold;\n        color: #1f77b4;\n        text-align: center;\n        margin-bottom: 2rem;\n    }\n    .metric-card {\n        background-color: #f0f2f6;\n        padding: 1rem;\n        border-radius: 0.5rem;\n        margin: 0.5rem 0;\n    }\n    </style>\n\"\"\", unsafe_allow_html=True)\n\ndef load_processed_data():\n    \"\"\"Carrega dados da base PostgreSQL\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty:\n            st.success(f\"✅ Dados carregados: {len(df):,} registros da base PostgreSQL\")\n            return df\n        return pd.DataFrame()\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados da base: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    # Header principal\n    st.markdown('<h1 class=\"main-header\">🚛 Insight Hub</h1>', unsafe_allow_html=True)\n    st.markdown('<p style=\"text-align: center; font-size: 1.2rem; color: #666;\">Plataforma de Monitoramento e Análise de Frotas Municipais</p>', unsafe_allow_html=True)\n    \n    # Verificar se há dados processados\n    df = load_processed_data()\n    \n    if df.empty:\n        st.warning(\"📁 Nenhum dado encontrado. Faça o upload de um arquivo CSV na página 'Upload CSV' para começar.\")\n        \n        # Informações sobre o sistema\n        st.markdown(\"---\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"\"\"\n            ### 🎯 **Funcionalidades Principais**\n            \n            - **📊 Análise Profissional**: KPIs em tempo real  \n            - **📁 Upload de CSV**: Processamento de dados telemáticos\n            - **🔍 Análise Detalhada**: Filtros avançados por cliente e período\n            - **📈 Comparação de Veículos**: Análise comparativa de performance\n            - **🔧 Manutenção Preditiva**: Alertas e previsões\n            - **🚨 Alertas**: Monitoramento em tempo real\n            - **🧠 Insights Automáticos**: Geração inteligente de relatórios\n            - **📄 Relatórios Avançados**: Documentação completa\n            - **🚨 Controle Operacional**: Conformidade municipal\n            \"\"\")\n        \n        with col2:\n            st.markdown(\"\"\"\n            ### 📋 **Campos Obrigatórios do CSV**\n            \n            - Cliente, Placa, Ativo, Data, Data (GPRS)\n            - Velocidade (Km/h), Ignição, Motorista\n            - GPS, GPRS, Localização, Endereço\n            - Tipo do Evento, Cerca, Saída, Entrada\n            - Pacote, Odômetro do período (Km)\n            - Horímetro do período, Horímetro embarcado\n            - Odômetro embarcado (Km), Bateria (%)\n            - Imagem, Tensão (V), Bloqueado\n            - Latitude, Longitude (coordenadas GPS)\n            \"\"\")\n        \n        st.markdown(\"---\")\n        st.info(\"💡 **Dica**: Comece fazendo o upload de um arquivo CSV com dados de frota na aba lateral.\")\n        \n    else:\n        # Mostrar resumo dos dados carregados\n        st.success(f\"✅ Dados carregados: {len(df):,} registros processados\")\n        \n        # Métricas gerais\n        st.markdown(\"### 📈 **Visão Geral dos Dados**\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            total_vehicles = df['placa'].nunique()\n            st.metric(\"🚗 Total de Veículos\", f\"{total_vehicles:,}\")\n        \n        with col2:\n            total_clients = df['cliente'].nunique()\n            st.metric(\"🏢 Total de Clientes\", f\"{total_clients:,}\")\n        \n        with col3:\n            date_range = df['data'].max() - df['data'].min()\n            st.metric(\"📅 Período dos Dados\", f\"{date_range.days} dias\")\n        \n        with col4:\n            total_records = len(df)\n            st.metric(\"📊 Total de Registros\", f\"{total_records:,}\")\n        \n        # Gráfico de distribuição temporal\n        st.markdown(\"### 📊 **Distribuição Temporal dos Dados**\")\n        \n        # Agrupar por data\n        daily_data = df.groupby(df['data'].dt.date).size().reset_index()\n        daily_data.columns = ['data', 'registros']\n        \n        fig = px.line(\n            daily_data, \n            x='data', \n            y='registros',\n            title='Registros por Data',\n            labels={'data': 'Data', 'registros': 'Número de Registros'}\n        )\n        fig.update_layout(height=400)\n        st.plotly_chart(fig, use_container_width=True)\n        \n        # Top veículos por atividade\n        st.markdown(\"### 🚗 **Veículos Mais Ativos**\")\n        \n        vehicle_activity = df.groupby('placa').size().sort_values(ascending=False).head(10)\n        \n        fig_bar = px.bar(\n            x=vehicle_activity.values,\n            y=vehicle_activity.index,\n            orientation='h',\n            title='Top 10 Veículos por Número de Registros',\n            labels={'x': 'Número de Registros', 'y': 'Placa'}\n        )\n        fig_bar.update_layout(height=400)\n        st.plotly_chart(fig_bar, use_container_width=True)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":5578},"debug_csv.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nDebug script to analyze the CSV file and column mapping issues\n\"\"\"\nimport pandas as pd\nimport sys\nsys.path.append('.')\n\ndef main():\n    print(\"🔍 Debugging CSV column mapping and data issues...\")\n    \n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    # Read the CSV with different separators\n    print(\"\\n📄 Testing CSV reading with different separators...\")\n    \n    separators = [',', ';']\n    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n    \n    df = None\n    used_sep = None\n    used_enc = None\n    \n    for sep in separators:\n        for enc in encodings:\n            try:\n                df = pd.read_csv(csv_file, sep=sep, encoding=enc)\n                used_sep = sep\n                used_enc = enc\n                print(f\"✅ Successfully read with separator='{sep}' and encoding='{enc}'\")\n                break\n            except Exception as e:\n                print(f\"❌ Failed with separator='{sep}' and encoding='{enc}': {str(e)[:100]}\")\n                continue\n        if df is not None:\n            break\n    \n    if df is None:\n        print(\"❌ Could not read CSV file with any combination\")\n        return\n    \n    print(f\"\\n📊 File info:\")\n    print(f\"Shape: {df.shape}\")\n    print(f\"Used separator: '{used_sep}'\")\n    print(f\"Used encoding: '{used_enc}'\")\n    \n    print(f\"\\n📋 Original columns:\")\n    for i, col in enumerate(df.columns):\n        print(f\"{i:2d}: '{col}' (len={len(col)})\")\n    \n    # Test normalization\n    print(f\"\\n🔧 Testing column normalization:\")\n    normalized_cols = [' '.join(col.strip().split()) for col in df.columns]\n    for i, (orig, norm) in enumerate(zip(df.columns, normalized_cols)):\n        if orig != norm:\n            print(f\"{i:2d}: '{orig}' -> '{norm}'\")\n    \n    # Test first few rows of key columns\n    print(f\"\\n📝 Sample data (first 3 rows):\")\n    key_cols = ['Cliente', 'Placa', 'Data']\n    for col in key_cols:\n        if col in df.columns:\n            print(f\"{col}: {df[col].head(3).tolist()}\")\n        else:\n            print(f\"❌ Column '{col}' not found\")\n    \n    # Check unique values in key columns\n    print(f\"\\n🔢 Unique value counts:\")\n    if 'Cliente' in df.columns:\n        print(f\"Unique clients: {df['Cliente'].nunique()}\")\n        print(f\"Client values: {df['Cliente'].unique()[:5]}\")\n    \n    if 'Placa' in df.columns:\n        print(f\"Unique plates: {df['Placa'].nunique()}\")\n        print(f\"Plate values: {df['Placa'].unique()[:5]}\")\n    \n    # Test date parsing\n    print(f\"\\n📅 Testing date parsing:\")\n    if 'Data' in df.columns:\n        date_sample = df['Data'].head(3)\n        print(f\"Original dates: {date_sample.tolist()}\")\n        \n        # Test without dayfirst\n        parsed_normal = pd.to_datetime(date_sample, errors='coerce')\n        print(f\"Parsed (normal): {parsed_normal.tolist()}\")\n        \n        # Test with dayfirst\n        parsed_dayfirst = pd.to_datetime(date_sample, errors='coerce', dayfirst=True)\n        print(f\"Parsed (dayfirst): {parsed_dayfirst.tolist()}\")\n        \n        # Check for NaT values\n        nat_count = parsed_dayfirst.isna().sum()\n        print(f\"NaT values with dayfirst=True: {nat_count}\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":3303},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"fpdf2>=2.8.4\",\n    \"numpy>=2.3.3\",\n    \"pandas>=2.3.2\",\n    \"plotly>=6.3.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"reportlab>=4.4.3\",\n    \"scikit-learn>=1.7.2\",\n    \"sqlalchemy>=2.0.43\",\n    \"streamlit-folium>=0.25.1\",\n    \"streamlit>=1.49.1\",\n    \"folium>=0.20.0\",\n]\n","size_bytes":413},"replit.md":{"content":"# Insight Hub - Fleet Monitoring System\n\n## Overview\n\nInsight Hub is a comprehensive fleet monitoring and analysis platform for municipal vehicles that processes real-time telematics data to provide intelligent insights about vehicle performance, compliance, and operational efficiency. The system processes CSV data uploads containing vehicle telemetry information and generates interactive dashboards, automated insights, and comparative analytics for fleet management.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Framework**: Streamlit-based multi-page web application\n- **Structure**: Page-based navigation with modular components\n- **Pages**: Dashboard, CSV Upload, Detailed Analysis, Vehicle Comparison, Automatic Insights\n- **Visualization**: Plotly for interactive charts and graphs\n- **Styling**: Custom CSS with responsive design\n\n### Data Processing Pipeline\n- **CSV Processor**: Handles upload and validation of telematics data files\n- **Field Validation**: 25 required fields including vehicle data, GPS coordinates, driver info, and operational metrics\n- **Data Storage**: Local CSV file storage in `/data/processed_data.csv`\n- **Analysis Engine**: Real-time data analysis with filtering capabilities\n\n### Core Components\n- **DataAnalyzer**: Central class for data filtering, KPI calculations, and statistical analysis\n- **InsightsGenerator**: Automated insight generation for performance, compliance, efficiency, and predictive analytics\n- **FleetVisualizations**: Chart and graph generation using Plotly\n- **CSVProcessor**: File upload validation and data structure verification\n\n### Data Model\n- **Vehicle Data**: License plate, asset ID, client information\n- **Telemetry**: GPS coordinates, speed, ignition status, odometer readings\n- **Operational**: Driver information, event types, battery levels, system status\n- **Temporal**: Date/time stamps for GPS and GPRS communications\n\n### Analytics Features\n- **KPI Dashboard**: Real-time metrics for fleet performance\n- **Filtering System**: Multi-dimensional filtering by client, vehicle, date range\n- **Comparative Analysis**: Vehicle-to-vehicle performance comparison\n- **Automated Insights**: Pattern recognition and recommendation generation\n- **Compliance Monitoring**: Speed limits, operational hours, GPS coverage analysis\n\n## External Dependencies\n\n### Core Libraries\n- **streamlit**: Web application framework\n- **pandas**: Data manipulation and analysis\n- **plotly**: Interactive visualization library\n- **numpy**: Numerical computing support\n\n### Data Processing\n- **datetime**: Date and time handling for temporal analysis\n- **os**: File system operations for data storage\n- **re**: Regular expressions for data validation\n\n### File Storage\n- **Local CSV Storage**: Processed data stored in local filesystem\n- **Upload Directory**: Temporary file handling for CSV uploads\n- **Data Persistence**: File-based data storage without external database\n\nNote: The system currently uses local file storage but could be extended to integrate with PostgreSQL or other database systems for enhanced data persistence and multi-user support.","size_bytes":3197},"step_by_step_migration.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nStep-by-step migration test to verify each component works\n\"\"\"\nimport pandas as pd\nimport sys\nsys.path.append('.')\n\nfrom database.db_manager import DatabaseManager\n\ndef test_step_by_step():\n    print(\"🔧 Step-by-step migration test...\")\n    \n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    # Step 1: Test CSV reading\n    print(\"\\n📁 Step 1: Testing CSV reading...\")\n    try:\n        df = None\n        separators = [',', ';']\n        encodings = ['latin-1', 'utf-8', 'iso-8859-1', 'windows-1252', 'cp1252']\n        \n        for sep in separators:\n            for enc in encodings:\n                try:\n                    test_df = pd.read_csv(csv_file, sep=sep, encoding=enc)\n                    if test_df.shape[1] > 1:  # Multiple columns\n                        df = test_df\n                        print(f\"✅ CSV read successfully: sep='{sep}', enc='{enc}', shape={df.shape}\")\n                        break\n                except:\n                    continue\n            if df is not None:\n                break\n        \n        if df is None:\n            print(\"❌ Could not read CSV\")\n            return\n            \n    except Exception as e:\n        print(f\"❌ CSV reading failed: {str(e)}\")\n        return\n    \n    # Step 2: Test column normalization\n    print(f\"\\n🔧 Step 2: Testing column normalization...\")\n    try:\n        print(f\"Original columns: {list(df.columns[:5])}\")\n        df.columns = [' '.join(col.strip().split()) for col in df.columns]\n        print(f\"Normalized columns: {list(df.columns[:5])}\")\n        print(f\"✅ Column normalization successful\")\n    except Exception as e:\n        print(f\"❌ Column normalization failed: {str(e)}\")\n        return\n    \n    # Step 3: Test small sample migration\n    print(f\"\\n🧪 Step 3: Testing small sample migration (first 5 rows)...\")\n    try:\n        small_df = df.head(5).copy()\n        result = DatabaseManager.migrate_csv_to_database_from_df(small_df, \"test_sample.csv\")\n        print(f\"Sample migration result: {result}\")\n        \n        if result.get('success'):\n            print(f\"✅ Sample migration successful: {result.get('records_processed', 0)} records processed\")\n        else:\n            print(f\"❌ Sample migration failed: {result.get('error', 'Unknown error')}\")\n            return\n    except Exception as e:\n        print(f\"❌ Sample migration failed: {str(e)}\")\n        return\n    \n    # Step 4: Test full migration if sample worked\n    print(f\"\\n🚀 Step 4: Testing full migration ({df.shape[0]} rows)...\")\n    try:\n        result = DatabaseManager.migrate_csv_to_database_from_df(df, \"full_migration.csv\")\n        print(f\"Full migration result: {result}\")\n        \n        if result.get('success'):\n            print(f\"✅ Full migration successful!\")\n            print(f\"   Records processed: {result.get('records_processed', 0):,}\")\n            print(f\"   Unique vehicles: {result.get('unique_vehicles', 0):,}\")\n            print(f\"   Unique clients: {result.get('unique_clients', 0):,}\")\n        else:\n            print(f\"❌ Full migration failed: {result.get('error', 'Unknown error')}\")\n    except Exception as e:\n        print(f\"❌ Full migration failed: {str(e)}\")\n\nif __name__ == \"__main__\":\n    test_step_by_step()","size_bytes":3328},"test_csv_reading.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nTest CSV reading with proper separator detection\n\"\"\"\nimport pandas as pd\n\ndef test_csv_reading():\n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    print(\"🧪 Testing CSV reading with proper separator detection...\")\n    \n    separators = [',', ';']\n    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n    \n    for sep in separators:\n        for enc in encodings:\n            try:\n                df = pd.read_csv(csv_file, sep=sep, encoding=enc)\n                print(f\"\\n✅ Separator='{sep}', Encoding='{enc}':\")\n                print(f\"   Shape: {df.shape}\")\n                print(f\"   Columns: {len(df.columns)}\")\n                if df.shape[1] > 1:\n                    print(f\"   First 3 column names: {list(df.columns[:3])}\")\n                    print(f\"   Sample row 0: {dict(list(df.iloc[0].items())[:3])}\")\n                    return df, sep, enc\n                else:\n                    print(f\"   ⚠️  Only {df.shape[1]} column - wrong separator\")\n            except Exception as e:\n                print(f\"❌ Separator='{sep}', Encoding='{enc}': {str(e)[:80]}\")\n    \n    return None, None, None\n\nif __name__ == \"__main__\":\n    df, sep, enc = test_csv_reading()\n    if df is not None:\n        print(f\"\\n🎉 Successfully found correct separator: '{sep}' with encoding: '{enc}'\")\n    else:\n        print(f\"\\n❌ No valid combination found\")","size_bytes":1476},"test_migration.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nTest script to verify the database migration fixes\n\"\"\"\nimport os\nimport sys\nfrom datetime import datetime\n\n# Add current directory to path\nsys.path.append('.')\n\nfrom database.db_manager import DatabaseManager\nfrom database.services import FleetDatabaseService\n\ndef main():\n    print(\"🧪 Testing database migration with user's CSV file...\")\n    print(f\"Timestamp: {datetime.now()}\")\n    \n    # Test file path\n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    if not os.path.exists(csv_file):\n        print(f\"❌ CSV file not found: {csv_file}\")\n        return\n    \n    print(f\"📁 Found CSV file: {csv_file}\")\n    \n    # Check file size\n    file_size = os.path.getsize(csv_file)\n    print(f\"📏 File size: {file_size:,} bytes\")\n    \n    # Run migration\n    print(\"\\n🚀 Starting migration...\")\n    result = DatabaseManager.migrate_csv_to_database(csv_file)\n    \n    print(f\"\\n📊 Migration Result:\")\n    print(f\"Success: {result.get('success', False)}\")\n    if result.get('success'):\n        print(f\"Records processed: {result.get('records_processed', 0):,}\")\n        print(f\"Unique vehicles: {result.get('unique_vehicles', 0):,}\")\n        print(f\"Unique clients: {result.get('unique_clients', 0):,}\")\n    else:\n        print(f\"Error: {result.get('error', 'Unknown error')}\")\n    \n    # Verify database state\n    print(\"\\n🔍 Verifying database state...\")\n    try:\n        with FleetDatabaseService() as db:\n            summary = db.get_fleet_summary()\n            print(f\"Total clients: {summary['total_clients']:,}\")\n            print(f\"Total vehicles: {summary['total_vehicles']:,}\")\n            print(f\"Total telematics records: {summary['total_records']:,}\")\n            print(f\"Date range: {summary['start_date']} to {summary['end_date']}\")\n            print(f\"Average speed: {summary['avg_speed']} km/h\")\n            print(f\"GPS coverage: {summary['gps_coverage']}%\")\n    except Exception as e:\n        print(f\"❌ Error getting database summary: {str(e)}\")\n    \n    print(\"\\n✅ Migration test complete!\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":2153},"verify_dashboard_data.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nVerify dashboard components work with migrated data\n\"\"\"\nimport sys\nsys.path.append('.')\n\nfrom database.db_manager import DatabaseManager\nfrom database.services import FleetDatabaseService\nfrom utils.data_analyzer import DataAnalyzer\n\ndef verify_dashboard_data():\n    print(\"🔍 Verifying dashboard data components...\")\n    \n    # Test 1: Check DatabaseManager.get_dashboard_data()\n    print(\"\\n📊 Test 1: DatabaseManager.get_dashboard_data()\")\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        print(f\"✅ Dashboard data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n        \n        if df.shape[0] > 0:\n            print(f\"   Date range: {df['data'].min()} to {df['data'].max()}\")\n            print(f\"   Unique clients: {df['cliente'].nunique()}\")\n            print(f\"   Unique vehicles: {df['placa'].nunique()}\")\n            print(f\"   Sample data: {df[['cliente', 'placa', 'velocidade_km']].head(2).to_dict('records')}\")\n        \n    except Exception as e:\n        print(f\"❌ Dashboard data loading failed: {str(e)}\")\n        return False\n    \n    # Test 2: Check if DataAnalyzer can load from database\n    print(\"\\n🧮 Test 2: DataAnalyzer.from_database()\")\n    try:\n        analyzer = DataAnalyzer.from_database()\n        if analyzer.data is not None and not analyzer.data.empty:\n            print(f\"✅ DataAnalyzer loaded: {analyzer.data.shape[0]:,} rows\")\n            \n            # Test KPIs calculation\n            kpis = analyzer.get_kpis()\n            print(f\"   KPIs: {kpis['total_veiculos']} vehicles, {kpis['velocidade_media']:.1f} avg speed\")\n        else:\n            print(f\"❌ DataAnalyzer data is empty\")\n            return False\n            \n    except Exception as e:\n        print(f\"❌ DataAnalyzer loading failed: {str(e)}\")\n        return False\n    \n    # Test 3: Check FleetDatabaseService summary\n    print(\"\\n📈 Test 3: FleetDatabaseService.get_fleet_summary()\")\n    try:\n        with FleetDatabaseService() as db:\n            summary = db.get_fleet_summary()\n            print(f\"✅ Fleet summary: {summary['total_records']:,} records\")\n            print(f\"   Vehicles: {summary['total_vehicles']}, Clients: {summary['total_clients']}\")\n            print(f\"   Speed stats: {summary['avg_speed']} avg, {summary['max_speed']} max\")\n            print(f\"   GPS coverage: {summary['gps_coverage']}%\")\n            \n    except Exception as e:\n        print(f\"❌ Fleet summary failed: {str(e)}\")\n        return False\n    \n    # Test 4: Test data filtering\n    print(\"\\n🔍 Test 4: Data filtering functionality\")\n    try:\n        # Test with client filter\n        filtered_df = DatabaseManager.get_dashboard_data(client_filter=\"JANDAIA\")\n        print(f\"✅ Client filter works: {filtered_df.shape[0]:,} records for JANDAIA\")\n        \n        # Test with vehicle filter\n        filtered_df = DatabaseManager.get_dashboard_data(vehicle_filter=\"TFE-6D41\")\n        print(f\"✅ Vehicle filter works: {filtered_df.shape[0]:,} records for TFE-6D41\")\n        \n    except Exception as e:\n        print(f\"❌ Data filtering failed: {str(e)}\")\n        return False\n    \n    print(\"\\n🎉 Dashboard verification complete - All components working correctly!\")\n    return True\n\nif __name__ == \"__main__\":\n    success = verify_dashboard_data()\n    if success:\n        print(\"\\n✅ DASHBOARD READY: All migrated data is accessible and dashboard components work correctly\")\n    else:\n        print(\"\\n❌ DASHBOARD ISSUES: Some components need attention\")","size_bytes":3522},"database/__init__.py":{"content":"# Database package initialization","size_bytes":33},"database/connection.py":{"content":"\"\"\"\nDatabase connection and session management\n\"\"\"\nimport os\nimport time\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.exc import DisconnectionError, OperationalError\n\n# Database connection URL from environment\nDATABASE_URL = os.getenv('DATABASE_URL')\n\n# Global variables for lazy initialization\nengine = None\nSessionLocal = None\nBase = declarative_base()\n\ndef initialize_database():\n    \"\"\"Initialize database connection lazily with robust SSL handling\"\"\"\n    global engine, SessionLocal\n    \n    if engine is not None:\n        try:\n            # Test existing connection\n            with engine.connect() as conn:\n                conn.execute(text(\"SELECT 1\"))\n            return True\n        except (DisconnectionError, OperationalError):\n            # Force recreation of engine\n            engine = None\n    \n    if not DATABASE_URL:\n        return False\n    \n    try:\n        # Create engine with robust SSL and pool settings\n        engine = create_engine(\n            DATABASE_URL,\n            pool_size=10,\n            max_overflow=20,\n            pool_pre_ping=True,  # Validates connections before use\n            pool_recycle=300,    # Recycle connections every 5 minutes\n            connect_args={\n                \"sslmode\": \"require\",\n                \"connect_timeout\": 10,\n                \"application_name\": \"insight_hub_fleet_monitor\"\n            }\n        )\n        \n        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n        \n        # Create tables if they don't exist\n        Base.metadata.create_all(bind=engine)\n        return True\n    except Exception:\n        return False\n\ndef get_db_session():\n    \"\"\"Get database session with retry logic\"\"\"\n    max_retries = 3\n    retry_delay = 1\n    \n    for attempt in range(max_retries):\n        session = None\n        try:\n            # Ensure database is initialized\n            if not initialize_database():\n                raise Exception(\"Failed to initialize database\")\n            \n            session = SessionLocal()\n            \n            # Test the connection\n            session.execute(text(\"SELECT 1\"))\n            return session\n            \n        except (DisconnectionError, OperationalError) as e:\n            if session:\n                session.close()\n            \n            if attempt < max_retries - 1:\n                time.sleep(retry_delay)\n                retry_delay *= 2  # Exponential backoff\n                \n                # Force engine recreation on connection errors\n                global engine\n                engine = None\n                continue\n            else:\n                raise e\n        except Exception as e:\n            if session:\n                session.close()\n            raise e\n\ndef close_db_session(session):\n    \"\"\"Close database session safely\"\"\"\n    if session:\n        try:\n            session.close()\n        except Exception:\n            pass  # Ignore errors when closing\n\ndef test_connection():\n    \"\"\"Test database connection health\"\"\"\n    try:\n        with get_db_session() as session:\n            session.execute(text(\"SELECT 1\"))\n        return True\n    except Exception:\n        return False","size_bytes":3274},"database/db_manager.py":{"content":"\"\"\"\nDatabase manager for integrating with existing CSV processing workflow\n\"\"\"\nimport os\nimport pandas as pd\nfrom typing import Optional, Dict, Any, List\nfrom datetime import datetime\nfrom database.services import FleetDatabaseService\nfrom database.connection import initialize_database\n\nclass DatabaseManager:\n    \"\"\"Manager to integrate database operations with existing workflow\"\"\"\n    \n    @staticmethod\n    def _safe_int_convert(value):\n        \"\"\"Safely convert value to int, handling special cases\"\"\"\n        if pd.isna(value) or value is None or str(value).strip() == '':\n            return 0\n        try:\n            # Handle special values like 'X, X, X, X'\n            val_str = str(value).strip()\n            if 'x' in val_str.lower() or ',' in val_str:\n                return 0\n            return int(float(val_str))  # float first to handle '1.0'\n        except (ValueError, TypeError):\n            return 0\n    \n    @staticmethod\n    def _safe_float_convert(value):\n        \"\"\"Safely convert value to float, handling special cases\"\"\"\n        if pd.isna(value) or value is None or str(value).strip() == '':\n            return 0.0\n        try:\n            # Handle special values like 'X, X, X, X'\n            val_str = str(value).strip()\n            if 'x' in val_str.lower() or val_str == '-':\n                return 0.0\n            # Remove any non-numeric characters except . and -\n            val_str = ''.join(c for c in val_str if c.isdigit() or c in '.-')\n            if not val_str or val_str == '.' or val_str == '-':\n                return 0.0\n            return float(val_str)\n        except (ValueError, TypeError):\n            return 0.0\n    \n    @staticmethod\n    def migrate_csv_to_database_with_progress(csv_file_path: str, progress_callback=None) -> Dict[str, Any]:\n        \"\"\"Migrate existing CSV data to database with progress callback\"\"\"\n        if not os.path.exists(csv_file_path):\n            return {'success': False, 'error': f'File not found: {csv_file_path}'}\n        \n        try:\n            # Try different separators and encodings with proper validation\n            df = None\n            separators = [',', ';']\n            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for sep in separators:\n                for enc in encodings:\n                    try:\n                        test_df = pd.read_csv(csv_file_path, sep=sep, encoding=enc)\n                        # Validate that we have multiple columns (not just one big column)\n                        if test_df.shape[1] > 1:\n                            df = test_df\n                            print(f\"Successfully read CSV with separator='{sep}' and encoding='{enc}' - Shape: {df.shape}\")\n                            break\n                        else:\n                            print(f\"Separator '{sep}' with encoding '{enc}' resulted in only {test_df.shape[1]} column(s) - trying next combination\")\n                    except Exception as e:\n                        print(f\"Failed with separator='{sep}' and encoding='{enc}': {str(e)[:100]}\")\n                        continue\n                if df is not None:\n                    break\n            \n            if df is None:\n                return {'success': False, 'error': 'Could not read CSV file with any encoding/separator combination that produces multiple columns'}\n            \n            return DatabaseManager.migrate_csv_to_database_from_df_with_progress(df, os.path.basename(csv_file_path), progress_callback)\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def migrate_csv_to_database(csv_file_path: str) -> Dict[str, Any]:\n        \"\"\"Migrate existing CSV data to database\"\"\"\n        if not os.path.exists(csv_file_path):\n            return {'success': False, 'error': f'File not found: {csv_file_path}'}\n        \n        try:\n            # Try different separators and encodings with proper validation\n            df = None\n            separators = [',', ';']\n            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for sep in separators:\n                for enc in encodings:\n                    try:\n                        test_df = pd.read_csv(csv_file_path, sep=sep, encoding=enc)\n                        # Validate that we have multiple columns (not just one big column)\n                        if test_df.shape[1] > 1:\n                            df = test_df\n                            print(f\"Successfully read CSV with separator='{sep}' and encoding='{enc}' - Shape: {df.shape}\")\n                            break\n                        else:\n                            print(f\"Separator '{sep}' with encoding '{enc}' resulted in only {test_df.shape[1]} column(s) - trying next combination\")\n                    except Exception as e:\n                        print(f\"Failed with separator='{sep}' and encoding='{enc}': {str(e)[:100]}\")\n                        continue\n                if df is not None:\n                    break\n            \n            if df is None:\n                return {'success': False, 'error': 'Could not read CSV file with any encoding/separator combination that produces multiple columns'}\n            \n            return DatabaseManager.migrate_csv_to_database_from_df(df, os.path.basename(csv_file_path))\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def migrate_csv_to_database_from_df_with_progress(df: pd.DataFrame, filename: str = \"uploaded_data.csv\", progress_callback=None) -> Dict[str, Any]:\n        \"\"\"Migrate DataFrame to database with progress callback\"\"\"\n        try:\n            # Normalize column names first - handle double spaces and irregular spacing\n            df.columns = [' '.join(col.strip().split()) for col in df.columns]\n            \n            total_rows = len(df)\n            processed_rows = 0\n            \n            # Convert to database format\n            records = []\n            for idx, row in df.iterrows():\n                # Get client name with fallback\n                cliente_name = row.get('cliente') or row.get('Cliente')\n                if pd.isna(cliente_name) or cliente_name is None or str(cliente_name).strip() == '':\n                    cliente_name = 'Cliente Desconhecido'  # Fallback\n                \n                # Get plate with fallback\n                placa_value = row.get('placa') or row.get('Placa')\n                if pd.isna(placa_value) or placa_value is None or str(placa_value).strip() == '':\n                    continue  # Skip records without plate\n                \n                record = {\n                    'cliente': str(cliente_name).strip(),\n                    'placa': str(placa_value).strip(),\n                    'ativo': row.get('ativo') or row.get('Ativo'),\n                    'data': pd.to_datetime(row.get('data') or row.get('Data'), errors='coerce', dayfirst=True),\n                    'data_gprs': pd.to_datetime(row.get('data_gprs') or row.get('Data (GPRS)'), errors='coerce', dayfirst=True),\n                    'velocidade_km': DatabaseManager._safe_float_convert(row.get('velocidade_km') or row.get('Velocidade (Km)')),\n                    'ignicao': row.get('ignicao') or row.get('Ignição'),\n                    'motorista': row.get('motorista') or row.get('Motorista'),\n                    'gps': DatabaseManager._safe_int_convert(row.get('gps') or row.get('GPS')),\n                    'gprs': DatabaseManager._safe_int_convert(row.get('gprs') or row.get('Gprs')),\n                    'localizacao': row.get('localizacao') or row.get('Localização'),\n                    'endereco': row.get('endereco') or row.get('Endereço'),\n                    'tipo_evento': row.get('tipo_evento') or row.get('Tipo do Evento'),\n                    'cerca': row.get('cerca') or row.get('Cerca'),\n                    'saida': DatabaseManager._safe_int_convert(row.get('saida') or row.get('Saida')),\n                    'entrada': DatabaseManager._safe_int_convert(row.get('entrada') or row.get('Entrada')),\n                    'pacote': row.get('pacote') or row.get('Pacote'),\n                    'odometro_periodo_km': DatabaseManager._safe_float_convert(row.get('odometro_periodo_km') or row.get('Odômetro Período (Km)')),\n                    'horimetro_periodo': row.get('horimetro_periodo') or row.get('Horímetro Período'),\n                    'horimetro_embarcado': row.get('horimetro_embarcado') or row.get('Horímetro Embarcado'),\n                    'odometro_embarcado_km': DatabaseManager._safe_float_convert(row.get('odometro_embarcado_km') or row.get('Odômetro Embarcado (Km)')),\n                    'bateria': DatabaseManager._safe_float_convert(row.get('bateria') or row.get('Bateria')),\n                    'imagem': row.get('imagem') or row.get('Imagem'),\n                    'tensao': DatabaseManager._safe_float_convert(row.get('tensao') or row.get('Tensão')),\n                    'bloqueado': DatabaseManager._safe_int_convert(row.get('bloqueado') or row.get('Bloqueado')),\n                    'latitude': DatabaseManager._safe_float_convert(row.get('latitude') or row.get('Latitude')),\n                    'longitude': DatabaseManager._safe_float_convert(row.get('longitude') or row.get('Longitude'))\n                }\n                \n                records.append(record)\n                processed_rows += 1\n                \n                # Report progress during data preparation\n                if progress_callback and processed_rows % 100 == 0:\n                    progress_callback(processed_rows, total_rows, \"preparando\")\n            \n            # Save to database with progress\n            if initialize_database():\n                with FleetDatabaseService() as db:\n                    records_saved = db.save_telematics_data_with_progress(records, progress_callback)\n                    \n                    # Save processing record\n                    unique_vehicles = len(set(r['placa'] for r in records))\n                    unique_clients = len(set(r['cliente'] for r in records))\n                    db.save_processing_history(\n                        filename=filename,\n                        records_processed=records_saved,\n                        unique_vehicles=unique_vehicles,\n                        unique_clients=unique_clients,\n                        file_size_bytes=0  # Will be updated if available\n                    )\n                    \n                    return {\n                        'success': True,\n                        'records_processed': records_saved,\n                        'unique_vehicles': unique_vehicles,\n                        'unique_clients': unique_clients\n                    }\n            else:\n                return {'success': False, 'error': 'Database initialization failed'}\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def migrate_csv_to_database_from_df(df: pd.DataFrame, filename: str = \"uploaded_data.csv\") -> Dict[str, Any]:\n        \"\"\"Migrate DataFrame to database\"\"\"\n        try:\n            # Normalize column names first - handle double spaces and irregular spacing\n            df.columns = [' '.join(col.strip().split()) for col in df.columns]\n            \n            # Convert to database format\n            records = []\n            for _, row in df.iterrows():\n                # Get client name with fallback\n                cliente_name = row.get('cliente') or row.get('Cliente')\n                if pd.isna(cliente_name) or cliente_name is None or str(cliente_name).strip() == '':\n                    cliente_name = 'Cliente Desconhecido'  # Fallback\n                \n                # Get plate with fallback\n                placa_value = row.get('placa') or row.get('Placa')\n                if pd.isna(placa_value) or placa_value is None or str(placa_value).strip() == '':\n                    continue  # Skip records without plate\n                \n                record = {\n                    'cliente': str(cliente_name).strip(),\n                    'placa': str(placa_value).strip(),\n                    'ativo': row.get('ativo') or row.get('Ativo'),\n                    'data': pd.to_datetime(row.get('data') or row.get('Data'), errors='coerce', dayfirst=True),\n                    'data_gprs': pd.to_datetime(row.get('data_gprs') or row.get('Data (GPRS)'), errors='coerce', dayfirst=True),\n                    'velocidade_km': DatabaseManager._safe_float_convert(row.get('velocidade_km') or row.get('Velocidade (Km)')),\n                    'ignicao': row.get('ignicao') or row.get('Ignição'),\n                    'motorista': row.get('motorista') or row.get('Motorista'),\n                    'gps': DatabaseManager._safe_int_convert(row.get('gps') or row.get('GPS')),\n                    'gprs': DatabaseManager._safe_int_convert(row.get('gprs') or row.get('Gprs')),\n                    'localizacao': row.get('localizacao') or row.get('Localização'),\n                    'endereco': row.get('endereco') or row.get('Endereço'),\n                    'tipo_evento': row.get('tipo_evento') or row.get('Tipo do Evento'),\n                    'cerca': row.get('cerca') or row.get('Cerca'),\n                    'saida': DatabaseManager._safe_int_convert(row.get('saida') or row.get('Saida')),\n                    'entrada': DatabaseManager._safe_int_convert(row.get('entrada') or row.get('Entrada')),\n                    'pacote': row.get('pacote') or row.get('Pacote'),\n                    'odometro_periodo_km': DatabaseManager._safe_float_convert(row.get('odometro_periodo_km') or row.get('Odômetro do período (Km)') or row.get('Odômetro do período  (Km)')),\n                    'horimetro_periodo': row.get('horimetro_periodo') or row.get('Horímetro do período'),\n                    'horimetro_embarcado': row.get('horimetro_embarcado') or row.get('Horímetro embarcado'),\n                    'odometro_embarcado_km': DatabaseManager._safe_float_convert(row.get('odometro_embarcado_km') or row.get('Odômetro embarcado (Km)')),\n                    'bateria': row.get('bateria') or row.get('Bateria'),\n                    'imagem': row.get('imagem') or row.get('Imagem'),\n                    'tensao': DatabaseManager._safe_float_convert(row.get('tensao') or row.get('Tensão')),\n                    'bloqueado': DatabaseManager._safe_int_convert(row.get('bloqueado') or row.get('Bloqueado'))\n                }\n                \n                # Add derived location data if available\n                location_field = row.get('localizacao') or row.get('Localização')\n                if pd.notna(location_field):\n                    try:\n                        location = str(location_field)\n                        if ',' in location:\n                            lat_str, lon_str = location.split(',')\n                            record['latitude'] = float(lat_str.strip())\n                            record['longitude'] = float(lon_str.strip())\n                    except:\n                        pass\n                \n                records.append(record)\n            \n            # Save to database\n            with FleetDatabaseService() as db:\n                records_saved = db.save_telematics_data(records)\n                \n                # Save processing history\n                unique_vehicles = df[df.columns[df.columns.str.contains('placa|Placa', case=False)].tolist()[0]].nunique()\n                unique_clients = df[df.columns[df.columns.str.contains('cliente|Cliente', case=False)].tolist()[0]].nunique()\n                \n                # Find date column\n                date_col = None\n                for col in df.columns:\n                    if 'data' in col.lower() and 'gprs' not in col.lower():\n                        date_col = col\n                        break\n                \n                date_range = (None, None)\n                if date_col and not df[date_col].isna().all():\n                    # Parse dates with Brazilian format\n                    date_series = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True)\n                    date_range = (date_series.min(), date_series.max())\n                \n                db.save_processing_history(\n                    filename=filename,\n                    records_processed=records_saved,\n                    unique_vehicles=unique_vehicles,\n                    unique_clients=unique_clients,\n                    date_range_start=pd.to_datetime(date_range[0]) if date_range[0] else None,\n                    date_range_end=pd.to_datetime(date_range[1]) if date_range[1] else None,\n                    file_size_bytes=len(str(df)) if df is not None else 0\n                )\n            \n            return {\n                'success': True,\n                'records_processed': records_saved,\n                'unique_vehicles': unique_vehicles,\n                'unique_clients': unique_clients\n            }\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def get_dashboard_data(client_filter: Optional[str] = None,\n                          vehicle_filter: Optional[str] = None,\n                          start_date: Optional[datetime] = None,\n                          end_date: Optional[datetime] = None) -> pd.DataFrame:\n        \"\"\"Get dashboard data with filters\"\"\"\n        with FleetDatabaseService() as db:\n            # Convert filter values to IDs if needed\n            client_id = None\n            vehicle_id = None\n            \n            if client_filter:\n                clients = db.get_all_clients()\n                for client in clients:\n                    if client.name == client_filter:\n                        client_id = client.id\n                        break\n            \n            if vehicle_filter:\n                vehicles = db.get_all_vehicles()\n                for vehicle in vehicles:\n                    if vehicle.plate == vehicle_filter:\n                        vehicle_id = vehicle.id\n                        break\n            \n            return db.get_telematics_dataframe(\n                client_id=client_id,\n                vehicle_id=vehicle_id,\n                start_date=start_date,\n                end_date=end_date\n            )\n    \n    @staticmethod\n    def get_fleet_summary() -> Dict[str, Any]:\n        \"\"\"Get fleet summary statistics\"\"\"\n        with FleetDatabaseService() as db:\n            return db.get_fleet_summary()\n    \n    @staticmethod\n    def get_processing_history() -> List[Dict[str, Any]]:\n        \"\"\"Get processing history for display\"\"\"\n        with FleetDatabaseService() as db:\n            history = db.get_processing_history()\n            return [\n                {\n                    'filename': h.filename,\n                    'upload_timestamp': h.upload_timestamp,\n                    'records_processed': h.records_processed,\n                    'unique_vehicles': h.unique_vehicles,\n                    'unique_clients': h.unique_clients,\n                    'processing_status': h.processing_status,\n                    'file_size_bytes': h.file_size_bytes\n                }\n                for h in history\n            ]\n    \n    @staticmethod\n    def clear_all_data() -> Dict[str, int]:\n        \"\"\"Clear all data from database\"\"\"\n        with FleetDatabaseService() as db:\n            return db.clear_all_data()\n    \n    @staticmethod\n    def has_data() -> bool:\n        \"\"\"Check if database has any telematics data\"\"\"\n        try:\n            if not initialize_database():\n                return False\n            with FleetDatabaseService() as db:\n                summary = db.get_fleet_summary()\n                return summary['total_records'] > 0\n        except:\n            return False\n    \n    @staticmethod\n    def get_client_list() -> List[str]:\n        \"\"\"Get list of all client names\"\"\"\n        with FleetDatabaseService() as db:\n            clients = db.get_all_clients()\n            return [client.name for client in clients]\n    \n    @staticmethod\n    def get_vehicle_list() -> List[str]:\n        \"\"\"Get list of all vehicle plates\"\"\"\n        with FleetDatabaseService() as db:\n            vehicles = db.get_all_vehicles()\n            return [vehicle.plate for vehicle in vehicles]","size_bytes":20434},"database/init_db.py":{"content":"\"\"\"\nInitialize database schema and create all tables\n\"\"\"\nfrom database.connection import engine, Base\nfrom database.models import (\n    Client, Vehicle, TelematicsData, ProcessingHistory, \n    InsightData, AlertConfiguration\n)\n\ndef create_all_tables():\n    \"\"\"Create all database tables\"\"\"\n    try:\n        Base.metadata.create_all(bind=engine)\n        print(\"✅ Database tables created successfully!\")\n        return True\n    except Exception as e:\n        print(f\"❌ Error creating database tables: {e}\")\n        return False\n\ndef drop_all_tables():\n    \"\"\"Drop all database tables (use with caution!)\"\"\"\n    try:\n        Base.metadata.drop_all(bind=engine)\n        print(\"✅ Database tables dropped successfully!\")\n        return True\n    except Exception as e:\n        print(f\"❌ Error dropping database tables: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    create_all_tables()","size_bytes":898},"database/models.py":{"content":"\"\"\"\nDatabase models for fleet monitoring system\n\"\"\"\nfrom sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, Text, ForeignKey\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.sql import func\nfrom database.connection import Base\n\nclass Client(Base):\n    \"\"\"Client/Customer table\"\"\"\n    __tablename__ = 'clients'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(255), unique=True, nullable=False)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Relationships\n    vehicles = relationship(\"Vehicle\", back_populates=\"client\")\n    telematics_data = relationship(\"TelematicsData\", back_populates=\"client\")\n\nclass Vehicle(Base):\n    \"\"\"Vehicle information table\"\"\"\n    __tablename__ = 'vehicles'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    plate = Column(String(20), unique=True, nullable=False, index=True)\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=False)\n    asset_id = Column(String(50))  # ID do ativo\n    driver_name = Column(String(255))  # Nome do motorista padrão\n    vehicle_type = Column(String(100))  # Tipo de veículo\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Relationships\n    client = relationship(\"Client\", back_populates=\"vehicles\")\n    telematics_data = relationship(\"TelematicsData\", back_populates=\"vehicle\")\n\nclass TelematicsData(Base):\n    \"\"\"Main telematics data table - stores all GPS and sensor data\"\"\"\n    __tablename__ = 'telematics_data'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    # Basic identification\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=False, index=True)\n    vehicle_id = Column(Integer, ForeignKey('vehicles.id'), nullable=False, index=True)\n    plate = Column(String(20), nullable=False, index=True)\n    asset_id = Column(String(50))\n    \n    # Timestamps\n    timestamp = Column(DateTime(timezone=True), nullable=False, index=True)\n    gprs_timestamp = Column(DateTime(timezone=True))\n    \n    # Location and GPS data\n    latitude = Column(Float)\n    longitude = Column(Float)\n    location = Column(String(255))  # Localização formatted\n    address = Column(Text)  # Endereço completo\n    gps_quality = Column(Boolean, default=False)  # GPS signal quality\n    gprs_quality = Column(Boolean, default=False)  # GPRS signal quality\n    \n    # Vehicle status\n    speed_kmh = Column(Float, default=0.0)  # Velocidade em km/h\n    ignition = Column(String(10))  # D=Dirigindo, L=Ligado, etc.\n    driver_name = Column(String(255))  # Motorista\n    blocked = Column(Boolean, default=False)  # Bloqueado\n    \n    # Event information\n    event_type = Column(String(100))  # Tipo do Evento\n    geofence = Column(String(255))  # Cerca eletrônica\n    entry = Column(Boolean, default=False)  # Entrada\n    exit = Column(Boolean, default=False)  # Saída\n    \n    # Technical data\n    packet_id = Column(String(50))  # ID do pacote\n    odometer_period_km = Column(Float, default=0.0)  # Odômetro do período\n    engine_hours_period = Column(String(20))  # Horímetro do período\n    engine_hours_total = Column(String(20))  # Horímetro embarcado\n    odometer_total_km = Column(Float, default=0.0)  # Odômetro embarcado\n    battery_level = Column(String(10))  # Nível da bateria\n    voltage = Column(Float)  # Tensão\n    image_url = Column(String(500))  # URL da imagem se disponível\n    \n    # Metadata\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    # Relationships\n    client = relationship(\"Client\", back_populates=\"telematics_data\")\n    vehicle = relationship(\"Vehicle\", back_populates=\"telematics_data\")\n\nclass ProcessingHistory(Base):\n    \"\"\"Track CSV file processing history\"\"\"\n    __tablename__ = 'processing_history'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    filename = Column(String(255), nullable=False)\n    upload_timestamp = Column(DateTime(timezone=True), server_default=func.now())\n    records_processed = Column(Integer, default=0)\n    records_failed = Column(Integer, default=0)\n    unique_vehicles = Column(Integer, default=0)\n    unique_clients = Column(Integer, default=0)\n    date_range_start = Column(DateTime(timezone=True))\n    date_range_end = Column(DateTime(timezone=True))\n    processing_status = Column(String(50), default='completed')  # completed, failed, processing\n    error_message = Column(Text)\n    file_size_bytes = Column(Integer)\n\nclass InsightData(Base):\n    \"\"\"Store generated insights and analysis results\"\"\"\n    __tablename__ = 'insights'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=True)\n    vehicle_id = Column(Integer, ForeignKey('vehicles.id'), nullable=True)\n    \n    # Insight content\n    title = Column(String(255), nullable=False)\n    description = Column(Text, nullable=False)\n    recommendation = Column(Text)\n    insight_type = Column(String(50), nullable=False)  # error, warning, info, success\n    priority = Column(Integer, default=3)  # 1=alta, 2=média, 3=baixa, 4=info\n    category = Column(String(100))  # compliance, efficiency, maintenance, etc.\n    \n    # Analysis metadata\n    analysis_period_start = Column(DateTime(timezone=True))\n    analysis_period_end = Column(DateTime(timezone=True))\n    confidence_score = Column(Float)  # For ML-generated insights\n    data_source = Column(String(100))  # manual, automated, ml_model\n    \n    # Timestamps\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Status tracking\n    is_active = Column(Boolean, default=True)\n    is_resolved = Column(Boolean, default=False)\n    resolved_at = Column(DateTime(timezone=True))\n    resolved_by = Column(String(255))\n\nclass AlertConfiguration(Base):\n    \"\"\"Configuration for real-time alerts\"\"\"\n    __tablename__ = 'alert_configurations'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=True)\n    vehicle_id = Column(Integer, ForeignKey('vehicles.id'), nullable=True)\n    \n    # Alert configuration\n    alert_type = Column(String(100), nullable=False)  # speed_limit, geofence, maintenance\n    threshold_value = Column(Float)\n    threshold_operator = Column(String(10))  # >, <, =, >=, <=\n    is_active = Column(Boolean, default=True)\n    \n    # Notification settings\n    notification_channels = Column(Text)  # JSON array of notification methods\n    cooldown_minutes = Column(Integer, default=15)  # Prevent spam\n    \n    # Metadata\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    created_by = Column(String(255))","size_bytes":6925},"database/services.py":{"content":"\"\"\"\nDatabase service layer for fleet monitoring operations\n\"\"\"\nfrom typing import List, Optional, Dict, Any, Tuple\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import func, and_, or_, Integer\nfrom database.connection import get_db_session, close_db_session, initialize_database\nfrom database.models import (\n    Client, Vehicle, TelematicsData, ProcessingHistory, \n    InsightData, AlertConfiguration\n)\n\nclass FleetDatabaseService:\n    \"\"\"Service class for all fleet monitoring database operations\"\"\"\n    \n    def __init__(self):\n        self.session = None\n    \n    def __enter__(self):\n        if not initialize_database():\n            raise Exception(\"Falha ao conectar com a base de dados\")\n        self.session = get_db_session()\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            if exc_type:\n                self.session.rollback()\n            else:\n                self.session.commit()\n            close_db_session(self.session)\n    \n    # Client operations\n    def get_or_create_client(self, client_name: str) -> Client:\n        \"\"\"Get existing client or create new one\"\"\"\n        client = self.session.query(Client).filter(Client.name == client_name).first()\n        if not client:\n            client = Client(name=client_name)\n            self.session.add(client)\n            self.session.flush()  # Get the ID\n        return client\n    \n    def get_all_clients(self) -> List[Client]:\n        \"\"\"Get all clients\"\"\"\n        return self.session.query(Client).all()\n    \n    # Vehicle operations\n    def get_or_create_vehicle(self, plate: str, client_id: int, asset_id: str = None) -> Vehicle:\n        \"\"\"Get existing vehicle or create new one\"\"\"\n        vehicle = self.session.query(Vehicle).filter(Vehicle.plate == plate).first()\n        if not vehicle:\n            vehicle = Vehicle(\n                plate=plate,\n                client_id=client_id,\n                asset_id=asset_id\n            )\n            self.session.add(vehicle)\n            self.session.flush()\n        return vehicle\n    \n    def get_vehicles_by_client(self, client_id: int) -> List[Vehicle]:\n        \"\"\"Get all vehicles for a client\"\"\"\n        return self.session.query(Vehicle).filter(Vehicle.client_id == client_id).all()\n    \n    def get_all_vehicles(self) -> List[Vehicle]:\n        \"\"\"Get all vehicles\"\"\"\n        return self.session.query(Vehicle).all()\n    \n    # Telematics data operations\n    def save_telematics_data_with_progress(self, data_records: List[Dict[str, Any]], progress_callback=None) -> int:\n        \"\"\"Save multiple telematics data records with progress callback\"\"\"\n        records_saved = 0\n        records_failed = 0\n        total_records = len(data_records)\n        \n        # Process in batches to avoid memory issues\n        batch_size = 50  # Smaller batch for more frequent progress updates\n        for i in range(0, total_records, batch_size):\n            batch = data_records[i:i + batch_size]\n            batch_saved = 0\n            \n            try:\n                for record in batch:\n                    try:\n                        # Validate timestamp before processing\n                        timestamp = record.get('data')\n                        if timestamp is None or pd.isna(timestamp):\n                            records_failed += 1\n                            continue\n                        \n                        # Get or create client and vehicle\n                        client = self.get_or_create_client(record['cliente'])\n                        vehicle = self.get_or_create_vehicle(\n                            record['placa'], \n                            client.id, \n                            record.get('ativo')\n                        )\n                        \n                        # Create telematics data record\n                        telematics = TelematicsData(\n                            client_id=client.id,\n                            vehicle_id=vehicle.id,\n                            plate=record['placa'],\n                            asset_id=record.get('ativo'),\n                            timestamp=timestamp,\n                            gprs_timestamp=record.get('data_gprs'),\n                            latitude=record.get('latitude'),\n                            longitude=record.get('longitude'),\n                            location=record.get('localizacao'),\n                            address=record.get('endereco'),\n                            gps_quality=record.get('gps', 0) == 1,\n                            gprs_quality=record.get('gprs', 0) == 1,\n                            speed_kmh=record.get('velocidade_km', 0.0),\n                            ignition=record.get('ignicao'),\n                            driver_name=record.get('motorista'),\n                            blocked=record.get('bloqueado', 0) == 1,\n                            event_type=record.get('tipo_evento'),\n                            geofence=record.get('cerca'),\n                            entry=record.get('entrada', 0) == 1,\n                            exit=record.get('saida', 0) == 1,\n                            packet_id=record.get('pacote'),\n                            odometer_period_km=record.get('odometro_periodo_km', 0.0),\n                            odometer_total_km=record.get('odometro_embarcado_km', 0.0),\n                            engine_hours_period=record.get('horimetro_periodo'),\n                            engine_hours_total=record.get('horimetro_embarcado'),\n                            battery_level=record.get('bateria'),\n                            voltage=record.get('tensao'),\n                            image_url=record.get('imagem')\n                        )\n                        \n                        self.session.add(telematics)\n                        batch_saved += 1\n                        records_saved += 1\n                        \n                        # Report progress every 10 records within batch\n                        if progress_callback and records_saved % 10 == 0:\n                            progress_callback(records_saved, total_records, \"inserindo\")\n                        \n                    except Exception as record_error:\n                        records_failed += 1\n                        print(f\"Failed to save record: {str(record_error)[:200]}\")\n                        continue\n                \n                # Commit the batch\n                self.session.commit()\n                \n                # Report progress after each batch\n                if progress_callback:\n                    progress_callback(records_saved, total_records, \"inserindo\")\n                \n            except Exception as batch_error:\n                self.session.rollback()\n                records_failed += batch_size\n                print(f\"Batch failed: {str(batch_error)[:200]}\")\n                continue\n        \n        print(f\"Batch insertion completed: {records_saved} saved, {records_failed} failed\")\n        return records_saved\n    \n    def save_telematics_data(self, data_records: List[Dict[str, Any]]) -> int:\n        \"\"\"Save multiple telematics data records with proper error handling\"\"\"\n        records_saved = 0\n        records_failed = 0\n        \n        # Process in batches to avoid memory issues\n        batch_size = 100\n        for i in range(0, len(data_records), batch_size):\n            batch = data_records[i:i + batch_size]\n            batch_saved = 0\n            \n            try:\n                for record in batch:\n                    try:\n                        # Validate timestamp before processing\n                        timestamp = record.get('data')\n                        if timestamp is None or pd.isna(timestamp):\n                            records_failed += 1\n                            continue\n                        \n                        # Get or create client and vehicle\n                        client = self.get_or_create_client(record['cliente'])\n                        vehicle = self.get_or_create_vehicle(\n                            record['placa'], \n                            client.id, \n                            record.get('ativo')\n                        )\n                        \n                        # Create telematics data record\n                        telematics = TelematicsData(\n                            client_id=client.id,\n                            vehicle_id=vehicle.id,\n                            plate=record['placa'],\n                            asset_id=record.get('ativo'),\n                            timestamp=timestamp,\n                            gprs_timestamp=record.get('data_gprs'),\n                            latitude=record.get('latitude'),\n                            longitude=record.get('longitude'),\n                            location=record.get('localizacao'),\n                            address=record.get('endereco'),\n                            gps_quality=record.get('gps', 0) == 1,\n                            gprs_quality=record.get('gprs', 0) == 1,\n                            speed_kmh=record.get('velocidade_km', 0.0),\n                            ignition=record.get('ignicao'),\n                            driver_name=record.get('motorista'),\n                            blocked=record.get('bloqueado', 0) == 1,\n                            event_type=record.get('tipo_evento'),\n                            geofence=record.get('cerca'),\n                            entry=record.get('entrada', 0) == 1,\n                            exit=record.get('saida', 0) == 1,\n                            packet_id=record.get('pacote'),\n                            odometer_period_km=record.get('odometro_periodo_km', 0.0),\n                            engine_hours_period=record.get('horimetro_periodo'),\n                            engine_hours_total=record.get('horimetro_embarcado'),\n                            odometer_total_km=record.get('odometro_embarcado_km', 0.0),\n                            battery_level=record.get('bateria'),\n                            voltage=record.get('tensao'),\n                            image_url=record.get('imagem')\n                        )\n                        \n                        self.session.add(telematics)\n                        batch_saved += 1\n                        \n                    except Exception as e:\n                        # Log individual record error but continue processing\n                        print(f\"Error processing record {record.get('placa', 'unknown')}: {str(e)}\")\n                        records_failed += 1\n                        continue\n                \n                # Commit the batch\n                self.session.commit()\n                records_saved += batch_saved\n                \n            except Exception as e:\n                # If batch commit fails, rollback and mark all batch records as failed\n                print(f\"Error committing batch {i//batch_size + 1}: {str(e)}\")\n                self.session.rollback()\n                records_failed += len(batch)\n        \n        print(f\"Migration complete: {records_saved} saved, {records_failed} failed\")\n        return records_saved\n    \n    def get_telematics_data(self, \n                           client_id: Optional[int] = None,\n                           vehicle_id: Optional[int] = None,\n                           plate: Optional[str] = None,\n                           start_date: Optional[datetime] = None,\n                           end_date: Optional[datetime] = None,\n                           limit: Optional[int] = None) -> List[TelematicsData]:\n        \"\"\"Get telematics data with filters\"\"\"\n        query = self.session.query(TelematicsData)\n        \n        if client_id:\n            query = query.filter(TelematicsData.client_id == client_id)\n        if vehicle_id:\n            query = query.filter(TelematicsData.vehicle_id == vehicle_id)\n        if plate:\n            query = query.filter(TelematicsData.plate == plate)\n        if start_date:\n            query = query.filter(TelematicsData.timestamp >= start_date)\n        if end_date:\n            query = query.filter(TelematicsData.timestamp <= end_date)\n        \n        query = query.order_by(TelematicsData.timestamp.desc())\n        \n        if limit:\n            query = query.limit(limit)\n        \n        return query.all()\n    \n    def get_telematics_dataframe(self, **filters) -> pd.DataFrame:\n        \"\"\"Get telematics data as pandas DataFrame\"\"\"\n        data = self.get_telematics_data(**filters)\n        \n        if not data:\n            return pd.DataFrame()\n        \n        # Convert to DataFrame with original column names for compatibility\n        records = []\n        for record in data:\n            records.append({\n                'cliente': record.client.name,\n                'placa': record.plate,\n                'ativo': record.asset_id,\n                'data': record.timestamp,\n                'data_gprs': record.gprs_timestamp,\n                'velocidade_km': record.speed_kmh,\n                'ignicao': record.ignition,\n                'motorista': record.driver_name,\n                'gps': 1 if record.gps_quality else 0,\n                'gprs': 1 if record.gprs_quality else 0,\n                'localizacao': record.location,\n                'endereco': record.address,\n                'tipo_evento': record.event_type,\n                'cerca': record.geofence,\n                'saida': 1 if record.exit else 0,\n                'entrada': 1 if record.entry else 0,\n                'pacote': record.packet_id,\n                'odometro_periodo_km': record.odometer_period_km,\n                'engine_hours_period': record.engine_hours_period,\n                'engine_hours_total': record.engine_hours_total,\n                'odometer_total_km': record.odometer_total_km,\n                'battery_level': record.battery_level,\n                'imagem': record.image_url,\n                'tensao': record.voltage,\n                'bloqueado': 1 if record.blocked else 0,\n                'latitude': record.latitude,\n                'longitude': record.longitude\n            })\n        \n        return pd.DataFrame(records)\n    \n    # Analytics and KPI methods\n    def get_fleet_summary(self) -> Dict[str, Any]:\n        \"\"\"Get overall fleet summary statistics\"\"\"\n        total_vehicles = self.session.query(Vehicle).count()\n        total_clients = self.session.query(Client).count()\n        \n        # Get latest data period\n        latest_data = self.session.query(\n            func.min(TelematicsData.timestamp).label('start_date'),\n            func.max(TelematicsData.timestamp).label('end_date'),\n            func.count(TelematicsData.id).label('total_records')\n        ).first()\n        \n        # Calculate average speed and total distance\n        speed_stats = self.session.query(\n            func.avg(TelematicsData.speed_kmh).label('avg_speed'),\n            func.max(TelematicsData.speed_kmh).label('max_speed'),\n            func.sum(TelematicsData.odometer_period_km).label('total_distance')\n        ).first()\n        \n        # GPS coverage - simplified approach\n        gps_coverage = self.session.query(\n            func.avg(TelematicsData.gps_quality.cast(Integer)).label('gps_coverage')\n        ).first()\n        \n        return {\n            'total_vehicles': total_vehicles,\n            'total_clients': total_clients,\n            'total_records': latest_data.total_records if latest_data.total_records else 0,\n            'start_date': latest_data.start_date,\n            'end_date': latest_data.end_date,\n            'avg_speed': round(speed_stats.avg_speed, 1) if speed_stats.avg_speed else 0,\n            'max_speed': speed_stats.max_speed if speed_stats.max_speed else 0,\n            'total_distance': round(speed_stats.total_distance, 1) if speed_stats.total_distance else 0,\n            'gps_coverage': round(gps_coverage.gps_coverage * 100, 1) if gps_coverage.gps_coverage else 0\n        }\n    \n    # Processing history operations\n    def save_processing_history(self, \n                               filename: str,\n                               records_processed: int,\n                               records_failed: int = 0,\n                               unique_vehicles: int = 0,\n                               unique_clients: int = 0,\n                               date_range_start: Optional[datetime] = None,\n                               date_range_end: Optional[datetime] = None,\n                               processing_status: str = 'completed',\n                               error_message: Optional[str] = None,\n                               file_size_bytes: Optional[int] = None) -> ProcessingHistory:\n        \"\"\"Save processing history record\"\"\"\n        history = ProcessingHistory(\n            filename=filename,\n            records_processed=records_processed,\n            records_failed=records_failed,\n            unique_vehicles=unique_vehicles,\n            unique_clients=unique_clients,\n            date_range_start=date_range_start,\n            date_range_end=date_range_end,\n            processing_status=processing_status,\n            error_message=error_message,\n            file_size_bytes=file_size_bytes\n        )\n        \n        self.session.add(history)\n        self.session.flush()\n        return history\n    \n    def get_processing_history(self, limit: int = 10) -> List[ProcessingHistory]:\n        \"\"\"Get recent processing history\"\"\"\n        return (self.session.query(ProcessingHistory)\n                .order_by(ProcessingHistory.upload_timestamp.desc())\n                .limit(limit)\n                .all())\n    \n    def clear_processing_history(self) -> int:\n        \"\"\"Clear all processing history\"\"\"\n        count = self.session.query(ProcessingHistory).count()\n        self.session.query(ProcessingHistory).delete()\n        return count\n    \n    def clear_all_data(self) -> Dict[str, int]:\n        \"\"\"Clear all telematics data and processing history\"\"\"\n        telematics_count = self.session.query(TelematicsData).count()\n        history_count = self.session.query(ProcessingHistory).count()\n        insights_count = self.session.query(InsightData).count()\n        vehicles_count = self.session.query(Vehicle).count()\n        clients_count = self.session.query(Client).count()\n        \n        # Clear all data\n        self.session.query(TelematicsData).delete()\n        self.session.query(ProcessingHistory).delete() \n        self.session.query(InsightData).delete()\n        self.session.query(Vehicle).delete()\n        self.session.query(Client).delete()\n        \n        return {\n            'telematics_data': telematics_count,\n            'processing_history': history_count, \n            'insights': insights_count,\n            'vehicles': vehicles_count,\n            'clients': clients_count\n        }\n    \n    # Insights operations\n    def save_insight(self, \n                    title: str,\n                    description: str,\n                    insight_type: str,\n                    priority: int = 3,\n                    recommendation: Optional[str] = None,\n                    category: Optional[str] = None,\n                    client_id: Optional[int] = None,\n                    vehicle_id: Optional[int] = None,\n                    confidence_score: Optional[float] = None,\n                    data_source: str = 'automated') -> InsightData:\n        \"\"\"Save a new insight\"\"\"\n        insight = InsightData(\n            title=title,\n            description=description,\n            recommendation=recommendation,\n            insight_type=insight_type,\n            priority=priority,\n            category=category,\n            client_id=client_id,\n            vehicle_id=vehicle_id,\n            confidence_score=confidence_score,\n            data_source=data_source\n        )\n        \n        self.session.add(insight)\n        self.session.flush()\n        return insight\n    \n    def get_insights(self, \n                    category: Optional[str] = None,\n                    client_id: Optional[int] = None,\n                    vehicle_id: Optional[int] = None,\n                    is_active: bool = True,\n                    limit: int = 50) -> List[InsightData]:\n        \"\"\"Get insights with filters\"\"\"\n        query = self.session.query(InsightData)\n        \n        if category:\n            query = query.filter(InsightData.category == category)\n        if client_id:\n            query = query.filter(InsightData.client_id == client_id)\n        if vehicle_id:\n            query = query.filter(InsightData.vehicle_id == vehicle_id)\n        if is_active is not None:\n            query = query.filter(InsightData.is_active == is_active)\n        \n        return query.order_by(InsightData.created_at.desc()).limit(limit).all()","size_bytes":20833},"pages/1_📊_Dashboard.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport sys\n\n# Adicionar o diretório raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.visualizations import FleetVisualizations\nfrom database.db_manager import DatabaseManager\n\nst.set_page_config(\n    page_title=\"Dashboard - Insight Hub\",\n    page_icon=\"📊\",\n    layout=\"wide\"\n)\n\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais)\"\"\"\n    try:\n        # Carregar dados diretamente da base de dados\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty:\n            st.success(f\"✅ Dados reais carregados: {len(df):,} registros da base de dados\")\n            return df\n        \n        # Se não há dados, mostrar mensagem clara\n        st.warning(\"⚠️ Nenhum dado encontrado na base de dados. Faça upload dos seus arquivos CSV.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados reais: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"📊 Análise Profissional\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"📁 Nenhum dado encontrado. Faça o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados da base de dados\n    analyzer = DataAnalyzer.from_database()\n    visualizer = FleetVisualizations(analyzer)\n    \n    # Sidebar com filtros\n    st.sidebar.header(\"🔍 Filtros\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por período\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    col_data1, col_data2 = st.sidebar.columns(2)\n    with col_data1:\n        data_inicio = st.date_input(\"Data Início:\", min_date, min_value=min_date, max_value=max_date)\n    with col_data2:\n        data_fim = st.date_input(\"Data Fim:\", max_date, min_value=min_date, max_value=max_date)\n    \n    # Filtro por veículo\n    veiculos_disponiveis = ['Todos']\n    if cliente_selecionado != \"Todos\":\n        veiculos_disponiveis.extend(sorted(df[df['cliente'] == cliente_selecionado]['placa'].unique().tolist()))\n    else:\n        veiculos_disponiveis.extend(sorted(df['placa'].unique().tolist()))\n    \n    veiculo_selecionado = st.sidebar.selectbox(\"Veículo:\", veiculos_disponiveis)\n    \n    # Aplicar filtros\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        placa=veiculo_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    if filtered_df.empty:\n        st.warning(\"⚠️ Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Obter KPIs\n    kpis = analyzer.get_kpis()\n    \n    # Verificar se há KPIs válidos\n    if not kpis:\n        st.warning(\"⚠️ Não foi possível calcular métricas com os filtros aplicados. Tente ajustar os filtros.\")\n        st.stop()\n    \n    # Mostrar métricas principais\n    st.header(\"📈 Métricas Principais\")\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            label=\"🚗 Total de Veículos\",\n            value=f\"{kpis['total_veiculos']:,}\",\n            delta=f\"{len(filtered_df):,} registros\"\n        )\n    \n    with col2:\n        st.metric(\n            label=\"⚡ Velocidade Média\",\n            value=f\"{kpis['velocidade_media']:.1f} km/h\",\n            delta=f\"Max: {kpis['velocidade_maxima']:.0f} km/h\"\n        )\n    \n    with col3:\n        st.metric(\n            label=\"🛣️ Distância Total\",\n            value=f\"{kpis['distancia_total']:.0f} km\",\n            delta=f\"{kpis['tempo_ativo_horas']:.1f} horas ativas\"\n        )\n    \n    with col4:\n        st.metric(\n            label=\"📡 Cobertura GPS\",\n            value=f\"{kpis['cobertura_gps']:.1f}%\",\n            delta=f\"{kpis['veiculos_bloqueados']} bloqueados\"\n        )\n    \n    st.markdown(\"---\")\n    \n    # Gráficos principais\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"📊 Distribuição de Velocidade\")\n        \n        speed_dist_fig = px.histogram(\n            filtered_df,\n            x='velocidade_km',\n            nbins=30,\n            title='Distribuição de Velocidade',\n            labels={'velocidade_km': 'Velocidade (km/h)', 'count': 'Frequência'}\n        )\n        speed_dist_fig.update_layout(height=400)\n        st.plotly_chart(speed_dist_fig, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"🚗 Top 10 Veículos Mais Ativos\")\n        \n        vehicle_activity = filtered_df.groupby('placa').size().sort_values(ascending=False).head(10)\n        \n        activity_fig = px.bar(\n            x=vehicle_activity.values,\n            y=vehicle_activity.index,\n            orientation='h',\n            title='Registros por Veículo',\n            labels={'x': 'Número de Registros', 'y': 'Placa'}\n        )\n        activity_fig.update_layout(height=400)\n        st.plotly_chart(activity_fig, use_container_width=True)\n    \n    # Análise temporal\n    st.subheader(\"⏰ Análise Temporal\")\n    \n    # Atividade por hora\n    hourly_activity = filtered_df.groupby(filtered_df['data'].dt.hour).agg({\n        'placa': 'nunique',\n        'velocidade_km': 'mean'\n    }).reset_index()\n    \n    col_temp1, col_temp2 = st.columns(2)\n    \n    with col_temp1:\n        hourly_vehicles_fig = px.line(\n            hourly_activity,\n            x='data',\n            y='placa',\n            title='Veículos Ativos por Hora',\n            labels={'data': 'Hora do Dia', 'placa': 'Número de Veículos Ativos'}\n        )\n        hourly_vehicles_fig.update_layout(height=350)\n        st.plotly_chart(hourly_vehicles_fig, use_container_width=True)\n    \n    with col_temp2:\n        hourly_speed_fig = px.line(\n            hourly_activity,\n            x='data',\n            y='velocidade_km',\n            title='Velocidade Média por Hora',\n            labels={'data': 'Hora do Dia', 'velocidade_km': 'Velocidade Média (km/h)'}\n        )\n        hourly_speed_fig.update_layout(height=350)\n        st.plotly_chart(hourly_speed_fig, use_container_width=True)\n    \n    # Atividade diária (se mais de um dia)\n    if kpis['periodo_dias'] > 1:\n        daily_activity = filtered_df.groupby(filtered_df['data'].dt.date).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean',\n            'odometro_periodo_km': 'sum'\n        }).reset_index()\n        \n        st.subheader(\"📅 Tendência Diária\")\n        \n        daily_fig = px.line(\n            daily_activity,\n            x='data',\n            y='placa',\n            title='Veículos Ativos por Dia',\n            labels={'data': 'Data', 'placa': 'Número de Veículos Ativos'}\n        )\n        daily_fig.update_layout(height=400)\n        st.plotly_chart(daily_fig, use_container_width=True)\n    \n    # Tabela de resumo por veículo\n    st.subheader(\"📋 Resumo por Veículo\")\n    \n    vehicle_summary = filtered_df.groupby('placa').agg({\n        'velocidade_km': ['count', 'mean', 'max'],\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.sum() / len(x)) * 100,\n        'bloqueado': 'any'\n    }).round(2)\n    \n    # Achatar MultiIndex\n    vehicle_summary.columns = [\n        'Registros', 'Vel. Média', 'Vel. Máxima', \n        'KM Total', 'GPS (%)', 'Bloqueado'\n    ]\n    \n    vehicle_summary = vehicle_summary.sort_values('Registros', ascending=False)\n    \n    st.dataframe(\n        vehicle_summary,\n        use_container_width=True,\n        height=300\n    )\n    \n    # Estatísticas do período filtrado\n    st.markdown(\"---\")\n    st.subheader(\"📊 Estatísticas do Período\")\n    \n    info_col1, info_col2, info_col3 = st.columns(3)\n    \n    with info_col1:\n        st.info(f\"\"\"\n        **📅 Período Analisado:**\n        - Início: {data_inicio.strftime('%d/%m/%Y')}\n        - Fim: {data_fim.strftime('%d/%m/%Y')}\n        - Duração: {kpis['periodo_dias']} dias\n        \"\"\")\n    \n    with info_col2:\n        st.info(f\"\"\"\n        **🚗 Frota:**\n        - Total de Veículos: {kpis['total_veiculos']:,}\n        - Total de Registros: {kpis['total_registros']:,}\n        - Média por Veículo: {kpis['total_registros']/kpis['total_veiculos']:.0f}\n        \"\"\")\n    \n    with info_col3:\n        st.info(f\"\"\"\n        **⚡ Performance:**\n        - Velocidade Média: {kpis['velocidade_media']:.1f} km/h\n        - Distância Total: {kpis['distancia_total']:.0f} km\n        - Tempo Ativo: {kpis['tempo_ativo_horas']:.1f} horas\n        \"\"\")\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":8815},"pages/2_📁_Upload_CSV.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime\nimport sys\n\n# Adicionar o diretório raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.csv_processor import CSVProcessor\nfrom database.db_manager import DatabaseManager\n\nst.set_page_config(\n    page_title=\"Upload CSV - Insight Hub\",\n    page_icon=\"📁\",\n    layout=\"wide\"\n)\n\ndef main():\n    st.title(\"📁 Upload e Processamento de Dados CSV\")\n    st.markdown(\"---\")\n    \n    # Informações sobre o formato esperado\n    with st.expander(\"📋 Formato do Arquivo CSV\", expanded=False):\n        st.markdown(\"\"\"\n        ### Campos Obrigatórios (25 campos):\n        \n        1. **Cliente** - Nome do cliente/empresa\n        2. **Placa** - Placa do veículo\n        3. **Ativo** - Identificador do ativo\n        4. **Data** - Data/hora do evento\n        5. **Data (GPRS)** - Data/hora de comunicação GPRS\n        6. **Velocidade (Km)** - Velocidade em km/h\n        7. **Ignição** - Status da ignição (D/L)\n        8. **Motorista** - Identificação do motorista\n        9. **GPS** - Status do GPS (0/1)\n        10. **Gprs** - Status do GPRS (0/1)\n        11. **Localização** - Coordenadas do veículo\n        12. **Endereço** - Endereço convertido\n        13. **Tipo do Evento** - Tipo do evento registrado\n        14. **Cerca** - Informações de cerca eletrônica\n        15. **Saida** - Status das saídas digitais\n        16. **Entrada** - Status das entradas digitais\n        17. **Pacote** - Informações de pacotes de dados\n        18. **Odômetro do período (Km)** - Km percorridos\n        19. **Horímetro do período** - Tempo de funcionamento\n        20. **Horímetro embarcado** - Contador de horas\n        21. **Odômetro embarcado (Km)** - Odômetro embarcado\n        22. **Bateria** - Nível de bateria\n        23. **Imagem** - Link para imagem (opcional)\n        24. **Tensão** - Tensão elétrica\n        25. **Bloqueado** - Status de bloqueio (0/1)\n        \n        ### Observações:\n        - O arquivo deve estar em formato CSV com separador vírgula (,)\n        - Todos os 25 campos são obrigatórios\n        - Tamanho máximo: 50MB\n        - Até 100.000 registros por arquivo\n        \"\"\")\n    \n    # Área de upload\n    st.subheader(\"📤 Fazer Upload do Arquivo CSV\")\n    \n    uploaded_files = st.file_uploader(\n        \"Selecione os arquivos CSV:\",\n        type=['csv'],\n        help=\"Múltiplos arquivos CSV com dados telemáticos da frota\",\n        accept_multiple_files=True\n    )\n    \n    if uploaded_files:\n        # Mostrar informações dos arquivos\n        st.info(f\"📁 **{len(uploaded_files)} arquivo(s) selecionado(s)**\")\n        \n        total_size = sum(f.size for f in uploaded_files)\n        if total_size > 200 * 1024 * 1024:  # 200MB total\n            st.error(\"❌ Total de arquivos muito grande! Tamanho máximo: 200MB\")\n            st.stop()\n        \n        # Mostrar lista de arquivos\n        for i, file in enumerate(uploaded_files):\n            size_mb = file.size / 1024 / 1024\n            st.write(f\"{i+1}. **{file.name}** - {size_mb:.2f} MB\")\n        \n        # Preview do primeiro arquivo\n        try:\n            first_file = uploaded_files[0]\n            preview_df = None\n            separators = [';', ',']\n            encodings = ['latin-1', 'utf-8', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for sep in separators:\n                for enc in encodings:\n                    try:\n                        first_file.seek(0)\n                        preview_df = pd.read_csv(first_file, sep=sep, encoding=enc, nrows=5)\n                        st.success(f\"📄 Formato detectado: separador '{sep}', encoding '{enc}'\")\n                        break\n                    except:\n                        continue\n                if preview_df is not None:\n                    break\n            \n            if preview_df is not None:\n                st.subheader(\"👀 Preview dos Dados\")\n                st.dataframe(preview_df, width='stretch')\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.metric(\"Colunas encontradas\", len(preview_df.columns))\n                with col2:\n                    st.metric(\"Total de arquivos\", len(uploaded_files))\n            \n            first_file.seek(0)\n            \n            # Botão para processar todos os arquivos\n            if st.button(\"🚀 Processar Todos os Arquivos\", type=\"primary\"):\n                # Ordenar arquivos por tamanho (menor para maior) para processamento mais eficiente\n                sorted_files = sorted(uploaded_files, key=lambda f: f.size)\n                st.info(f\"📊 Arquivos ordenados por tamanho: menor → maior para otimizar processamento\")\n                process_multiple_csv_files(sorted_files)\n                \n        except Exception as e:\n            st.error(f\"❌ Erro ao ler os arquivos: {str(e)}\")\n    \n    # Mostrar histórico de arquivos processados\n    show_processing_history()\n\ndef process_multiple_csv_files(uploaded_files):\n    \"\"\"Processa múltiplos arquivos CSV com progresso detalhado\"\"\"\n    \n    # Container principal para progresso\n    progress_container = st.container()\n    \n    with progress_container:\n        st.markdown(\"### 📊 Progresso de Processamento\")\n        \n        # Progresso geral\n        overall_progress = st.progress(0)\n        overall_status = st.empty()\n        \n        # Progresso do arquivo atual\n        current_file_info = st.empty()\n        file_progress_bar = st.progress(0)\n        file_status = st.empty()\n        \n        # Métricas em tempo real\n        metrics_cols = st.columns(3)\n        with metrics_cols[0]:\n            files_metric = st.empty()\n        with metrics_cols[1]:\n            records_metric = st.empty()\n        with metrics_cols[2]:\n            speed_metric = st.empty()\n    \n    total_files = len(uploaded_files)\n    processed_files = 0\n    total_records = 0\n    failed_files = 0\n    \n    import time\n    start_time = time.time()\n    \n    for i, uploaded_file in enumerate(uploaded_files):\n        file_start_time = time.time()\n        \n        # Atualizar progresso geral\n        overall_progress.progress(i / total_files)\n        overall_status.markdown(f\"📂 **Processando arquivo {i+1} de {total_files}**\")\n        \n        # Informações do arquivo atual\n        file_size_mb = uploaded_file.size / (1024 * 1024)\n        current_file_info.markdown(f\"\"\"\n        📄 **{uploaded_file.name}**  \n        📏 Tamanho: {file_size_mb:.1f} MB\n        \"\"\")\n        \n        # Resetar progresso do arquivo\n        file_progress_bar.progress(0)\n        file_status.markdown(\"🔄 Iniciando processamento...\")\n        \n        # Atualizar métricas\n        files_metric.metric(\n            label=\"📁 Arquivos\",\n            value=f\"{processed_files + failed_files}/{total_files}\",\n            delta=f\"{i}/{total_files}\"\n        )\n        records_metric.metric(\n            label=\"📊 Registros\",\n            value=f\"{total_records:,}\",\n            delta=\"Processando...\"\n        )\n        \n        elapsed_time = time.time() - start_time\n        if elapsed_time > 0:\n            files_per_min = (i / elapsed_time) * 60\n            speed_metric.metric(\n                label=\"⚡ Velocidade\",\n                value=f\"{files_per_min:.1f}\",\n                delta=\"arquivos/min\"\n            )\n        \n        # Processar arquivo com callback de progresso\n        result = process_single_csv_file_with_progress(\n            uploaded_file, \n            file_progress_bar, \n            file_status\n        )\n        \n        # Finalizar progresso do arquivo\n        file_progress_bar.progress(1.0)\n        file_processing_time = time.time() - file_start_time\n        \n        if result['success']:\n            processed_files += 1\n            records_processed = result.get('records_processed', 0)\n            total_records += records_processed\n            \n            file_status.markdown(f\"✅ **Concluído!** {records_processed:,} registros em {file_processing_time:.1f}s\")\n            \n            # Log de sucesso\n            with st.expander(f\"✅ {uploaded_file.name}\", expanded=False):\n                st.success(f\"Processado com sucesso: {records_processed:,} registros\")\n                st.info(f\"Tempo: {file_processing_time:.1f}s | Velocidade: {records_processed/file_processing_time:.0f} reg/s\")\n        else:\n            failed_files += 1\n            file_status.markdown(\"❌ **Erro no processamento**\")\n            \n            # Log de erro\n            with st.expander(f\"❌ {uploaded_file.name}\", expanded=True):\n                st.error(f\"Erro: {result.get('error', 'Erro desconhecido')}\")\n        \n        # Pequena pausa para visualização\n        time.sleep(0.1)\n    \n    # Finalizar progresso geral\n    overall_progress.progress(1.0)\n    total_time = time.time() - start_time\n    \n    overall_status.markdown(\"🎉 **Processamento Finalizado!**\")\n    current_file_info.markdown(\"📁 **Todos os arquivos processados**\")\n    file_status.markdown(f\"⏱️ Tempo total: {total_time:.1f}s\")\n    \n    # Métricas finais\n    files_metric.metric(\n        label=\"📁 Arquivos\",\n        value=f\"{processed_files}/{total_files}\",\n        delta=f\"{failed_files} erros\" if failed_files > 0 else \"Todos OK\"\n    )\n    records_metric.metric(\n        label=\"📊 Registros\",\n        value=f\"{total_records:,}\",\n        delta=\"Processados\"\n    )\n    speed_metric.metric(\n        label=\"⚡ Velocidade média\",\n        value=f\"{total_records/total_time:.0f}\" if total_time > 0 else \"0\",\n        delta=\"registros/s\"\n    )\n    \n    # Resumo final\n    if processed_files == total_files:\n        st.success(f\"\"\"\n        🎉 **Processamento 100% concluído!**\n        - ✅ {processed_files} arquivos processados com sucesso\n        - 📊 {total_records:,} registros inseridos na base de dados\n        - ⏱️ Tempo total: {total_time:.1f} segundos\n        - ⚡ Velocidade média: {total_records/total_time:.0f} registros/segundo\n        \"\"\")\n    else:\n        st.warning(f\"\"\"\n        ⚠️ **Processamento concluído com erros**\n        - ✅ {processed_files} arquivos processados com sucesso\n        - ❌ {failed_files} arquivos com erro\n        - 📊 {total_records:,} registros inseridos na base de dados\n        - ⏱️ Tempo total: {total_time:.1f} segundos\n        \"\"\")\n\ndef process_single_csv_file_with_progress(uploaded_file, progress_bar, status_display):\n    \"\"\"Processa um único arquivo CSV com progresso em tempo real\"\"\"\n    \n    try:\n        import tempfile\n        import time\n        \n        # Fase 1: Salvando arquivo temporário\n        status_display.markdown(\"📥 Salvando arquivo temporário...\")\n        progress_bar.progress(0.1)\n        \n        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:\n            tmp_file.write(uploaded_file.getvalue())\n            tmp_path = tmp_file.name\n        \n        # Fase 2: Analisando estrutura\n        status_display.markdown(\"🔍 Analisando estrutura do arquivo...\")\n        progress_bar.progress(0.2)\n        time.sleep(0.1)\n        \n        # Contagem estimada de linhas para melhor progresso\n        try:\n            with open(tmp_path, 'r', encoding='latin-1') as f:\n                estimated_rows = sum(1 for _ in f) - 1  # -1 para header\n        except:\n            estimated_rows = 1000  # Fallback\n        \n        status_display.markdown(f\"📊 Arquivo com ~{estimated_rows:,} registros detectados\")\n        progress_bar.progress(0.3)\n        \n        # Fase 3: Processando dados\n        status_display.markdown(\"⚙️ Processando e validando dados...\")\n        progress_bar.progress(0.5)\n        time.sleep(0.2)\n        \n        # Fase 4: Inserindo na base de dados\n        status_display.markdown(\"💾 Inserindo registros na base de dados...\")\n        progress_bar.progress(0.7)\n        \n        # Usar DatabaseManager para migrar com progresso\n        def update_progress(current, total, phase):\n            progress = 0.7 + (current / total) * 0.2  # 70% até 90%\n            progress_bar.progress(progress)\n            if phase == \"preparando\":\n                status_display.markdown(f\"⚙️ Preparando dados: {current:,}/{total:,}\")\n            elif phase == \"inserindo\":\n                status_display.markdown(f\"💾 Inserindo registros: {current:,}/{total:,}\")\n        \n        result = DatabaseManager.migrate_csv_to_database_with_progress(tmp_path, update_progress)\n        \n        # Fase 5: Finalizando\n        status_display.markdown(\"🔄 Finalizando processamento...\")\n        progress_bar.progress(0.9)\n        time.sleep(0.1)\n        \n        # Limpar arquivo temporário\n        import os\n        os.unlink(tmp_path)\n        \n        return result\n        \n    except Exception as e:\n        status_display.markdown(f\"❌ Erro: {str(e)}\")\n        return {\n            'success': False,\n            'error': f'Erro no processamento: {str(e)}',\n            'records_processed': 0\n        }\n\ndef process_single_csv_file(uploaded_file):\n    \"\"\"Processa um único arquivo CSV (versão simplificada para compatibilidade)\"\"\"\n    \n    try:\n        import tempfile\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:\n            tmp_file.write(uploaded_file.getvalue())\n            tmp_path = tmp_file.name\n        \n        result = DatabaseManager.migrate_csv_to_database(tmp_path)\n        \n        import os\n        os.unlink(tmp_path)\n        \n        return result\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'error': f'Erro no processamento: {str(e)}',\n            'records_processed': 0\n        }\n\ndef show_processing_summary(summary, filename):\n    \"\"\"Mostra resumo do processamento\"\"\"\n    \n    st.subheader(\"📊 Resumo do Processamento\")\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            label=\"📁 Total de Registros\",\n            value=f\"{summary.get('total_registros', 0):,}\"\n        )\n    \n    with col2:\n        st.metric(\n            label=\"🚗 Total de Veículos\",\n            value=f\"{summary.get('total_veiculos', 0):,}\"\n        )\n    \n    with col3:\n        st.metric(\n            label=\"🏢 Total de Clientes\",\n            value=f\"{summary.get('total_clientes', 0):,}\"\n        )\n    \n    with col4:\n        st.metric(\n            label=\"📅 Período\",\n            value=f\"{(summary.get('periodo_fim', datetime.now()) - summary.get('periodo_inicio', datetime.now())).days + 1} dias\"\n        )\n    \n    # Métricas adicionais\n    st.markdown(\"### 📈 Métricas dos Dados\")\n    \n    col5, col6, col7, col8 = st.columns(4)\n    \n    with col5:\n        st.metric(\n            label=\"⚡ Velocidade Média\",\n            value=f\"{summary.get('velocidade_media', 0):.1f} km/h\"\n        )\n    \n    with col6:\n        st.metric(\n            label=\"🏎️ Velocidade Máxima\",\n            value=f\"{summary.get('velocidade_maxima', 0):.0f} km/h\"\n        )\n    \n    with col7:\n        st.metric(\n            label=\"🛣️ Total KM\",\n            value=f\"{summary.get('total_km_periodo', 0):.0f} km\"\n        )\n    \n    with col8:\n        st.metric(\n            label=\"📡 Com GPS\",\n            value=f\"{summary.get('registros_com_gps', 0):,}\"\n        )\n\ndef save_processing_record(filename, summary):\n    \"\"\"Salva registro do processamento\"\"\"\n    try:\n        os.makedirs('data', exist_ok=True)\n        \n        record = {\n            'timestamp': datetime.now().isoformat(),\n            'filename': filename,\n            'summary': summary\n        }\n        \n        # Carregar histórico existente\n        history_file = 'data/processing_history.csv'\n        \n        if os.path.exists(history_file):\n            history_df = pd.read_csv(history_file)\n        else:\n            history_df = pd.DataFrame()\n        \n        # Adicionar novo registro\n        new_record = pd.DataFrame([{\n            'timestamp': record['timestamp'],\n            'filename': filename,\n            'total_registros': summary.get('total_registros', 0),\n            'total_veiculos': summary.get('total_veiculos', 0),\n            'total_clientes': summary.get('total_clientes', 0),\n            'periodo_inicio': summary.get('periodo_inicio', ''),\n            'periodo_fim': summary.get('periodo_fim', ''),\n            'velocidade_media': summary.get('velocidade_media', 0),\n            'total_km': summary.get('total_km_periodo', 0)\n        }])\n        \n        history_df = pd.concat([history_df, new_record], ignore_index=True)\n        history_df.to_csv(history_file, index=False)\n        \n    except Exception as e:\n        st.warning(f\"⚠️ Não foi possível salvar o registro: {str(e)}\")\n\ndef show_processing_history():\n    \"\"\"Mostra histórico de processamentos da base de dados\"\"\"\n    try:\n        # Tentar buscar histórico da base de dados com retry\n        history_records = None\n        retry_count = 0\n        max_retries = 3\n        \n        while retry_count < max_retries:\n            try:\n                history_records = DatabaseManager.get_processing_history()\n                break  # Success - exit retry loop\n            except Exception as retry_e:\n                retry_count += 1\n                if retry_count >= max_retries:\n                    # Se todas as tentativas falharam, mostrar mensagem amigável\n                    st.warning(\"⚠️ Problema temporário de conexão com a base de dados. O histórico não pode ser carregado no momento.\")\n                    if st.button(\"🔄 Tentar Novamente\"):\n                        st.rerun()\n                    return\n                else:\n                    # Aguardar um pouco antes da próxima tentativa\n                    import time\n                    time.sleep(0.5)\n        \n        if not history_records:\n            st.info(\"📋 Nenhum histórico de processamento encontrado.\")\n            return\n        \n        st.markdown(\"### 📋 Histórico de Processamento\")\n        \n        # Converter para DataFrame para exibição\n        history_df = pd.DataFrame(history_records)\n        \n        # Formatação dos dados\n        if not history_df.empty:\n            # Renomear colunas para português\n            history_df = history_df.rename(columns={\n                'filename': 'Nome do Arquivo',\n                'upload_timestamp': 'Data/Hora',\n                'records_processed': 'Registros',\n                'unique_vehicles': 'Veículos',\n                'unique_clients': 'Clientes',\n                'processing_status': 'Status',\n                'file_size_bytes': 'Tamanho (bytes)'\n            })\n            \n            # Formatar a coluna de data/hora\n            if 'Data/Hora' in history_df.columns:\n                history_df['Data/Hora'] = pd.to_datetime(history_df['Data/Hora']).dt.strftime('%d/%m/%Y %H:%M:%S')\n            \n            # Formatar status\n            if 'Status' in history_df.columns:\n                history_df['Status'] = history_df['Status'].map({\n                    'completed': '✅ Concluído',\n                    'failed': '❌ Erro',\n                    'processing': '🔄 Processando'\n                }).fillna('❓ Desconhecido')\n            \n            # Formatar números\n            for col in ['Registros', 'Veículos', 'Clientes']:\n                if col in history_df.columns:\n                    history_df[col] = history_df[col].apply(lambda x: f\"{x:,}\" if pd.notnull(x) else \"0\")\n            \n            # Ordenar por data mais recente\n            if 'Data/Hora' in history_df.columns:\n                history_df = history_df.sort_values('Data/Hora', ascending=False)\n            \n            # Mostrar tabela\n            st.dataframe(\n                history_df[['Nome do Arquivo', 'Data/Hora', 'Registros', 'Veículos', 'Clientes', 'Status']], \n                use_container_width=True,\n                hide_index=True\n            )\n            \n            # Estatísticas gerais do histórico\n            total_records = sum([int(r.get('records_processed', 0)) for r in history_records])\n            total_files = len(history_records)\n            \n            col1, col2, col3 = st.columns(3)\n            with col1:\n                st.metric(\"📁 Arquivos Processados\", total_files)\n            with col2:\n                st.metric(\"📊 Total de Registros\", f\"{total_records:,}\")\n            with col3:\n                successful_files = len([r for r in history_records if r.get('processing_status') == 'completed'])\n                st.metric(\"✅ Taxa de Sucesso\", f\"{(successful_files/total_files*100):.1f}%\" if total_files > 0 else \"0%\")\n        \n    except Exception as e:\n        st.error(f\"❌ Erro ao carregar histórico: {str(e)}\")\n        \n    # Botões de ação  \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        if st.button(\"🗑️ Limpar Histórico\", type=\"secondary\", help=\"Remove apenas os registros do histórico (mantém os dados)\"):\n            # Implementar limpeza do histórico se necessário\n            st.info(\"Funcionalidade de limpeza de histórico será implementada.\")\n    \n    with col2:\n        # Inicializar session state se não existir\n        if \"confirm_clear_data\" not in st.session_state:\n            st.session_state.confirm_clear_data = False\n        \n        if st.button(\"🗂️ Limpar Todos os Dados\", type=\"secondary\", help=\"Remove TODOS os dados da base (histórico + registros)\"):\n            st.session_state.confirm_clear_data = True\n        \n        # Mostrar confirmação se solicitada\n        if st.session_state.confirm_clear_data:\n            st.warning(\"⚠️ **ATENÇÃO**: Esta ação irá remover TODOS os dados da base de dados!\")\n            \n            col_conf1, col_conf2 = st.columns(2)\n            \n            with col_conf1:\n                if st.button(\"✅ SIM, Limpar Tudo\", type=\"primary\"):\n                    try:\n                        with st.spinner(\"🔄 Limpando todos os dados...\"):\n                            # Clear all data from database\n                            result = DatabaseManager.clear_all_data()\n                        \n                        st.success(f\"\"\"\n                        🎉 **Limpeza completa realizada com sucesso!**\n                        \n                        **Dados removidos:**\n                        - 🗂️ {result.get('telematics_data', 0):,} registros telemétricos\n                        - 📋 {result.get('processing_history', 0)} registros de histórico  \n                        - 🚗 {result.get('vehicles', 0)} veículos\n                        - 🏢 {result.get('clients', 0)} clientes\n                        \n                        **Sistema resetado!** Agora você pode fazer novos uploads.\n                        \"\"\")\n                        \n                        # Reset session state\n                        st.session_state.confirm_clear_data = False\n                        \n                        # Recarregar a página para refletir mudanças\n                        st.rerun()\n                        \n                    except Exception as e:\n                        st.error(f\"❌ Erro ao limpar dados: {str(e)}\")\n                        st.session_state.confirm_clear_data = False\n            \n            with col_conf2:\n                if st.button(\"❌ Cancelar\", type=\"secondary\"):\n                    st.session_state.confirm_clear_data = False\n                    st.rerun()\n\ndef show_processing_history_old():\n    \"\"\"Mostra histórico de processamentos (versão antiga usando arquivo)\"\"\"\n    history_file = 'data/processing_history.csv'\n    \n    if not os.path.exists(history_file):\n        st.info(\"📋 Nenhum histórico de processamento encontrado.\")\n        return\n    \n    try:\n        history_df = pd.read_csv(history_file)\n        \n        if history_df.empty:\n            st.info(\"📋 Nenhum arquivo processado ainda.\")\n            return\n        \n        st.subheader(\"📋 Histórico de Processamentos\")\n        \n        # Ordenar por timestamp mais recente\n        history_df['timestamp'] = pd.to_datetime(history_df['timestamp'])\n        history_df = history_df.sort_values('timestamp', ascending=False)\n        \n        # Formatar para exibição\n        display_df = history_df.copy()\n        display_df['timestamp'] = display_df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')\n        display_df = display_df.rename(columns={\n            'timestamp': 'Data/Hora',\n            'filename': 'Arquivo',\n            'total_registros': 'Registros',\n            'total_veiculos': 'Veículos',\n            'total_clientes': 'Clientes',\n            'velocidade_media': 'Vel. Média',\n            'total_km': 'Total KM'\n        })\n        \n        # Selecionar colunas para exibição\n        display_columns = ['Data/Hora', 'Arquivo', 'Registros', 'Veículos', 'Clientes', 'Vel. Média', 'Total KM']\n        \n        st.dataframe(\n            display_df[display_columns].head(10),\n            use_container_width=True,\n            hide_index=True\n        )\n        \n        # Botões de ação\n        col_btn1, col_btn2 = st.columns(2)\n        \n        with col_btn1:\n            if st.button(\"🗑️ Limpar Histórico\", type=\"secondary\"):\n                if os.path.exists(history_file):\n                    os.remove(history_file)\n                    st.success(\"✅ Histórico limpo com sucesso!\")\n                    st.rerun()\n        \n        with col_btn2:\n            # Botão para limpar todos os dados da base PostgreSQL\n            if st.button(\"🗂️ Limpar Todos os Dados\", type=\"secondary\"):\n                if 'confirm_clear_all' not in st.session_state:\n                    st.session_state.confirm_clear_all = False\n                \n                if not st.session_state.confirm_clear_all:\n                    st.warning(\"⚠️ Isso removerá TODOS os dados da base PostgreSQL!\")\n                    if st.button(\"⚠️ CONFIRMAR LIMPEZA TOTAL\", type=\"primary\"):\n                        st.session_state.confirm_clear_all = True\n                        st.rerun()\n                else:\n                    # Executar limpeza da base PostgreSQL\n                    result = DatabaseManager.clear_all_data()\n                    if result:\n                        st.success(\"✅ Todos os dados foram removidos da base PostgreSQL!\")\n                        st.session_state.confirm_clear_all = False\n                        st.rerun()\n                    else:\n                        st.error(\"❌ Erro ao limpar dados da base PostgreSQL\")\n        \n    except Exception as e:\n        error_message = str(e)\n        if \"SSL connection has been closed unexpectedly\" in error_message:\n            st.warning(\"⚠️ Problema temporário de conexão SSL com a base de dados. Tente recarregar a página.\")\n            if st.button(\"🔄 Recarregar Página\"):\n                st.rerun()\n        elif \"psycopg2.OperationalError\" in error_message:\n            st.warning(\"⚠️ Problema de conexão com a base de dados PostgreSQL. Verifique se o sistema está funcionando corretamente.\")\n            if st.button(\"🔄 Tentar Novamente\"):\n                st.rerun()\n        else:\n            st.error(f\"❌ Erro ao carregar histórico: {error_message}\")\n            if st.button(\"🔄 Tentar Novamente\"):\n                st.rerun()\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":27406},"pages/3_🔍_Análise_Detalhada.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport sys\n\n# Adicionar o diretório raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.visualizations import FleetVisualizations\n\nst.set_page_config(\n    page_title=\"Análise Detalhada - Insight Hub\",\n    page_icon=\"🔍\",\n    layout=\"wide\"\n)\n\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais)\"\"\"\n    try:\n        # Importar DatabaseManager\n        from database.db_manager import DatabaseManager\n        \n        # Carregar APENAS da base de dados - sem fallbacks fictícios\n        if DatabaseManager.has_data():\n            df = DatabaseManager.get_dashboard_data()\n            if not df.empty:\n                st.success(f\"✅ Dados reais carregados: {len(df):,} registros da base de dados\")\n                return df\n        \n        # Se não há dados reais, mostrar mensagem clara\n        st.warning(\"⚠️ Nenhum dado real encontrado na base de dados. Faça upload dos seus arquivos CSV.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados reais: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"🔍 Análise Detalhada da Frota\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"📁 Nenhum dado encontrado. Faça o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados da base de dados\n    analyzer = DataAnalyzer.from_database()\n    visualizer = FleetVisualizations(analyzer)\n    \n    # Sidebar com filtros avançados\n    st.sidebar.header(\"🔍 Filtros Avançados\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por período\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    data_range = st.sidebar.date_input(\n        \"Período:\",\n        value=[min_date, max_date],\n        min_value=min_date,\n        max_value=max_date\n    )\n    \n    if len(data_range) == 2:\n        data_inicio, data_fim = data_range\n    else:\n        data_inicio = data_range[0]\n        data_fim = max_date\n    \n    # Filtro por múltiplos veículos\n    veiculos_disponiveis = []\n    if cliente_selecionado != \"Todos\":\n        veiculos_disponiveis = sorted(df[df['cliente'] == cliente_selecionado]['placa'].unique().tolist())\n    else:\n        veiculos_disponiveis = sorted(df['placa'].unique().tolist())\n    \n    veiculos_selecionados = st.sidebar.multiselect(\n        \"Veículos:\",\n        veiculos_disponiveis,\n        default=veiculos_disponiveis[:5] if len(veiculos_disponiveis) > 5 else veiculos_disponiveis\n    )\n    \n    # Filtros de velocidade\n    st.sidebar.subheader(\"⚡ Filtros de Velocidade\")\n    velocidade_min = st.sidebar.number_input(\"Velocidade Mínima (km/h):\", min_value=0, max_value=200, value=0)\n    velocidade_max = st.sidebar.number_input(\"Velocidade Máxima (km/h):\", min_value=0, max_value=200, value=200)\n    \n    # Aplicar filtros\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    # Filtrar por veículos selecionados\n    if veiculos_selecionados:\n        filtered_df = filtered_df[filtered_df['placa'].isin(veiculos_selecionados)]\n    \n    # Filtrar por velocidade\n    filtered_df = filtered_df[\n        (filtered_df['velocidade_km'] >= velocidade_min) & \n        (filtered_df['velocidade_km'] <= velocidade_max)\n    ]\n    \n    if filtered_df.empty:\n        st.warning(\"⚠️ Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Atualizar analyzer com dados filtrados\n    analyzer.filtered_df = filtered_df\n    \n    # Tabs para diferentes análises\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n        \"📊 Visão Geral\",\n        \"⚡ Análise de Velocidade\", \n        \"🛣️ Análise Operacional\",\n        \"📡 Compliance\",\n        \"⏰ Padrões Temporais\"\n    ])\n    \n    with tab1:\n        show_overview_analysis(analyzer)\n    \n    with tab2:\n        show_speed_analysis(analyzer)\n    \n    with tab3:\n        show_operational_analysis(analyzer)\n    \n    with tab4:\n        show_compliance_analysis(analyzer)\n    \n    with tab5:\n        show_temporal_patterns(analyzer)\n\ndef show_overview_analysis(analyzer):\n    \"\"\"Mostra análise geral\"\"\"\n    st.header(\"📊 Visão Geral dos Dados Filtrados\")\n    \n    kpis = analyzer.get_kpis()\n    \n    # Verificar se há KPIs válidos\n    if not kpis:\n        st.warning(\"⚠️ Não foi possível calcular métricas. Verifique se há dados para os filtros aplicados.\")\n        return\n    \n    # Métricas principais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"🚗 Veículos\", f\"{kpis['total_veiculos']:,}\")\n    \n    with col2:\n        st.metric(\"📊 Registros\", f\"{kpis['total_registros']:,}\")\n    \n    with col3:\n        st.metric(\"⚡ Vel. Média\", f\"{kpis['velocidade_media']:.1f} km/h\")\n    \n    with col4:\n        st.metric(\"🛣️ Distância\", f\"{kpis['distancia_total']:.0f} km\")\n    \n    # Gráficos de distribuição\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"📈 Atividade por Veículo\")\n        \n        vehicle_activity = analyzer.filtered_df.groupby('placa').size().sort_values(ascending=False).head(15)\n        \n        fig_activity = px.bar(\n            x=vehicle_activity.values,\n            y=vehicle_activity.index,\n            orientation='h',\n            title='Top 15 Veículos por Atividade',\n            labels={'x': 'Número de Registros', 'y': 'Placa'}\n        )\n        fig_activity.update_layout(height=500)\n        st.plotly_chart(fig_activity, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"🎯 Distribuição de Clientes\")\n        \n        client_dist = analyzer.filtered_df['cliente'].value_counts()\n        \n        fig_clients = px.pie(\n            values=client_dist.values,\n            names=client_dist.index,\n            title='Distribuição por Cliente'\n        )\n        fig_clients.update_layout(height=500)\n        st.plotly_chart(fig_clients, use_container_width=True)\n    \n    # Estatísticas detalhadas por veículo\n    st.subheader(\"📋 Estatísticas Detalhadas por Veículo\")\n    \n    vehicle_stats = analyzer.filtered_df.groupby('placa').agg({\n        'velocidade_km': ['count', 'mean', 'max', 'std'],\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.sum() / len(x)) * 100,\n        'bloqueado': lambda x: x.sum()\n    }).round(2)\n    \n    # Achatar MultiIndex\n    vehicle_stats.columns = [\n        'Registros', 'Vel. Média', 'Vel. Máxima', 'Vel. Desvio',\n        'KM Total', 'GPS (%)', 'Bloqueios'\n    ]\n    \n    vehicle_stats = vehicle_stats.sort_values('Registros', ascending=False)\n    \n    # Adicionar classificação de performance\n    vehicle_stats['Classificação'] = vehicle_stats.apply(\n        lambda row: '🏆 Excelente' if row['GPS (%)'] > 95 and row['Vel. Média'] < 60 and row['Bloqueios'] == 0\n        else '✅ Bom' if row['GPS (%)'] > 90 and row['Vel. Média'] < 70\n        else '⚠️ Atenção' if row['GPS (%)'] > 80\n        else '❌ Crítico', axis=1\n    )\n    \n    st.dataframe(\n        vehicle_stats,\n        use_container_width=True,\n        height=400\n    )\n    \n    # Download dos dados\n    csv_data = vehicle_stats.to_csv().encode('utf-8')\n    st.download_button(\n        label=\"📥 Download Estatísticas CSV\",\n        data=csv_data,\n        file_name=f\"estatisticas_veiculos_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\",\n        mime=\"text/csv\"\n    )\n\ndef show_speed_analysis(analyzer):\n    \"\"\"Análise detalhada de velocidade\"\"\"\n    st.header(\"⚡ Análise Detalhada de Velocidade\")\n    \n    speed_analysis = analyzer.get_speed_analysis()\n    \n    if not speed_analysis:\n        st.warning(\"Dados insuficientes para análise de velocidade.\")\n        return\n    \n    # Métricas de velocidade\n    col1, col2, col3, col4 = st.columns(4)\n    \n    df = analyzer.filtered_df\n    \n    with col1:\n        st.metric(\"⚡ Velocidade Média\", f\"{df['velocidade_km'].mean():.1f} km/h\")\n    \n    with col2:\n        st.metric(\"🏎️ Velocidade Máxima\", f\"{df['velocidade_km'].max():.0f} km/h\")\n    \n    with col3:\n        st.metric(\"🐌 Velocidade Mínima\", f\"{df['velocidade_km'].min():.0f} km/h\")\n    \n    with col4:\n        st.metric(\"📊 Desvio Padrão\", f\"{df['velocidade_km'].std():.1f} km/h\")\n    \n    # Gráficos de velocidade\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"📊 Distribuição por Faixas\")\n        \n        if 'distribuicao' in speed_analysis:\n            dist_data = speed_analysis['distribuicao']\n            \n            fig_ranges = px.pie(\n                values=dist_data.values,\n                names=dist_data.index,\n                title='Distribuição por Faixas de Velocidade'\n            )\n            st.plotly_chart(fig_ranges, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"⏰ Velocidade por Hora\")\n        \n        if 'velocidade_por_hora' in speed_analysis:\n            hourly_speed = speed_analysis['velocidade_por_hora']\n            \n            fig_hourly = px.line(\n                x=hourly_speed.index,\n                y=hourly_speed.values,\n                title='Velocidade Média por Hora do Dia',\n                labels={'x': 'Hora', 'y': 'Velocidade Média (km/h)'}\n            )\n            st.plotly_chart(fig_hourly, use_container_width=True)\n    \n    # Top veículos por velocidade\n    st.subheader(\"🏎️ Ranking de Velocidade por Veículo\")\n    \n    col_speed1, col_speed2 = st.columns(2)\n    \n    with col_speed1:\n        st.write(\"**Maiores Velocidades Médias:**\")\n        \n        if 'velocidade_media_por_veiculo' in speed_analysis:\n            top_speed_avg = speed_analysis['velocidade_media_por_veiculo'].head(10)\n            \n            fig_avg = px.bar(\n                x=top_speed_avg.values,\n                y=top_speed_avg.index,\n                orientation='h',\n                title='Top 10 - Velocidade Média',\n                labels={'x': 'Velocidade Média (km/h)', 'y': 'Placa'}\n            )\n            st.plotly_chart(fig_avg, use_container_width=True)\n    \n    with col_speed2:\n        st.write(\"**Maiores Velocidades Máximas:**\")\n        \n        if 'velocidade_maxima_por_veiculo' in speed_analysis:\n            top_speed_max = speed_analysis['velocidade_maxima_por_veiculo'].head(10)\n            \n            fig_max = px.bar(\n                x=top_speed_max.values,\n                y=top_speed_max.index,\n                orientation='h',\n                title='Top 10 - Velocidade Máxima',\n                labels={'x': 'Velocidade Máxima (km/h)', 'y': 'Placa'},\n                color=top_speed_max.values,\n                color_continuous_scale='Reds'\n            )\n            st.plotly_chart(fig_max, use_container_width=True)\n    \n    # Análise de excesso de velocidade\n    st.subheader(\"🚨 Análise de Excesso de Velocidade\")\n    \n    speed_limit = st.slider(\"Definir Limite de Velocidade (km/h):\", 40, 120, 80)\n    \n    violations = df[df['velocidade_km'] > speed_limit]\n    \n    col_viol1, col_viol2, col_viol3 = st.columns(3)\n    \n    with col_viol1:\n        st.metric(\"🚨 Total de Violações\", f\"{len(violations):,}\")\n    \n    with col_viol2:\n        st.metric(\"📊 % do Total\", f\"{(len(violations)/len(df)*100):.1f}%\")\n    \n    with col_viol3:\n        violating_vehicles = violations['placa'].nunique()\n        st.metric(\"🚗 Veículos Envolvidos\", f\"{violating_vehicles:,}\")\n    \n    if not violations.empty:\n        # Violações por veículo\n        violations_by_vehicle = violations.groupby('placa').size().sort_values(ascending=False).head(10)\n        \n        fig_violations = px.bar(\n            x=violations_by_vehicle.values,\n            y=violations_by_vehicle.index,\n            orientation='h',\n            title=f'Top 10 - Violações acima de {speed_limit} km/h',\n            labels={'x': 'Número de Violações', 'y': 'Placa'},\n            color=violations_by_vehicle.values,\n            color_continuous_scale='Reds'\n        )\n        st.plotly_chart(fig_violations, use_container_width=True)\n\ndef show_operational_analysis(analyzer):\n    \"\"\"Análise operacional detalhada\"\"\"\n    st.header(\"🛣️ Análise Operacional\")\n    \n    operational = analyzer.get_operational_analysis()\n    \n    if not operational:\n        st.warning(\"Dados insuficientes para análise operacional.\")\n        return\n    \n    # Estatísticas operacionais\n    df = analyzer.filtered_df\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        total_km = df['odometro_periodo_km'].sum()\n        st.metric(\"🛣️ Total KM\", f\"{total_km:.0f} km\")\n    \n    with col2:\n        if 'horimetro_periodo_horas' in df.columns:\n            total_hours = df['horimetro_periodo_horas'].sum()\n        else:\n            total_hours = 0\n        st.metric(\"⏰ Horas Ativas\", f\"{total_hours:.1f} h\")\n    \n    with col3:\n        avg_km_per_vehicle = total_km / df['placa'].nunique() if df['placa'].nunique() > 0 else 0\n        st.metric(\"📊 KM por Veículo\", f\"{avg_km_per_vehicle:.1f} km\")\n    \n    with col4:\n        gps_coverage = (df['gps'].sum() / len(df)) * 100\n        st.metric(\"📡 Cobertura GPS\", f\"{gps_coverage:.1f}%\")\n    \n    # Gráficos operacionais\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"📈 Quilometragem por Veículo\")\n        \n        km_by_vehicle = df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False).head(15)\n        \n        fig_km = px.bar(\n            x=km_by_vehicle.values,\n            y=km_by_vehicle.index,\n            orientation='h',\n            title='Top 15 - Quilometragem Total',\n            labels={'x': 'Quilometragem (km)', 'y': 'Placa'}\n        )\n        st.plotly_chart(fig_km, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"⏰ Utilização por Hora\")\n        \n        hourly_usage = df.groupby(df['data'].dt.hour).agg({\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        }).reset_index()\n        \n        fig_hourly = px.bar(\n            hourly_usage,\n            x='data',\n            y='placa',\n            title='Veículos Ativos por Hora',\n            labels={'data': 'Hora', 'placa': 'Número de Veículos'}\n        )\n        st.plotly_chart(fig_hourly, use_container_width=True)\n    \n    # Análise de eficiência\n    st.subheader(\"📊 Análise de Eficiência\")\n    \n    efficiency_data = []\n    for placa in df['placa'].unique():\n        vehicle_data = df[df['placa'] == placa]\n        \n        total_records = len(vehicle_data)\n        total_km = vehicle_data['odometro_periodo_km'].sum()\n        avg_speed = vehicle_data['velocidade_km'].mean()\n        gps_coverage = (vehicle_data['gps'].mean()) * 100\n        \n        # Tempo parado\n        stopped_time = len(vehicle_data[vehicle_data['velocidade_km'] == 0]) / total_records * 100\n        \n        # Score de eficiência\n        efficiency_score = (\n            (avg_speed / 80 * 30) +  # Velocidade adequada (30%)\n            (gps_coverage) * 0.3 +    # Cobertura GPS (30%)\n            ((100 - stopped_time) * 0.2) + # Tempo ativo (20%)\n            (min(total_km / 1000, 1) * 20)  # Produtividade KM (20%)\n        )\n        \n        efficiency_data.append({\n            'Placa': placa,\n            'Registros': total_records,\n            'Total KM': total_km,\n            'Vel. Média': avg_speed,\n            'GPS (%)': gps_coverage,\n            'Tempo Parado (%)': stopped_time,\n            'Score Eficiência': min(efficiency_score, 100)\n        })\n    \n    efficiency_df = pd.DataFrame(efficiency_data)\n    efficiency_df = efficiency_df.sort_values('Score Eficiência', ascending=False)\n    \n    # Colorir por score de eficiência\n    def color_efficiency(val):\n        if val >= 80:\n            return 'background-color: #d4edda'  # Verde claro\n        elif val >= 60:\n            return 'background-color: #fff3cd'  # Amarelo claro\n        else:\n            return 'background-color: #f8d7da'  # Vermelho claro\n    \n    styled_df = efficiency_df.style.applymap(color_efficiency, subset=['Score Eficiência'])\n    \n    st.dataframe(styled_df, use_container_width=True, height=400)\n    \n    # Top performers\n    col_top1, col_top2 = st.columns(2)\n    \n    with col_top1:\n        st.subheader(\"🏆 Mais Eficientes\")\n        top_efficient = efficiency_df.head(5)\n        for _, row in top_efficient.iterrows():\n            st.success(f\"**{row['Placa']}** - Score: {row['Score Eficiência']:.1f}%\")\n    \n    with col_top2:\n        st.subheader(\"⚠️ Necessitam Atenção\")\n        low_efficient = efficiency_df.tail(5)\n        for _, row in low_efficient.iterrows():\n            st.warning(f\"**{row['Placa']}** - Score: {row['Score Eficiência']:.1f}%\")\n\ndef show_compliance_analysis(analyzer):\n    \"\"\"Análise de compliance\"\"\"\n    st.header(\"📡 Análise de Compliance\")\n    \n    compliance = analyzer.get_compliance_analysis()\n    \n    if not compliance:\n        st.warning(\"Dados insuficientes para análise de compliance.\")\n        return\n    \n    # Métricas de compliance\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"🚨 Violações Velocidade\", f\"{compliance.get('violacoes_velocidade', 0):,}\")\n    \n    with col2:\n        st.metric(\"📡 Problemas GPS\", f\"{compliance.get('veiculos_baixo_gps', 0):,}\")\n    \n    with col3:\n        st.metric(\"🔒 Veículos Bloqueados\", f\"{compliance.get('veiculos_bloqueados', 0):,}\")\n    \n    with col4:\n        if compliance.get('score_compliance'):\n            avg_score = sum(compliance['score_compliance'].values()) / len(compliance['score_compliance'])\n            st.metric(\"📊 Score Médio\", f\"{avg_score:.1f}%\")\n    \n    # Gráficos de compliance\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"🎯 Score de Compliance por Veículo\")\n        \n        if compliance.get('score_compliance'):\n            scores_df = pd.DataFrame(\n                list(compliance['score_compliance'].items()),\n                columns=['Placa', 'Score']\n            ).sort_values('Score', ascending=True).tail(15)\n            \n            # Definir cores baseadas no score\n            colors = ['#d62728' if score < 70 else '#ff7f0e' if score < 90 else '#2ca02c' \n                     for score in scores_df['Score']]\n            \n            fig_scores = px.bar(\n                scores_df,\n                x='Score',\n                y='Placa',\n                orientation='h',\n                title='Score de Compliance (Bottom 15)',\n                color=scores_df['Score'],\n                color_continuous_scale='RdYlGn'\n            )\n            st.plotly_chart(fig_scores, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"📡 Cobertura GPS por Veículo\")\n        \n        if 'cobertura_gps_por_veiculo' in compliance:\n            gps_coverage = compliance['cobertura_gps_por_veiculo'].head(15)\n            \n            fig_gps = px.bar(\n                x=gps_coverage.values,\n                y=gps_coverage.index,\n                orientation='h',\n                title='Cobertura GPS (Bottom 15)',\n                color=gps_coverage.values,\n                color_continuous_scale='RdYlGn',\n                labels={'x': 'Cobertura GPS (%)', 'y': 'Placa'}\n            )\n            st.plotly_chart(fig_gps, use_container_width=True)\n    \n    # Detalhes de violações\n    if 'detalhes_violacoes' in compliance and not compliance['detalhes_violacoes'].empty:\n        st.subheader(\"🚨 Detalhes das Violações de Velocidade\")\n        \n        violations_detail = compliance['detalhes_violacoes'].head(15).reset_index()\n        violations_detail.columns = ['Placa', 'Número de Violações']\n        \n        fig_violations = px.bar(\n            violations_detail,\n            x='Número de Violações',\n            y='Placa',\n            orientation='h',\n            title='Top 15 - Violações de Velocidade',\n            color='Número de Violações',\n            color_continuous_scale='Reds'\n        )\n        st.plotly_chart(fig_violations, use_container_width=True)\n    \n    # Recomendações de compliance\n    st.subheader(\"💡 Recomendações de Melhoria\")\n    \n    recommendations = []\n    \n    if compliance.get('violacoes_velocidade', 0) > 0:\n        recommendations.append(\"🚨 **Controle de Velocidade**: Implementar treinamento de condutores sobre limites de velocidade\")\n    \n    if compliance.get('veiculos_baixo_gps', 0) > 0:\n        recommendations.append(\"📡 **Melhoria GPS**: Verificar equipamentos e cobertura de sinal GPS\")\n    \n    if compliance.get('veiculos_bloqueados', 0) > 0:\n        recommendations.append(\"🔒 **Revisão de Bloqueios**: Analisar motivos de bloqueio e normalizar situação\")\n    \n    if compliance.get('score_compliance'):\n        low_score_vehicles = [k for k, v in compliance['score_compliance'].items() if v < 70]\n        if low_score_vehicles:\n            recommendations.append(f\"⚠️ **Atenção Especial**: {len(low_score_vehicles)} veículos com score baixo precisam de ação imediata\")\n    \n    if recommendations:\n        for rec in recommendations:\n            st.info(rec)\n    else:\n        st.success(\"✅ **Excelente!** A frota está em conformidade com os padrões estabelecidos.\")\n\ndef show_temporal_patterns(analyzer):\n    \"\"\"Análise de padrões temporais\"\"\"\n    st.header(\"⏰ Padrões Temporais de Uso\")\n    \n    patterns = analyzer.get_temporal_patterns()\n    df = analyzer.filtered_df\n    \n    if not patterns or df.empty:\n        st.warning(\"Dados insuficientes para análise temporal.\")\n        return\n    \n    # Padrões por hora do dia\n    st.subheader(\"🕐 Padrões por Hora do Dia\")\n    \n    hourly_data = df.groupby(df['data'].dt.hour).agg({\n        'placa': 'nunique',\n        'velocidade_km': 'mean',\n        'odometro_periodo_km': 'sum'\n    }).reset_index()\n    \n    col_hour1, col_hour2 = st.columns(2)\n    \n    with col_hour1:\n        fig_hourly_vehicles = px.line(\n            hourly_data,\n            x='data',\n            y='placa',\n            title='Veículos Ativos por Hora',\n            labels={'data': 'Hora', 'placa': 'Número de Veículos'}\n        )\n        st.plotly_chart(fig_hourly_vehicles, use_container_width=True)\n    \n    with col_hour2:\n        fig_hourly_speed = px.line(\n            hourly_data,\n            x='data',\n            y='velocidade_km',\n            title='Velocidade Média por Hora',\n            labels={'data': 'Hora', 'velocidade_km': 'Velocidade (km/h)'}\n        )\n        st.plotly_chart(fig_hourly_speed, use_container_width=True)\n    \n    # Padrões por dia da semana\n    # Definir mapeamento de dias para português (usado em múltiplos lugares)\n    dias_ordem = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    dias_pt = ['Segunda', 'Terça', 'Quarta', 'Quinta', 'Sexta', 'Sábado', 'Domingo']\n    \n    if len(df['data'].dt.date.unique()) > 7:  # Mais de uma semana de dados\n        st.subheader(\"📅 Padrões por Dia da Semana\")\n        \n        df['dia_semana'] = df['data'].dt.day_name()\n        \n        weekly_data = df.groupby('dia_semana').agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean',\n            'odometro_periodo_km': 'sum'\n        }).reindex(dias_ordem).reset_index()\n        \n        weekly_data['dia_semana'] = dias_pt\n        \n        col_week1, col_week2 = st.columns(2)\n        \n        with col_week1:\n            fig_weekly_vehicles = px.bar(\n                weekly_data,\n                x='dia_semana',\n                y='placa',\n                title='Veículos Ativos por Dia da Semana',\n                labels={'dia_semana': 'Dia da Semana', 'placa': 'Número de Veículos'}\n            )\n            st.plotly_chart(fig_weekly_vehicles, use_container_width=True)\n        \n        with col_week2:\n            fig_weekly_km = px.bar(\n                weekly_data,\n                x='dia_semana',\n                y='odometro_periodo_km',\n                title='Quilometragem por Dia da Semana',\n                labels={'dia_semana': 'Dia da Semana', 'odometro_periodo_km': 'KM Total'}\n            )\n            st.plotly_chart(fig_weekly_km, use_container_width=True)\n    \n    # Heatmap de atividade\n    st.subheader(\"🔥 Mapa de Calor - Atividade por Hora e Dia\")\n    \n    # Criar dados para heatmap\n    df['hora'] = df['data'].dt.hour\n    df['dia'] = df['data'].dt.day_name()\n    \n    heatmap_data = df.groupby(['dia', 'hora']).size().unstack(fill_value=0)\n    \n    # Reordenar dias da semana\n    dias_ordem = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    heatmap_data = heatmap_data.reindex(dias_ordem)\n    heatmap_data.index = dias_pt\n    \n    fig_heatmap = px.imshow(\n        heatmap_data,\n        title='Atividade da Frota (Registros por Hora/Dia)',\n        labels={'x': 'Hora do Dia', 'y': 'Dia da Semana', 'color': 'Número de Registros'},\n        aspect='auto'\n    )\n    \n    st.plotly_chart(fig_heatmap, use_container_width=True)\n    \n    # Insights temporais\n    st.subheader(\"💡 Insights Temporais\")\n    \n    # Pico de atividade\n    peak_hour = hourly_data.loc[hourly_data['placa'].idxmax(), 'data']\n    peak_vehicles = hourly_data['placa'].max()\n    \n    st.info(f\"🕐 **Pico de Atividade**: {peak_hour}h com {peak_vehicles} veículos ativos\")\n    \n    # Período de menor atividade\n    low_hour = hourly_data.loc[hourly_data['placa'].idxmin(), 'data']\n    low_vehicles = hourly_data['placa'].min()\n    \n    st.info(f\"😴 **Menor Atividade**: {low_hour}h com {low_vehicles} veículos ativos\")\n    \n    # Horário de maior velocidade média\n    fastest_hour = hourly_data.loc[hourly_data['velocidade_km'].idxmax(), 'data']\n    fastest_speed = hourly_data['velocidade_km'].max()\n    \n    st.info(f\"🏎️ **Maior Velocidade Média**: {fastest_hour}h com {fastest_speed:.1f} km/h\")\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":26415},"pages/4_📈_Comparação_Veículos.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport sys\n\n# Adicionar o diretório raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.visualizations import FleetVisualizations\n\nst.set_page_config(\n    page_title=\"Comparação de Veículos - Insight Hub\",\n    page_icon=\"📈\",\n    layout=\"wide\"\n)\n\n@st.cache_data(ttl=300)  # Cache por 5 minutos\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais) com otimizações\"\"\"\n    try:\n        # Importar DatabaseManager\n        from database.db_manager import DatabaseManager\n        \n        # Carregar TODOS os dados por padrão conforme solicitado pelo usuário\n        with st.spinner(\"🔄 Carregando todos os dados para comparação...\"):\n            df = DatabaseManager.get_dashboard_data()\n            \n            if not df.empty:\n                st.success(f\"✅ Todos os dados carregados: {len(df):,} registros\")\n                return df\n        \n        st.warning(\"⚠️ Nenhum dado encontrado na base de dados.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"📈 Comparação de Veículos\")\n    st.markdown(\"Análise comparativa detalhada entre veículos da frota\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"📁 Nenhum dado encontrado. Faça o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados carregados (mais eficiente)\n    analyzer = DataAnalyzer(df)\n    visualizer = FleetVisualizations(analyzer)\n    \n    # Sidebar com filtros\n    st.sidebar.header(\"🔍 Filtros para Comparação\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por período\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    data_range = st.sidebar.date_input(\n        \"Período:\",\n        value=[min_date, max_date],\n        min_value=min_date,\n        max_value=max_date\n    )\n    \n    if len(data_range) == 2:\n        data_inicio, data_fim = data_range\n    else:\n        data_inicio = data_range[0]\n        data_fim = max_date\n    \n    # Aplicar filtros iniciais\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    if filtered_df.empty:\n        st.warning(\"⚠️ Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Seleção de veículos para comparação\n    st.subheader(\"🚗 Seleção de Veículos para Comparação\")\n    \n    # Mostrar estatísticas rápidas dos veículos disponíveis\n    vehicle_stats = filtered_df.groupby('placa').agg({\n        'velocidade_km': ['count', 'mean'],\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.mean() * 100)\n    }).round(2)\n    \n    vehicle_stats.columns = ['Registros', 'Vel. Média', 'KM Total', 'GPS (%)']\n    vehicle_stats = vehicle_stats.sort_values('Registros', ascending=False)\n    \n    col_selection1, col_selection2 = st.columns([2, 1])\n    \n    with col_selection1:\n        # Multiselect para escolher veículos\n        veiculos_disponiveis = vehicle_stats.index.tolist()\n        \n        if len(veiculos_disponiveis) < 2:\n            st.error(\"❌ Pelo menos 2 veículos são necessários para comparação.\")\n            st.stop()\n        \n        # Sugerir veículos com mais atividade como padrão\n        default_vehicles = veiculos_disponiveis[:min(5, len(veiculos_disponiveis))]\n        \n        veiculos_selecionados = st.multiselect(\n            \"Selecione os veículos para comparar:\",\n            veiculos_disponiveis,\n            default=default_vehicles,\n            help=\"Selecione entre 2 e 10 veículos para comparação\"\n        )\n        \n        if len(veiculos_selecionados) < 2:\n            st.warning(\"⚠️ Selecione pelo menos 2 veículos para comparação.\")\n            st.stop()\n        \n        if len(veiculos_selecionados) > 10:\n            st.warning(\"⚠️ Máximo de 10 veículos permitidos para melhor visualização.\")\n            veiculos_selecionados = veiculos_selecionados[:10]\n    \n    with col_selection2:\n        st.subheader(\"📊 Veículos Disponíveis\")\n        st.dataframe(\n            vehicle_stats.head(10),\n            use_container_width=True,\n            height=300\n        )\n    \n    # Filtrar dados para veículos selecionados\n    comparison_df = filtered_df[filtered_df['placa'].isin(veiculos_selecionados)]\n    analyzer.filtered_df = comparison_df\n    \n    # Executar comparação\n    comparison_data = analyzer.compare_vehicles(veiculos_selecionados)\n    \n    if not comparison_data:\n        st.error(\"❌ Não foi possível gerar dados de comparação.\")\n        st.stop()\n    \n    # Tabs para diferentes tipos de comparação\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n        \"📊 Visão Geral\",\n        \"⚡ Performance\",\n        \"🛣️ Operacional\",\n        \"📡 Qualidade\",\n        \"📈 Gráficos Detalhados\"\n    ])\n    \n    with tab1:\n        show_overview_comparison(comparison_data, comparison_df)\n    \n    with tab2:\n        show_performance_comparison(comparison_data, comparison_df)\n    \n    with tab3:\n        show_operational_comparison(comparison_data, comparison_df)\n    \n    with tab4:\n        show_quality_comparison(comparison_data, comparison_df)\n    \n    with tab5:\n        show_detailed_charts(comparison_data, comparison_df, analyzer)\n\ndef show_overview_comparison(comparison_data, df):\n    \"\"\"Mostra comparação geral entre veículos\"\"\"\n    st.header(\"📊 Visão Geral Comparativa\")\n    \n    # Converter dados para DataFrame\n    comparison_df = pd.DataFrame(comparison_data).T\n    comparison_df = comparison_df.round(2)\n    \n    # Métricas principais em colunas\n    st.subheader(\"📈 Métricas Principais\")\n    \n    metrics_to_show = [\n        ('total_registros', 'Total de Registros'),\n        ('velocidade_media', 'Velocidade Média (km/h)'),\n        ('distancia_total', 'Distância Total (km)'),\n        ('tempo_ativo', 'Tempo Ativo (h)'),\n        ('cobertura_gps', 'Cobertura GPS (%)'),\n        ('violacoes_velocidade', 'Violações de Velocidade'),\n        ('bloqueios', 'Bloqueios')\n    ]\n    \n    # Criar DataFrame para exibição\n    display_data = []\n    for placa, data in comparison_data.items():\n        row = {'Placa': placa}\n        for metric_key, metric_name in metrics_to_show:\n            if metric_key in data:\n                value = data[metric_key]\n                if metric_key in ['velocidade_media', 'cobertura_gps']:\n                    row[metric_name] = f\"{value:.1f}\"\n                elif metric_key in ['distancia_total', 'tempo_ativo']:\n                    row[metric_name] = f\"{value:.1f}\"\n                else:\n                    row[metric_name] = f\"{int(value):,}\"\n            else:\n                row[metric_name] = \"N/A\"\n        display_data.append(row)\n    \n    display_df = pd.DataFrame(display_data)\n    \n    # Estilizar tabela com cores baseadas na performance\n    def highlight_best_worst(s):\n        \"\"\"Destaca melhores e piores valores\"\"\"\n        if s.name == 'Placa':\n            return [''] * len(s)\n        \n        # Converter para numérico, ignorando strings\n        numeric_values = []\n        for val in s:\n            try:\n                numeric_values.append(float(val.replace(',', '')) if isinstance(val, str) and val != 'N/A' else float(val))\n            except:\n                numeric_values.append(0)\n        \n        if not numeric_values or all(v == 0 for v in numeric_values):\n            return [''] * len(s)\n        \n        # Para violações e bloqueios, menor é melhor\n        if 'Violações' in s.name or 'Bloqueios' in s.name:\n            min_val = min(numeric_values)\n            colors = ['background-color: #d4edda' if v == min_val and v == 0 \n                     else 'background-color: #f8d7da' if v == max(numeric_values) and v > 0\n                     else '' for v in numeric_values]\n        else:\n            # Para outras métricas, maior é melhor\n            max_val = max(numeric_values)\n            min_val = min(numeric_values)\n            colors = ['background-color: #d4edda' if v == max_val \n                     else 'background-color: #fff3cd' if v == min_val\n                     else '' for v in numeric_values]\n        \n        return colors\n    \n    styled_df = display_df.style.apply(highlight_best_worst, axis=0)\n    st.dataframe(styled_df, use_container_width=True, hide_index=True)\n    \n    # Ranking geral\n    st.subheader(\"🏆 Ranking Geral\")\n    \n    # Calcular score geral para cada veículo\n    scores = {}\n    for placa, data in comparison_data.items():\n        # Normalizar métricas (0-100)\n        vel_score = max(0, 100 - abs(data.get('velocidade_media', 50) - 50))  # Ideal ~50 km/h\n        gps_score = data.get('cobertura_gps', 0)\n        dist_score = min(100, (data.get('distancia_total', 0) / 1000) * 100)  # Normalizar por 1000km\n        \n        # Penalizar violações e bloqueios\n        violation_penalty = min(50, data.get('violacoes_velocidade', 0) * 5)\n        block_penalty = data.get('bloqueios', 0) * 20\n        \n        # Score final\n        final_score = (vel_score * 0.2 + gps_score * 0.3 + dist_score * 0.3 + \n                      (100 - violation_penalty) * 0.1 + (100 - block_penalty) * 0.1)\n        \n        scores[placa] = max(0, min(100, final_score))\n    \n    # Ordenar por score\n    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    \n    col_rank1, col_rank2 = st.columns(2)\n    \n    with col_rank1:\n        st.write(\"**🥇 Top Performers:**\")\n        for i, (placa, score) in enumerate(sorted_scores[:3]):\n            medal = [\"🥇\", \"🥈\", \"🥉\"][i]\n            st.success(f\"{medal} **{placa}** - Score: {score:.1f}/100\")\n    \n    with col_rank2:\n        st.write(\"**⚠️ Necessitam Atenção:**\")\n        for placa, score in sorted_scores[-3:]:\n            if score < 70:\n                st.warning(f\"⚠️ **{placa}** - Score: {score:.1f}/100\")\n            else:\n                st.info(f\"📊 **{placa}** - Score: {score:.1f}/100\")\n    \n    # Gráfico de radar comparativo\n    st.subheader(\"🎯 Comparação em Radar\")\n    \n    # Preparar dados para gráfico de radar\n    categories = ['Velocidade', 'Cobertura GPS', 'Produtividade', 'Conformidade']\n    \n    fig = go.Figure()\n    \n    colors = px.colors.qualitative.Set3\n    \n    for i, (placa, data) in enumerate(comparison_data.items()):\n        # Normalizar valores para 0-100\n        vel_norm = max(0, 100 - abs(data.get('velocidade_media', 50) - 50))\n        gps_norm = data.get('cobertura_gps', 0)\n        prod_norm = min(100, (data.get('distancia_total', 0) / 1000) * 100)\n        conf_norm = max(0, 100 - (data.get('violacoes_velocidade', 0) * 10))\n        \n        values = [vel_norm, gps_norm, prod_norm, conf_norm]\n        \n        fig.add_trace(go.Scatterpolar(\n            r=values + [values[0]],  # Fechar o polígono\n            theta=categories + [categories[0]],\n            fill='toself',\n            name=placa,\n            line_color=colors[i % len(colors)]\n        ))\n    \n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 100]\n            )),\n        showlegend=True,\n        title=\"Comparação Multidimensional de Performance\",\n        height=500\n    )\n    \n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_performance_comparison(comparison_data, df):\n    \"\"\"Mostra comparação de performance\"\"\"\n    st.header(\"⚡ Comparação de Performance\")\n    \n    # Gráficos de velocidade\n    col_perf1, col_perf2 = st.columns(2)\n    \n    with col_perf1:\n        st.subheader(\"📊 Velocidade Média\")\n        \n        vel_data = {placa: data['velocidade_media'] for placa, data in comparison_data.items()}\n        \n        fig_vel = px.bar(\n            x=list(vel_data.keys()),\n            y=list(vel_data.values()),\n            title='Velocidade Média por Veículo',\n            labels={'x': 'Placa', 'y': 'Velocidade Média (km/h)'},\n            color=list(vel_data.values()),\n            color_continuous_scale='Viridis'\n        )\n        fig_vel.update_layout(height=400)\n        st.plotly_chart(fig_vel, use_container_width=True)\n    \n    with col_perf2:\n        st.subheader(\"🛣️ Distância Total\")\n        \n        dist_data = {placa: data['distancia_total'] for placa, data in comparison_data.items()}\n        \n        fig_dist = px.bar(\n            x=list(dist_data.keys()),\n            y=list(dist_data.values()),\n            title='Distância Total por Veículo',\n            labels={'x': 'Placa', 'y': 'Distância Total (km)'},\n            color=list(dist_data.values()),\n            color_continuous_scale='Blues'\n        )\n        fig_dist.update_layout(height=400)\n        st.plotly_chart(fig_dist, use_container_width=True)\n    \n    # Análise de distribuição de velocidade por veículo\n    st.subheader(\"📈 Distribuição de Velocidade por Veículo\")\n    \n    fig_box = px.box(\n        df,\n        x='placa',\n        y='velocidade_km',\n        title='Distribuição de Velocidade (Box Plot)',\n        labels={'placa': 'Placa', 'velocidade_km': 'Velocidade (km/h)'}\n    )\n    fig_box.update_layout(height=400)\n    st.plotly_chart(fig_box, use_container_width=True)\n    \n    # Métricas de eficiência\n    st.subheader(\"🎯 Métricas de Eficiência\")\n    \n    efficiency_data = []\n    for placa, data in comparison_data.items():\n        if data['tempo_ativo'] > 0:\n            km_por_hora = data['distancia_total'] / data['tempo_ativo']\n        else:\n            km_por_hora = 0\n        \n        efficiency_data.append({\n            'Placa': placa,\n            'KM por Hora': round(km_por_hora, 2),\n            'Registros por Dia': round(data['total_registros'] / 30, 1),  # Assumindo ~30 dias\n            'Utilização (%)': min(100, round((data['tempo_ativo'] / (30 * 24)) * 100, 1))\n        })\n    \n    efficiency_df = pd.DataFrame(efficiency_data)\n    efficiency_df = efficiency_df.sort_values('KM por Hora', ascending=False)\n    \n    st.dataframe(efficiency_df, use_container_width=True, hide_index=True)\n    \n    # Gráfico de eficiência\n    fig_eff = px.scatter(\n        efficiency_df,\n        x='KM por Hora',\n        y='Utilização (%)',\n        text='Placa',\n        title='Eficiência: KM/Hora vs Utilização',\n        labels={'KM por Hora': 'Quilômetros por Hora', 'Utilização (%)': 'Utilização (%)'}\n    )\n    fig_eff.update_traces(textposition=\"top center\")\n    fig_eff.update_layout(height=400)\n    st.plotly_chart(fig_eff, use_container_width=True)\n\ndef show_operational_comparison(comparison_data, df):\n    \"\"\"Mostra comparação operacional\"\"\"\n    st.header(\"🛣️ Comparação Operacional\")\n    \n    # Tempo ativo vs distância\n    col_op1, col_op2 = st.columns(2)\n    \n    with col_op1:\n        st.subheader(\"⏰ Tempo Ativo\")\n        \n        tempo_data = {placa: data['tempo_ativo'] for placa, data in comparison_data.items()}\n        \n        fig_tempo = px.bar(\n            x=list(tempo_data.keys()),\n            y=list(tempo_data.values()),\n            title='Tempo Ativo por Veículo',\n            labels={'x': 'Placa', 'y': 'Tempo Ativo (horas)'},\n            color=list(tempo_data.values()),\n            color_continuous_scale='Oranges'\n        )\n        fig_tempo.update_layout(height=400)\n        st.plotly_chart(fig_tempo, use_container_width=True)\n    \n    with col_op2:\n        st.subheader(\"📊 Total de Registros\")\n        \n        registros_data = {placa: data['total_registros'] for placa, data in comparison_data.items()}\n        \n        fig_reg = px.bar(\n            x=list(registros_data.keys()),\n            y=list(registros_data.values()),\n            title='Total de Registros por Veículo',\n            labels={'x': 'Placa', 'y': 'Total de Registros'},\n            color=list(registros_data.values()),\n            color_continuous_scale='Greens'\n        )\n        fig_reg.update_layout(height=400)\n        st.plotly_chart(fig_reg, use_container_width=True)\n    \n    # Análise temporal de atividade\n    st.subheader(\"⏰ Padrões de Atividade por Hora\")\n    \n    # Atividade por hora para cada veículo\n    hourly_activity = df.groupby(['placa', df['data'].dt.hour]).size().unstack(fill_value=0)\n    \n    # Criar heatmap\n    fig_heatmap = px.imshow(\n        hourly_activity,\n        title='Atividade por Hora (Registros)',\n        labels={'x': 'Hora do Dia', 'y': 'Placa', 'color': 'Número de Registros'},\n        aspect='auto'\n    )\n    fig_heatmap.update_layout(height=500)\n    st.plotly_chart(fig_heatmap, use_container_width=True)\n    \n    # Análise de picos de atividade\n    st.subheader(\"📈 Análise de Picos de Atividade\")\n    \n    peak_analysis = []\n    for placa in df['placa'].unique():\n        vehicle_data = df[df['placa'] == placa]\n        hourly_counts = vehicle_data.groupby(vehicle_data['data'].dt.hour).size()\n        \n        if not hourly_counts.empty:\n            peak_hour = hourly_counts.idxmax()\n            peak_count = hourly_counts.max()\n            avg_count = hourly_counts.mean()\n            \n            peak_analysis.append({\n                'Placa': placa,\n                'Hora de Pico': f\"{peak_hour}h\",\n                'Registros no Pico': peak_count,\n                'Média por Hora': round(avg_count, 1),\n                'Intensidade do Pico': round(peak_count / avg_count, 1) if avg_count > 0 else 0\n            })\n    \n    peak_df = pd.DataFrame(peak_analysis)\n    peak_df = peak_df.sort_values('Intensidade do Pico', ascending=False)\n    \n    st.dataframe(peak_df, use_container_width=True, hide_index=True)\n\ndef show_quality_comparison(comparison_data, df):\n    \"\"\"Mostra comparação de qualidade dos dados\"\"\"\n    st.header(\"📡 Comparação de Qualidade\")\n    \n    # Métricas de qualidade\n    col_qual1, col_qual2 = st.columns(2)\n    \n    with col_qual1:\n        st.subheader(\"📡 Cobertura GPS\")\n        \n        gps_data = {placa: data['cobertura_gps'] for placa, data in comparison_data.items()}\n        \n        fig_gps = px.bar(\n            x=list(gps_data.keys()),\n            y=list(gps_data.values()),\n            title='Cobertura GPS por Veículo (%)',\n            labels={'x': 'Placa', 'y': 'Cobertura GPS (%)'},\n            color=list(gps_data.values()),\n            color_continuous_scale='RdYlGn'\n        )\n        fig_gps.update_layout(height=400)\n        # Adicionar linha de referência para 95%\n        fig_gps.add_hline(y=95, line_dash=\"dash\", line_color=\"red\", \n                         annotation_text=\"Meta: 95%\")\n        st.plotly_chart(fig_gps, use_container_width=True)\n    \n    with col_qual2:\n        st.subheader(\"🚨 Violações de Velocidade\")\n        \n        violacoes_data = {placa: data['violacoes_velocidade'] for placa, data in comparison_data.items()}\n        \n        fig_viol = px.bar(\n            x=list(violacoes_data.keys()),\n            y=list(violacoes_data.values()),\n            title='Violações de Velocidade por Veículo',\n            labels={'x': 'Placa', 'y': 'Número de Violações'},\n            color=list(violacoes_data.values()),\n            color_continuous_scale='Reds'\n        )\n        fig_viol.update_layout(height=400)\n        st.plotly_chart(fig_viol, use_container_width=True)\n    \n    # Score de qualidade combinado\n    st.subheader(\"🎯 Score de Qualidade Geral\")\n    \n    quality_scores = []\n    for placa, data in comparison_data.items():\n        # Calcular score de qualidade (0-100)\n        gps_score = data['cobertura_gps']\n        violation_penalty = min(50, data['violacoes_velocidade'] * 5)  # Máximo 50 pontos de penalidade\n        block_penalty = data['bloqueios'] * 20  # 20 pontos por bloqueio\n        \n        final_score = max(0, gps_score - violation_penalty - block_penalty)\n        \n        quality_scores.append({\n            'Placa': placa,\n            'Cobertura GPS (%)': round(data['cobertura_gps'], 1),\n            'Violações': data['violacoes_velocidade'],\n            'Bloqueios': data['bloqueios'],\n            'Score de Qualidade': round(final_score, 1)\n        })\n    \n    quality_df = pd.DataFrame(quality_scores)\n    quality_df = quality_df.sort_values('Score de Qualidade', ascending=False)\n    \n    # Colorir por score\n    def color_quality_score(val):\n        if val >= 90:\n            return 'background-color: #d4edda'  # Verde\n        elif val >= 70:\n            return 'background-color: #fff3cd'  # Amarelo\n        else:\n            return 'background-color: #f8d7da'  # Vermelho\n    \n    styled_quality = quality_df.style.applymap(\n        color_quality_score, \n        subset=['Score de Qualidade']\n    )\n    \n    st.dataframe(styled_quality, use_container_width=True, hide_index=True)\n    \n    # Gráfico de score de qualidade\n    fig_quality = px.bar(\n        quality_df,\n        x='Placa',\n        y='Score de Qualidade',\n        title='Score de Qualidade por Veículo',\n        labels={'Score de Qualidade': 'Score de Qualidade (0-100)'},\n        color='Score de Qualidade',\n        color_continuous_scale='RdYlGn'\n    )\n    fig_quality.update_layout(height=400)\n    # Linha de meta para 80%\n    fig_quality.add_hline(y=80, line_dash=\"dash\", line_color=\"orange\", \n                         annotation_text=\"Meta: 80%\")\n    st.plotly_chart(fig_quality, use_container_width=True)\n\ndef show_detailed_charts(comparison_data, df, analyzer):\n    \"\"\"Mostra gráficos detalhados\"\"\"\n    st.header(\"📈 Gráficos Detalhados\")\n    \n    # Seletor de tipo de gráfico\n    chart_type = st.selectbox(\n        \"Selecione o tipo de análise:\",\n        [\n            \"Evolução Temporal\",\n            \"Comparação de Velocidade\",\n            \"Análise de Correlação\",\n            \"Dispersão Multivariada\"\n        ]\n    )\n    \n    if chart_type == \"Evolução Temporal\":\n        show_temporal_evolution(df)\n    elif chart_type == \"Comparação de Velocidade\":\n        show_speed_comparison_charts(df)\n    elif chart_type == \"Análise de Correlação\":\n        show_correlation_analysis(df)\n    elif chart_type == \"Dispersão Multivariada\":\n        show_multivariate_scatter(comparison_data)\n\ndef show_temporal_evolution(df):\n    \"\"\"Mostra evolução temporal dos veículos\"\"\"\n    st.subheader(\"⏰ Evolução Temporal\")\n    \n    # Agregar dados por dia\n    daily_data = df.groupby(['placa', df['data'].dt.date]).agg({\n        'velocidade_km': 'mean',\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.mean() * 100)\n    }).reset_index()\n    \n    # Gráfico de velocidade ao longo do tempo\n    fig_temporal = px.line(\n        daily_data,\n        x='data',\n        y='velocidade_km',\n        color='placa',\n        title='Evolução da Velocidade Média por Dia',\n        labels={'data': 'Data', 'velocidade_km': 'Velocidade Média (km/h)'}\n    )\n    fig_temporal.update_layout(height=500)\n    st.plotly_chart(fig_temporal, use_container_width=True)\n    \n    # Gráfico de quilometragem acumulada\n    daily_data['km_acumulado'] = daily_data.groupby('placa')['odometro_periodo_km'].cumsum()\n    \n    fig_cumulative = px.line(\n        daily_data,\n        x='data',\n        y='km_acumulado',\n        color='placa',\n        title='Quilometragem Acumulada por Dia',\n        labels={'data': 'Data', 'km_acumulado': 'KM Acumulado'}\n    )\n    fig_cumulative.update_layout(height=500)\n    st.plotly_chart(fig_cumulative, use_container_width=True)\n\ndef show_speed_comparison_charts(df):\n    \"\"\"Mostra gráficos de comparação de velocidade\"\"\"\n    st.subheader(\"⚡ Comparação Detalhada de Velocidade\")\n    \n    # Histograma comparativo\n    fig_hist = px.histogram(\n        df,\n        x='velocidade_km',\n        color='placa',\n        nbins=30,\n        title='Distribuição de Velocidade por Veículo',\n        labels={'velocidade_km': 'Velocidade (km/h)', 'count': 'Frequência'},\n        opacity=0.7\n    )\n    fig_hist.update_layout(height=500)\n    st.plotly_chart(fig_hist, use_container_width=True)\n    \n    # Violin plot\n    fig_violin = px.violin(\n        df,\n        x='placa',\n        y='velocidade_km',\n        title='Distribuição de Velocidade (Violin Plot)',\n        labels={'placa': 'Placa', 'velocidade_km': 'Velocidade (km/h)'}\n    )\n    fig_violin.update_layout(height=500)\n    st.plotly_chart(fig_violin, use_container_width=True)\n\ndef show_correlation_analysis(df):\n    \"\"\"Mostra análise de correlação\"\"\"\n    st.subheader(\"🔗 Análise de Correlação\")\n    \n    # Calcular correlações por veículo\n    correlation_data = []\n    \n    for placa in df['placa'].unique():\n        vehicle_data = df[df['placa'] == placa]\n        \n        if len(vehicle_data) > 10:  # Mínimo de dados para correlação\n            # Adicionar variáveis temporais\n            vehicle_data = vehicle_data.copy()\n            vehicle_data['hora'] = vehicle_data['data'].dt.hour\n            vehicle_data['dia_semana'] = vehicle_data['data'].dt.dayofweek\n            \n            # Correlação entre velocidade e hora\n            corr_vel_hora = vehicle_data['velocidade_km'].corr(vehicle_data['hora'])\n            \n            # Correlação entre velocidade e GPS\n            corr_vel_gps = vehicle_data['velocidade_km'].corr(vehicle_data['gps'].astype(int))\n            \n            correlation_data.append({\n                'Placa': placa,\n                'Velocidade vs Hora': round(corr_vel_hora, 3),\n                'Velocidade vs GPS': round(corr_vel_gps, 3),\n                'Registros': len(vehicle_data)\n            })\n    \n    if correlation_data:\n        corr_df = pd.DataFrame(correlation_data)\n        st.dataframe(corr_df, use_container_width=True, hide_index=True)\n        \n        # Gráfico de scatter: velocidade vs hora para todos os veículos\n        fig_scatter = px.scatter(\n            df,\n            x=df['data'].dt.hour,\n            y='velocidade_km',\n            color='placa',\n            title='Velocidade vs Hora do Dia',\n            labels={'x': 'Hora do Dia', 'velocidade_km': 'Velocidade (km/h)'},\n            opacity=0.6\n        )\n        fig_scatter.update_layout(height=500)\n        st.plotly_chart(fig_scatter, use_container_width=True)\n\ndef show_multivariate_scatter(comparison_data):\n    \"\"\"Mostra análise multivariada\"\"\"\n    st.subheader(\"🎯 Análise Multivariada\")\n    \n    # Converter para DataFrame\n    multi_df = pd.DataFrame(comparison_data).T.reset_index()\n    multi_df.columns = ['Placa'] + list(multi_df.columns[1:])\n    \n    # Scatter plot multidimensional\n    x_axis = st.selectbox(\"Eixo X:\", ['velocidade_media', 'distancia_total', 'tempo_ativo', 'cobertura_gps'])\n    y_axis = st.selectbox(\"Eixo Y:\", ['cobertura_gps', 'velocidade_media', 'distancia_total', 'tempo_ativo'])\n    size_var = st.selectbox(\"Tamanho:\", ['total_registros', 'distancia_total', 'tempo_ativo'])\n    color_var = st.selectbox(\"Cor:\", ['violacoes_velocidade', 'cobertura_gps', 'bloqueios'])\n    \n    fig_multi = px.scatter(\n        multi_df,\n        x=x_axis,\n        y=y_axis,\n        size=size_var,\n        color=color_var,\n        text='Placa',\n        title=f'Análise Multivariada: {x_axis} vs {y_axis}',\n        labels={\n            x_axis: x_axis.replace('_', ' ').title(),\n            y_axis: y_axis.replace('_', ' ').title()\n        }\n    )\n    fig_multi.update_traces(textposition=\"top center\")\n    fig_multi.update_layout(height=600)\n    st.plotly_chart(fig_multi, use_container_width=True)\n    \n    # Matriz de correlação\n    st.subheader(\"📊 Matriz de Correlação\")\n    \n    numeric_columns = ['velocidade_media', 'distancia_total', 'tempo_ativo', \n                      'cobertura_gps', 'violacoes_velocidade', 'bloqueios', 'total_registros']\n    \n    corr_matrix = multi_df[numeric_columns].corr()\n    \n    fig_corr = px.imshow(\n        corr_matrix,\n        title='Matriz de Correlação entre Métricas',\n        labels={'color': 'Correlação'},\n        color_continuous_scale='RdBu'\n    )\n    fig_corr.update_layout(height=500)\n    st.plotly_chart(fig_corr, use_container_width=True)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":28471},"pages/4_🔮_Manutenção_Preditiva.py":{"content":"\"\"\"\nPágina de Manutenção Preditiva - Análise Avançada com ML\n\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nfrom utils.ml_predictive import PredictiveMaintenanceAnalyzer\n\n# Configuração da página\nst.set_page_config(\n    page_title=\"Manutenção Preditiva\",\n    page_icon=\"🔮\",\n    layout=\"wide\"\n)\n\nst.title(\"🔮 Manutenção Preditiva\")\nst.markdown(\"Análise avançada com Machine Learning para prevenção de falhas\")\n\n# Verificar se há dados\nif not DatabaseManager.has_data():\n    st.warning(\"⚠️ Nenhum dado encontrado. Faça o upload de um arquivo CSV primeiro.\")\n    st.stop()\n\n# Carregar dados\nwith st.spinner(\"Carregando dados...\"):\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if df.empty:\n            st.error(\"❌ Erro ao carregar dados da base de dados\")\n            st.stop()\n    except Exception as e:\n        st.error(f\"❌ Erro: {e}\")\n        st.stop()\n\n# Filtros laterais\nst.sidebar.header(\"🔧 Filtros\")\n\n# Filtro de cliente\nclientes = sorted(df['cliente'].unique())\ncliente_selecionado = st.sidebar.selectbox(\n    \"Cliente:\",\n    options=['Todos'] + clientes,\n    index=0\n)\n\n# Filtro de veículo\nif cliente_selecionado != 'Todos':\n    veiculos = sorted(df[df['cliente'] == cliente_selecionado]['placa'].unique())\nelse:\n    veiculos = sorted(df['placa'].unique())\n\nveiculo_selecionado = st.sidebar.selectbox(\n    \"Veículo:\",\n    options=['Todos'] + list(veiculos),\n    index=0\n)\n\n# Filtro de período\nperiodo = st.sidebar.selectbox(\n    \"Período:\",\n    options=['Últimos 7 dias', 'Últimos 30 dias', 'Todos'],\n    index=1\n)\n\n# Aplicar filtros\ndf_filtrado = df.copy()\n\nif cliente_selecionado != 'Todos':\n    df_filtrado = df_filtrado[df_filtrado['cliente'] == cliente_selecionado]\n\nif veiculo_selecionado != 'Todos':\n    df_filtrado = df_filtrado[df_filtrado['placa'] == veiculo_selecionado]\n\n# Filtro de período\nif periodo != 'Todos':\n    data_limite = datetime.now()\n    if periodo == 'Últimos 7 dias':\n        data_limite -= timedelta(days=7)\n    elif periodo == 'Últimos 30 dias':\n        data_limite -= timedelta(days=30)\n    \n    df_filtrado = df_filtrado[df_filtrado['data'] >= data_limite]\n\nif df_filtrado.empty:\n    st.warning(\"⚠️ Nenhum dado encontrado com os filtros aplicados.\")\n    st.stop()\n\n# Análise de Manutenção Preditiva\nst.header(\"🤖 Análise de Machine Learning\")\n\nwith st.spinner(\"Executando análise preditiva...\"):\n    analyzer = PredictiveMaintenanceAnalyzer()\n    resultado = analyzer.analyze_vehicle_health(df_filtrado)\n\nif resultado['status'] == 'error':\n    st.error(f\"❌ {resultado['message']}\")\n    st.stop()\n\n# Dashboard de Health Scores\ncol1, col2, col3, col4 = st.columns(4)\n\nhealth_scores = resultado['health_scores']\n\nwith col1:\n    score_geral = health_scores.get('geral', 0)\n    cor_geral = 'green' if score_geral > 80 else 'orange' if score_geral > 60 else 'red'\n    st.metric(\n        label=\"🏥 Saúde Geral\",\n        value=f\"{score_geral}%\",\n        delta=None\n    )\n    st.markdown(f\"<div style='color:{cor_geral}'>●</div>\", unsafe_allow_html=True)\n\nwith col2:\n    score_bateria = health_scores.get('bateria', 0)\n    cor_bateria = 'green' if score_bateria > 70 else 'orange' if score_bateria > 50 else 'red'\n    st.metric(\n        label=\"🔋 Bateria\",\n        value=f\"{score_bateria}%\"\n    )\n    st.markdown(f\"<div style='color:{cor_bateria}'>●</div>\", unsafe_allow_html=True)\n\nwith col3:\n    score_comportamento = health_scores.get('comportamento', 0)\n    cor_comportamento = 'green' if score_comportamento > 70 else 'orange' if score_comportamento > 50 else 'red'\n    st.metric(\n        label=\"📊 Comportamento\",\n        value=f\"{score_comportamento}%\"\n    )\n    st.markdown(f\"<div style='color:{cor_comportamento}'>●</div>\", unsafe_allow_html=True)\n\nwith col4:\n    score_velocidade = health_scores.get('velocidade', 0)\n    cor_velocidade = 'green' if score_velocidade > 70 else 'orange' if score_velocidade > 50 else 'red'\n    st.metric(\n        label=\"🚗 Velocidade\",\n        value=f\"{score_velocidade}%\"\n    )\n    st.markdown(f\"<div style='color:{cor_velocidade}'>●</div>\", unsafe_allow_html=True)\n\n# Gráfico de Health Scores\nfig_health = go.Figure()\n\nscores = [health_scores.get(k, 0) for k in ['bateria', 'comportamento', 'velocidade']]\nlabels = ['Bateria', 'Comportamento', 'Velocidade']\ncolors = ['#ff7f0e', '#2ca02c', '#d62728']\n\nfig_health.add_trace(go.Bar(\n    x=labels,\n    y=scores,\n    marker_color=colors,\n    text=[f\"{s}%\" for s in scores],\n    textposition='auto',\n))\n\nfig_health.update_layout(\n    title=\"📊 Scores de Saúde do Veículo\",\n    yaxis_title=\"Score (%)\",\n    yaxis=dict(range=[0, 100]),\n    showlegend=False,\n    height=400\n)\n\nst.plotly_chart(fig_health, use_container_width=True)\n\n# Alertas de Manutenção\nst.header(\"🚨 Alertas de Manutenção\")\n\nmaintenance_alerts = resultado['maintenance_alerts']\n\nif maintenance_alerts:\n    for alert in maintenance_alerts:\n        severidade = alert['severidade']\n        cor = '🔴' if severidade == 'Alta' else '🟡' if severidade == 'Média' else '🟢'\n        \n        with st.expander(f\"{cor} {alert['tipo']} - {severidade}\"):\n            st.write(f\"**Descrição:** {alert['descricao']}\")\n            st.write(f\"**Prazo:** {alert['prazo']}\")\nelse:\n    st.success(\"✅ Nenhum alerta de manutenção detectado!\")\n\n# Anomalias Detectadas\nst.header(\"⚠️ Anomalias Detectadas\")\n\nanomalies = resultado['anomalies']\nanomaly_count = anomalies.get('count', 0)\n\ncol1, col2 = st.columns([1, 2])\n\nwith col1:\n    st.metric(\n        label=\"Total de Anomalias\",\n        value=anomaly_count\n    )\n    \n    if anomaly_count > 0:\n        severity = anomalies.get('severity', 'Baixa')\n        color_map = {'Alta': '🔴', 'Média': '🟡', 'Baixa': '🟢'}\n        st.write(f\"**Severidade:** {color_map.get(severity, '🟢')} {severity}\")\n\nwith col2:\n    # Gráfico de anomalias ao longo do tempo\n    if anomaly_count > 0 and 'indices' in anomalies:\n        try:\n            anomaly_indices = anomalies['indices']\n            if anomaly_indices:\n                df_anomalies = df_filtrado.iloc[anomaly_indices].copy()\n                \n                fig_anomalies = px.scatter(\n                    df_anomalies,\n                    x='data',\n                    y='velocidade_km',\n                    title=\"Anomalias de Velocidade\",\n                    color_discrete_sequence=['red'],\n                    hover_data=['placa']\n                )\n                \n                fig_anomalies.update_layout(height=300)\n                st.plotly_chart(fig_anomalies, use_container_width=True)\n        except Exception as e:\n            st.write(\"Dados de anomalias não disponíveis para visualização\")\n\n# Padrões de Uso\nst.header(\"📈 Padrões de Uso\")\n\npatterns = resultado.get('patterns', {})\n\nif patterns:\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.subheader(\"🚗 Padrões de Velocidade\")\n        vel_patterns = patterns.get('velocidade', {})\n        if vel_patterns:\n            st.write(f\"**Velocidade Média:** {vel_patterns.get('media', 0)} km/h\")\n            st.write(f\"**Velocidade Máxima:** {vel_patterns.get('maxima', 0)} km/h\")\n            st.write(f\"**Excessos de Velocidade:** {vel_patterns.get('excessos', 0)}\")\n            st.write(f\"**Paradas:** {vel_patterns.get('paradas', 0)}\")\n    \n    with col2:\n        st.subheader(\"⏰ Padrões Temporais\")\n        temp_patterns = patterns.get('uso_temporal', {})\n        if temp_patterns:\n            st.write(f\"**Horário de Pico:** {temp_patterns.get('horario_pico', 'N/A')}h\")\n            st.write(f\"**Horário de Menor Uso:** {temp_patterns.get('horario_baixo', 'N/A')}h\")\n            st.write(f\"**Uso Noturno:** {temp_patterns.get('uso_noturno', 0)} registros\")\n\n# Recomendações\nst.header(\"💡 Recomendações\")\n\nrecommendations = resultado.get('recommendations', [])\nif recommendations:\n    for rec in recommendations:\n        st.write(f\"• {rec}\")\nelse:\n    st.info(\"📋 Nenhuma recomendação específica no momento.\")\n\n# Informações adicionais\nst.header(\"ℹ️ Informações da Análise\")\n\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    st.metric(\"📊 Registros Analisados\", len(df_filtrado))\n\nwith col2:\n    periodo_analise = df_filtrado['data'].max() - df_filtrado['data'].min()\n    st.metric(\"📅 Período\", f\"{periodo_analise.days} dias\")\n\nwith col3:\n    st.metric(\"🚗 Veículos\", df_filtrado['placa'].nunique())\n\nst.info(\"🤖 **Sobre a Análise:** Esta análise utiliza algoritmos de Machine Learning (Isolation Forest) para detectar anomalias e padrões nos dados telemáticos, fornecendo insights preditivos para manutenção preventiva.\")","size_bytes":8846},"pages/5_🧠_Insights_Automáticos.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport sys\n\n# Adicionar o diretório raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.insights_generator import InsightsGenerator\n\nst.set_page_config(\n    page_title=\"Insights Automáticos - Insight Hub\",\n    page_icon=\"🧠\",\n    layout=\"wide\"\n)\n\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais)\"\"\"\n    try:\n        # Importar DatabaseManager\n        from database.db_manager import DatabaseManager\n        \n        # Carregar APENAS da base de dados - sem fallbacks fictícios\n        if DatabaseManager.has_data():\n            df = DatabaseManager.get_dashboard_data()\n            if not df.empty:\n                st.success(f\"✅ Dados reais carregados: {len(df):,} registros da base de dados\")\n                return df\n        \n        # Se não há dados reais, mostrar mensagem clara\n        st.warning(\"⚠️ Nenhum dado real encontrado na base de dados. Faça upload dos seus arquivos CSV.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados reais: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"🧠 Insights Automáticos\")\n    st.markdown(\"Análise inteligente e recomendações baseadas em dados da frota\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"📁 Nenhum dado encontrado. Faça o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados da base de dados\n    analyzer = DataAnalyzer.from_database()\n    \n    # Sidebar com filtros\n    st.sidebar.header(\"🔍 Filtros para Análise\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por período\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    # Período de análise\n    periodo_analise = st.sidebar.selectbox(\n        \"Período de Análise:\",\n        [\n            \"Últimos 7 dias\",\n            \"Últimos 30 dias\",\n            \"Últimos 90 dias\",\n            \"Todo o período\",\n            \"Personalizado\"\n        ]\n    )\n    \n    # Calcular datas baseado no período\n    if periodo_analise == \"Últimos 7 dias\":\n        data_inicio = max_date - timedelta(days=7)\n        data_fim = max_date\n    elif periodo_analise == \"Últimos 30 dias\":\n        data_inicio = max_date - timedelta(days=30)\n        data_fim = max_date\n    elif periodo_analise == \"Últimos 90 dias\":\n        data_inicio = max_date - timedelta(days=90)\n        data_fim = max_date\n    elif periodo_analise == \"Todo o período\":\n        data_inicio = min_date\n        data_fim = max_date\n    else:  # Personalizado\n        data_range = st.sidebar.date_input(\n            \"Período Personalizado:\",\n            value=[min_date, max_date],\n            min_value=min_date,\n            max_value=max_date\n        )\n        if len(data_range) == 2:\n            data_inicio, data_fim = data_range\n        else:\n            data_inicio = data_range[0]\n            data_fim = max_date\n    \n    # Aplicar filtros\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    if filtered_df.empty:\n        st.warning(\"⚠️ Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Inicializar gerador de insights\n    insights_generator = InsightsGenerator(analyzer)\n    \n    # Gerar insights\n    with st.spinner(\"🧠 Gerando insights inteligentes...\"):\n        insights = insights_generator.generate_all_insights()\n    \n    # Tabs para diferentes tipos de insights\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n        \"📊 Resumo Executivo\",\n        \"⚠️ Alertas Críticos\",\n        \"📈 Oportunidades\",\n        \"🔮 Predições\",\n        \"📋 Relatório Completo\"\n    ])\n    \n    with tab1:\n        show_executive_summary(insights, analyzer)\n    \n    with tab2:\n        show_critical_alerts(insights, analyzer)\n    \n    with tab3:\n        show_opportunities(insights, analyzer)\n    \n    with tab4:\n        show_predictions(insights, analyzer)\n    \n    with tab5:\n        show_complete_report(insights, insights_generator, analyzer)\n\ndef show_executive_summary(insights, analyzer):\n    \"\"\"Mostra resumo executivo dos insights\"\"\"\n    st.header(\"📊 Resumo Executivo\")\n    st.markdown(\"**Visão geral da performance da sua frota com recomendações práticas**\")\n    \n    kpis = analyzer.get_kpis()\n    \n    # Verificar se há KPIs válidos\n    if not kpis:\n        st.warning(\"⚠️ Não foi possível calcular métricas para gerar insights. Verifique se há dados nos filtros aplicados.\")\n        return\n    \n    # KPIs principais com contexto\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"🚗 Veículos Analisados\", f\"{kpis['total_veiculos']:,}\")\n    \n    with col2:\n        st.metric(\"📊 Total de Registros\", f\"{kpis['total_registros']:,}\")\n    \n    with col3:\n        st.metric(\"📅 Período\", f\"{kpis['periodo_dias']} dias\")\n    \n    with col4:\n        total_insights = len(insights)\n        critical_insights = len([i for i in insights if i['type'] == 'error'])\n        st.metric(\"🧠 Insights Gerados\", f\"{total_insights}\", delta=f\"{critical_insights} críticos\")\n    \n    # Resumo por categoria\n    st.subheader(\"📋 Resumo por Categoria\")\n    \n    insight_summary = {\n        'error': {'count': 0, 'label': '🚨 Críticos', 'color': '#dc3545'},\n        'warning': {'count': 0, 'label': '⚠️ Atenção', 'color': '#ffc107'},\n        'info': {'count': 0, 'label': 'ℹ️ Informativos', 'color': '#17a2b8'},\n        'success': {'count': 0, 'label': '✅ Positivos', 'color': '#28a745'}\n    }\n    \n    for insight in insights:\n        insight_summary[insight['type']]['count'] += 1\n    \n    col_summary = st.columns(4)\n    \n    for i, (type_key, data) in enumerate(insight_summary.items()):\n        with col_summary[i]:\n            st.metric(\n                label=data['label'],\n                value=data['count'],\n                delta=f\"{(data['count']/len(insights)*100):.0f}%\" if insights else \"0%\"\n            )\n    \n    # Gráfico de distribuição de insights\n    if insights:\n        fig_insights = px.pie(\n            values=[data['count'] for data in insight_summary.values()],\n            names=[data['label'] for data in insight_summary.values()],\n            title='Distribuição de Insights por Categoria',\n            color_discrete_sequence=[data['color'] for data in insight_summary.values()]\n        )\n        st.plotly_chart(fig_insights, use_container_width=True)\n    \n    # Top 3 insights mais importantes - versão melhorada\n    st.subheader(\"🎯 Ações Prioritárias para Sua Frota\")\n    st.markdown(\"**As 3 ações mais importantes que você deve tomar agora:**\")\n    \n    priority_insights = sorted(insights, key=lambda x: x['priority'])[:3]\n    \n    for i, insight in enumerate(priority_insights):\n        icon = \"🚨\" if insight['type'] == 'error' else \"⚠️\" if insight['type'] == 'warning' else \"✅\" if insight['type'] == 'success' else \"ℹ️\"\n        priority_label = \"URGENTE\" if insight['type'] == 'error' else \"IMPORTANTE\" if insight['type'] == 'warning' else \"POSITIVO\" if insight['type'] == 'success' else \"INFORMATIVO\"\n        \n        with st.container():\n            if insight['type'] == 'error':\n                st.error(f\"{icon} **PRIORIDADE {priority_label}:** {insight['title']}\")\n            elif insight['type'] == 'warning':\n                st.warning(f\"{icon} **PRIORIDADE {priority_label}:** {insight['title']}\")\n            else:\n                st.info(f\"{icon} **{priority_label}:** {insight['title']}\")\n            \n            col_insight1, col_insight2 = st.columns([2, 1])\n            with col_insight1:\n                st.markdown(f\"**💡 O que fazer:** {insight['recommendation']}\")\n                st.markdown(f\"**📝 Por quê:** {insight['description']}\")\n            \n            with col_insight2:\n                if insight['type'] == 'error':\n                    st.metric(\"⏰ Prazo\", \"24-48 horas\", delta=\"URGENTE\")\n                elif insight['type'] == 'warning':\n                    st.metric(\"⏰ Prazo\", \"1-2 semanas\", delta=\"Importante\")\n                else:\n                    st.metric(\"📊 Status\", \"Monitorar\", delta=\"Positivo\")\n            \n            st.markdown(\"---\")\n            \n            # Adicionar métricas relacionadas se disponível\n            if 'compliance' in insight['title'].lower():\n                compliance = analyzer.get_compliance_analysis()\n                if compliance:\n                    col_comp1, col_comp2 = st.columns(2)\n                    with col_comp1:\n                        st.metric(\"Violações\", compliance.get('violacoes_velocidade', 0))\n                    with col_comp2:\n                        st.metric(\"Score Médio\", f\"{sum(compliance.get('score_compliance', {}).values()) / len(compliance.get('score_compliance', {})):.1f}%\" if compliance.get('score_compliance') else \"N/A\")\n    \n    # Resumo de performance geral\n    st.subheader(\"📈 Performance Geral da Frota\")\n    \n    # Score geral baseado nos insights\n    total_score = 100\n    for insight in insights:\n        if insight['type'] == 'error':\n            total_score -= 15\n        elif insight['type'] == 'warning':\n            total_score -= 8\n        elif insight['type'] == 'success':\n            total_score += 5\n    \n    total_score = max(0, min(100, total_score))\n    \n    # Gauge para score geral\n    fig_gauge = go.Figure(go.Indicator(\n        mode = \"gauge+number+delta\",\n        value = total_score,\n        domain = {'x': [0, 1], 'y': [0, 1]},\n        title = {'text': \"Score Geral da Frota\"},\n        delta = {'reference': 80},\n        gauge = {\n            'axis': {'range': [None, 100]},\n            'bar': {'color': \"darkblue\"},\n            'steps': [\n                {'range': [0, 50], 'color': \"lightgray\"},\n                {'range': [50, 80], 'color': \"gray\"}],\n            'threshold': {\n                'line': {'color': \"red\", 'width': 4},\n                'thickness': 0.75,\n                'value': 90}}))\n    \n    fig_gauge.update_layout(height=400)\n    st.plotly_chart(fig_gauge, use_container_width=True)\n\ndef show_critical_alerts(insights, analyzer):\n    \"\"\"Mostra alertas críticos com planos de ação claros\"\"\"\n    st.header(\"🚨 Alertas Críticos\")\n    st.markdown(\"**Problemas que precisam de sua atenção imediata com planos de ação específicos**\")\n    \n    critical_insights = [i for i in insights if i['type'] == 'error']\n    warning_insights = [i for i in insights if i['type'] == 'warning']\n    \n    if not critical_insights and not warning_insights:\n        st.success(\"✅ **Excelente!** Nenhum alerta crítico identificado.\")\n        st.balloons()\n        return\n    \n    # Alertas críticos\n    if critical_insights:\n        st.subheader(\"🚨 Requer Ação Imediata\")\n        \n        for insight in critical_insights:\n            with st.container():\n                st.error(f\"**{insight['title']}**\")\n                col_alert1, col_alert2 = st.columns([2, 1])\n                \n                with col_alert1:\n                    st.write(f\"📝 **Problema:** {insight['description']}\")\n                    st.write(f\"💡 **Ação Recomendada:** {insight['recommendation']}\")\n                \n                with col_alert2:\n                    st.metric(\"Prioridade\", \"ALTA\", delta=\"Crítico\")\n                \n                st.markdown(\"---\")\n    \n    # Alertas de atenção\n    if warning_insights:\n        st.subheader(\"⚠️ Requer Atenção\")\n        \n        for insight in warning_insights:\n            with st.container():\n                st.warning(f\"**{insight['title']}**\")\n                col_warn1, col_warn2 = st.columns([2, 1])\n                \n                with col_warn1:\n                    st.write(f\"📝 **Observação:** {insight['description']}\")\n                    st.write(f\"💡 **Recomendação:** {insight['recommendation']}\")\n                \n                with col_warn2:\n                    st.metric(\"Prioridade\", \"MÉDIA\", delta=\"Atenção\")\n                \n                st.markdown(\"---\")\n    \n    # Análise detalhada dos problemas\n    st.subheader(\"🔍 Análise Detalhada dos Problemas\")\n    \n    compliance = analyzer.get_compliance_analysis()\n    \n    if compliance:\n        col_detail1, col_detail2, col_detail3 = st.columns(3)\n        \n        with col_detail1:\n            st.metric(\n                \"🚨 Violações de Velocidade\",\n                f\"{compliance.get('violacoes_velocidade', 0):,}\",\n                help=\"Registros com velocidade acima do limite\"\n            )\n        \n        with col_detail2:\n            st.metric(\n                \"📡 Problemas de GPS\",\n                f\"{compliance.get('veiculos_baixo_gps', 0):,}\",\n                help=\"Veículos com cobertura GPS abaixo de 95%\"\n            )\n        \n        with col_detail3:\n            st.metric(\n                \"🔒 Veículos Bloqueados\",\n                f\"{compliance.get('veiculos_bloqueados', 0):,}\",\n                help=\"Veículos com status de bloqueio ativo\"\n            )\n        \n        # Gráfico de veículos com problemas\n        if compliance.get('score_compliance'):\n            problem_vehicles = {k: v for k, v in compliance['score_compliance'].items() if v < 70}\n            \n            if problem_vehicles:\n                st.subheader(\"🚗 Veículos que Necessitam Atenção Urgente\")\n                \n                fig_problems = px.bar(\n                    x=list(problem_vehicles.keys()),\n                    y=list(problem_vehicles.values()),\n                    title='Score de Compliance - Veículos Problemáticos',\n                    labels={'x': 'Placa', 'y': 'Score de Compliance (%)'},\n                    color=list(problem_vehicles.values()),\n                    color_continuous_scale='Reds'\n                )\n                fig_problems.add_hline(y=70, line_dash=\"dash\", line_color=\"red\", \n                                     annotation_text=\"Limite Mínimo: 70%\")\n                st.plotly_chart(fig_problems, use_container_width=True)\n    \n    # Plano de ação recomendado - versão melhorada\n    st.subheader(\"📋 Plano de Ação Detalhado\")\n    st.markdown(\"**Cronograma específico para resolver os problemas identificados:**\")\n    \n    action_plan = []\n    \n    if critical_insights:\n        for insight in critical_insights:\n            action_plan.append({\n                'Prioridade': '🚨 CRÍTICA',\n                'Prazo': '24-48 horas',\n                'Ação': f\"Resolver: {insight['title']}\",\n                'Como Fazer': insight['recommendation'],\n                'Responsável': 'Gestor de Frota + Equipe Técnica'\n            })\n    \n    if warning_insights:\n        action_plan.append({\n            'Prioridade': 'MÉDIA',\n            'Prazo': '1 semana',\n            'Ação': 'Implementar melhorias para alertas de atenção',\n            'Responsável': 'Equipe Operacional'\n        })\n    \n    if compliance and compliance.get('violacoes_velocidade', 0) > 0:\n        action_plan.append({\n            'Prioridade': 'ALTA',\n            'Prazo': '3 dias',\n            'Ação': 'Treinamento de condutores sobre limites de velocidade',\n            'Responsável': 'RH/Treinamento'\n        })\n    \n    if action_plan:\n        action_df = pd.DataFrame(action_plan)\n        st.dataframe(action_df, use_container_width=True, hide_index=True)\n\ndef show_opportunities(insights, analyzer):\n    \"\"\"Mostra oportunidades de melhoria com potencial de economia\"\"\"\n    st.header(\"📈 Oportunidades de Economia e Melhoria\")\n    st.markdown(\"**Onde você pode economizar dinheiro e melhorar a eficiência da sua frota**\")\n    \n    info_insights = [i for i in insights if i['type'] == 'info']\n    success_insights = [i for i in insights if i['type'] == 'success']\n    \n    # Oportunidades identificadas\n    st.subheader(\"💡 Oportunidades Identificadas\")\n    \n    if info_insights:\n        for insight in info_insights:\n            with st.container():\n                st.info(f\"**{insight['title']}**\")\n                col_opp1, col_opp2 = st.columns([3, 1])\n                \n                with col_opp1:\n                    st.write(f\"📊 **Análise:** {insight['description']}\")\n                    st.write(f\"🎯 **Oportunidade:** {insight['recommendation']}\")\n                \n                with col_opp2:\n                    # Estimar impacto potencial com valores em reais\n                    kpis = analyzer.get_kpis()\n                    num_vehicles = kpis.get('total_veiculos', 1)\n                    \n                    if 'velocidade' in insight['title'].lower():\n                        economy_month = num_vehicles * 300  # R$ 300/mês por veículo em combustível\n                        st.metric(\"💰 Economia/Mês\", f\"R$ {economy_month:,}\", delta=\"Combustível\")\n                    elif 'gps' in insight['title'].lower():\n                        st.metric(\"📊 Melhoria\", \"10-20%\", delta=\"Controle\")\n                    else:\n                        st.metric(\"⚡ Impacto\", \"Médio\", delta=\"Eficiência\")\n                \n                st.markdown(\"---\")\n    \n    # Pontos fortes identificados\n    if success_insights:\n        st.subheader(\"✅ Pontos Fortes da Frota\")\n        \n        for insight in success_insights:\n            st.success(f\"**{insight['title']}** - {insight['description']}\")\n    \n    # Análise de eficiência\n    st.subheader(\"📊 Análise de Eficiência\")\n    \n    efficiency = analyzer.get_efficiency_metrics()\n    \n    if efficiency and 'eficiencia_por_veiculo' in efficiency:\n        # Identificar top performers e underperformers\n        vehicle_efficiency = efficiency['eficiencia_por_veiculo']\n        \n        if vehicle_efficiency is not None and hasattr(vehicle_efficiency, 'empty') and not vehicle_efficiency.empty:\n            # Converter series para lista de dicts\n            efficiency_data = []\n            for placa, data in vehicle_efficiency.items():\n                if isinstance(data, dict):\n                    efficiency_data.append({\n                        'Placa': placa,\n                        'KM por Dia': data.get('km_por_dia', 0),\n                        'Utilização Diária': data.get('utilizacao_diaria', 0),\n                        'Velocidade Média': data.get('velocidade_media', 0),\n                        'Tempo Parado (%)': data.get('tempo_parado_pct', 0)\n                    })\n            \n            if efficiency_data:\n                efficiency_df = pd.DataFrame(efficiency_data)\n                \n                # Top performers\n                top_performers = efficiency_df.nlargest(5, 'KM por Dia')\n                \n                col_eff1, col_eff2 = st.columns(2)\n                \n                with col_eff1:\n                    st.write(\"**🏆 Top Performers (KM por Dia):**\")\n                    for _, row in top_performers.iterrows():\n                        st.success(f\"**{row['Placa']}** - {row['KM por Dia']:.1f} km/dia\")\n                \n                with col_eff2:\n                    st.write(\"**⚠️ Baixa Utilização:**\")\n                    low_performers = efficiency_df.nsmallest(5, 'Utilização Diária')\n                    for _, row in low_performers.iterrows():\n                        if row['Utilização Diária'] < 10:\n                            st.warning(f\"**{row['Placa']}** - {row['Utilização Diária']:.1f} reg/dia\")\n    \n    # Oportunidades de otimização\n    st.subheader(\"🎯 Oportunidades de Otimização\")\n    \n    optimization_opportunities = [\n        {\n            'Área': 'Otimização de Rotas',\n            'Descrição': 'Análise de padrões de movimentação para reduzir distâncias',\n            'Impacto Estimado': '10-15% redução de combustível',\n            'Investimento': 'Baixo',\n            'Prazo': '2-4 semanas'\n        },\n        {\n            'Área': 'Treinamento de Condutores',\n            'Descrição': 'Programa de condução econômica e segura',\n            'Impacto Estimado': '15-20% redução de violações',\n            'Investimento': 'Médio',\n            'Prazo': '1-2 meses'\n        },\n        {\n            'Área': 'Manutenção Preventiva',\n            'Descrição': 'Implementação de manutenção baseada em dados',\n            'Impacto Estimado': '20-30% redução de quebras',\n            'Investimento': 'Alto',\n            'Prazo': '3-6 meses'\n        }\n    ]\n    \n    for opp in optimization_opportunities:\n        with st.expander(f\"🎯 {opp['Área']}\"):\n            col_desc, col_impact = st.columns([2, 1])\n            \n            with col_desc:\n                st.write(f\"**Descrição:** {opp['Descrição']}\")\n                st.write(f\"**Prazo de Implementação:** {opp['Prazo']}\")\n            \n            with col_impact:\n                st.metric(\"Impacto Estimado\", opp['Impacto Estimado'])\n                st.metric(\"Investimento\", opp['Investimento'])\n\ndef show_predictions(insights, analyzer):\n    \"\"\"Mostra predições práticas para planejamento\"\"\"\n    st.header(\"🔮 Previsões para Planejamento\")\n    st.markdown(\"**Prepare-se para o que está por vir: manutenções, custos e necessidades da frota**\")\n    \n    df = analyzer.filtered_df\n    \n    if len(df) < 14:  # Menos de 2 semanas de dados\n        st.warning(\"⚠️ **Dados insuficientes para previsões.** Carregue pelo menos 14 dias de dados para ter previsões confiáveis.\")\n        return\n    \n    # Análise de tendências\n    st.subheader(\"📈 Análise de Tendências\")\n    \n    # Dividir dados em períodos\n    df_sorted = df.sort_values('data')\n    mid_point = len(df_sorted) // 2\n    \n    first_half = df_sorted.iloc[:mid_point]\n    second_half = df_sorted.iloc[mid_point:]\n    \n    # Calcular mudanças\n    trends = {}\n    \n    if not first_half.empty and not second_half.empty:\n        trends['velocidade'] = second_half['velocidade_km'].mean() - first_half['velocidade_km'].mean()\n        trends['gps_coverage'] = (second_half['gps'].mean() - first_half['gps'].mean()) * 100\n        trends['activity'] = len(second_half) / len(first_half) - 1\n    \n    col_trend1, col_trend2, col_trend3 = st.columns(3)\n    \n    with col_trend1:\n        if trends.get('velocidade', 0) != 0:\n            st.metric(\n                \"Velocidade Média\",\n                f\"{second_half['velocidade_km'].mean():.1f} km/h\",\n                delta=f\"{trends['velocidade']:.1f} km/h\"\n            )\n    \n    with col_trend2:\n        if trends.get('gps_coverage', 0) != 0:\n            st.metric(\n                \"Cobertura GPS\",\n                f\"{second_half['gps'].mean()*100:.1f}%\",\n                delta=f\"{trends['gps_coverage']:.1f}%\"\n            )\n    \n    with col_trend3:\n        if trends.get('activity', 0) != 0:\n            st.metric(\n                \"Atividade da Frota\",\n                f\"{len(second_half):,} registros\",\n                delta=f\"{trends['activity']*100:.1f}%\"\n            )\n    \n    # Predições para próximo período - versão melhorada\n    st.subheader(\"📅 O que Esperar nos Próximos 30 Dias\")\n    st.markdown(\"**Planeje-se para estas necessidades previstas da sua frota:**\")\n    \n    predictions = []\n    \n    # Predição de manutenção\n    high_usage_vehicles = df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False).head(10)\n    \n    for placa, total_km in high_usage_vehicles.items():\n        if total_km > 1000:  # Veículos com alta quilometragem\n            vehicle_data = df[df['placa'] == placa]\n            daily_avg = total_km / len(vehicle_data['data'].dt.date.unique())\n            \n            predicted_km_month = daily_avg * 30\n            \n            days_to_maintenance = int(5000/daily_avg)\n            cost_estimate = 800  # Estimativa de custo de manutenção\n            predictions.append({\n                'Tipo': '🔧 Manutenção',\n                'Veículo': placa,\n                'Quando': f'Em {days_to_maintenance} dias',\n                'Custo Estimado': f'R$ {cost_estimate:,}',\n                'Ação Necessária': 'Agendar revisão preventiva',\n                'Confiança': '85%'\n            })\n    \n    # Predição de compliance\n    compliance = analyzer.get_compliance_analysis()\n    if compliance and compliance.get('score_compliance'):\n        declining_vehicles = {k: v for k, v in compliance['score_compliance'].items() if v < 80}\n        \n        for placa in declining_vehicles:\n            predictions.append({\n                'Tipo': '⚠️ Compliance',\n                'Veículo': placa,\n                'Quando': 'Próximas 2 semanas',\n                'Custo Estimado': 'R$ 500-2.000 (multas)',\n                'Ação Necessária': 'Treinamento urgente do motorista',\n                'Confiança': '75%'\n            })\n    \n    # Exibir predições\n    if predictions:\n        predictions_df = pd.DataFrame(predictions)\n        \n        # Colorir por tipo\n        def color_prediction_type(val):\n            if val == 'Manutenção Preventiva':\n                return 'background-color: #fff3cd'\n            elif val == 'Compliance':\n                return 'background-color: #f8d7da'\n            else:\n                return ''\n        \n        styled_predictions = predictions_df.style.applymap(\n            color_prediction_type, \n            subset=['Tipo']\n        )\n        \n        st.dataframe(styled_predictions, use_container_width=True, hide_index=True)\n    else:\n        st.info(\"ℹ️ Nenhuma predição crítica identificada para os próximos 30 dias.\")\n    \n    # Gráfico de tendência temporal\n    st.subheader(\"📊 Visualização de Tendências\")\n    \n    # Agregar dados por semana\n    df['semana'] = df['data'].dt.to_period('W')\n    weekly_trends = df.groupby('semana').agg({\n        'velocidade_km': 'mean',\n        'gps': lambda x: x.mean() * 100,\n        'placa': 'nunique'\n    }).reset_index()\n    \n    weekly_trends['semana'] = weekly_trends['semana'].dt.start_time\n    \n    fig_trends = px.line(\n        weekly_trends,\n        x='semana',\n        y=['velocidade_km', 'gps', 'placa'],\n        title='Tendências Semanais da Frota',\n        labels={'value': 'Valor', 'semana': 'Semana', 'variable': 'Métrica'}\n    )\n    \n    st.plotly_chart(fig_trends, use_container_width=True)\n    \n    # Alertas preditivos\n    st.subheader(\"🚨 Alertas Preditivos\")\n    \n    predictive_alerts = []\n    \n    # Alerta de tendência decrescente na cobertura GPS\n    if trends.get('gps_coverage', 0) < -5:\n        predictive_alerts.append({\n            'Alerta': '📡 Degradação de GPS',\n            'Descrição': 'Cobertura GPS em declínio detectada',\n            'Probabilidade': '80%',\n            'Ação': 'Verificar equipamentos GPS da frota'\n        })\n    \n    # Alerta de aumento de velocidade\n    if trends.get('velocidade', 0) > 5:\n        predictive_alerts.append({\n            'Alerta': '⚡ Aumento de Velocidade',\n            'Descrição': 'Velocidades médias em crescimento',\n            'Probabilidade': '75%',\n            'Ação': 'Reforçar treinamento de condutores'\n        })\n    \n    if predictive_alerts:\n        for alert in predictive_alerts:\n            st.warning(f\"**{alert['Alerta']}** - {alert['Descrição']} (Probabilidade: {alert['Probabilidade']})\")\n            st.write(f\"💡 **Ação Recomendada:** {alert['Ação']}\")\n    else:\n        st.success(\"✅ Nenhum alerta preditivo identificado. Tendências estáveis.\")\n\ndef show_complete_report(insights, insights_generator, analyzer):\n    \"\"\"Mostra relatório completo\"\"\"\n    st.header(\"📋 Relatório Completo de Insights\")\n    \n    # Informações do relatório\n    col_info1, col_info2 = st.columns(2)\n    \n    with col_info1:\n        st.info(f\"\"\"\n        **📊 Informações do Relatório:**\n        - Data de Geração: {datetime.now().strftime('%d/%m/%Y %H:%M')}\n        - Total de Insights: {len(insights)}\n        - Período Analisado: {analyzer.get_kpis()['periodo_dias']} dias\n        \"\"\")\n    \n    with col_info2:\n        st.info(f\"\"\"\n        **🚗 Dados da Frota:**\n        - Veículos Analisados: {analyzer.get_kpis()['total_veiculos']:,}\n        - Registros Processados: {analyzer.get_kpis()['total_registros']:,}\n        - Score Geral: {calculate_general_score(insights):.1f}/100\n        \"\"\")\n    \n    # Filtro por categoria\n    categoria_filtro = st.selectbox(\n        \"Filtrar por categoria:\",\n        [\"Todos\", \"Críticos\", \"Atenção\", \"Informativos\", \"Positivos\"]\n    )\n    \n    filtered_insights = insights\n    if categoria_filtro != \"Todos\":\n        type_map = {\n            \"Críticos\": \"error\",\n            \"Atenção\": \"warning\", \n            \"Informativos\": \"info\",\n            \"Positivos\": \"success\"\n        }\n        filtered_insights = [i for i in insights if i['type'] == type_map[categoria_filtro]]\n    \n    # Exibir insights filtrados\n    if filtered_insights:\n        for i, insight in enumerate(filtered_insights, 1):\n            icon_map = {\n                'error': '🚨',\n                'warning': '⚠️',\n                'info': 'ℹ️',\n                'success': '✅'\n            }\n            \n            icon = icon_map.get(insight['type'], 'ℹ️')\n            \n            with st.expander(f\"{i}. {icon} {insight['title']}\"):\n                col_insight1, col_insight2 = st.columns([3, 1])\n                \n                with col_insight1:\n                    st.write(f\"**📝 Descrição:** {insight['description']}\")\n                    st.write(f\"**💡 Recomendação:** {insight['recommendation']}\")\n                    st.write(f\"**🕒 Gerado em:** {insight['timestamp'].strftime('%d/%m/%Y %H:%M')}\")\n                \n                with col_insight2:\n                    priority_text = {1: \"ALTA\", 2: \"MÉDIA\", 3: \"BAIXA\", 4: \"INFO\"}\n                    st.metric(\"Prioridade\", priority_text.get(insight['priority'], \"N/A\"))\n                    \n                    type_text = {\n                        'error': \"CRÍTICO\",\n                        'warning': \"ATENÇÃO\", \n                        'info': \"INFO\",\n                        'success': \"POSITIVO\"\n                    }\n                    st.metric(\"Categoria\", type_text.get(insight['type'], \"N/A\"))\n    else:\n        st.info(f\"Nenhum insight encontrado para a categoria '{categoria_filtro}'.\")\n    \n    # Botões de ação\n    st.markdown(\"---\")\n    st.subheader(\"📤 Exportar Relatório\")\n    \n    col_export1, col_export2, col_export3 = st.columns(3)\n    \n    with col_export1:\n        # Export texto\n        if st.button(\"📄 Exportar como Texto\", use_container_width=True):\n            report_text = insights_generator.export_insights_to_text()\n            st.download_button(\n                label=\"📥 Download Relatório TXT\",\n                data=report_text,\n                file_name=f\"relatorio_insights_{datetime.now().strftime('%Y%m%d_%H%M')}.txt\",\n                mime=\"text/plain\"\n            )\n    \n    with col_export2:\n        # Export CSV\n        if st.button(\"📊 Exportar como CSV\", use_container_width=True):\n            insights_df = pd.DataFrame([\n                {\n                    'Timestamp': insight['timestamp'],\n                    'Titulo': insight['title'],\n                    'Descricao': insight['description'],\n                    'Recomendacao': insight['recommendation'],\n                    'Tipo': insight['type'],\n                    'Prioridade': insight['priority']\n                }\n                for insight in insights\n            ])\n            \n            csv_data = insights_df.to_csv(index=False).encode('utf-8')\n            st.download_button(\n                label=\"📥 Download Relatório CSV\",\n                data=csv_data,\n                file_name=f\"insights_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\",\n                mime=\"text/csv\"\n            )\n    \n    with col_export3:\n        # Gerar novo relatório\n        if st.button(\"🔄 Gerar Novo Relatório\", use_container_width=True):\n            st.rerun()\n    \n    # Estatísticas do relatório\n    st.markdown(\"---\")\n    st.subheader(\"📈 Estatísticas do Relatório\")\n    \n    col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)\n    \n    with col_stats1:\n        critical_count = len([i for i in insights if i['type'] == 'error'])\n        st.metric(\"🚨 Insights Críticos\", critical_count)\n    \n    with col_stats2:\n        warning_count = len([i for i in insights if i['type'] == 'warning'])\n        st.metric(\"⚠️ Requer Atenção\", warning_count)\n    \n    with col_stats3:\n        positive_count = len([i for i in insights if i['type'] == 'success'])\n        st.metric(\"✅ Pontos Positivos\", positive_count)\n    \n    with col_stats4:\n        total_recommendations = len([i for i in insights if i['recommendation']])\n        st.metric(\"💡 Recomendações\", total_recommendations)\n\ndef calculate_general_score(insights):\n    \"\"\"Calcula score geral baseado nos insights\"\"\"\n    if not insights:\n        return 75  # Score neutro\n    \n    score = 100\n    \n    for insight in insights:\n        if insight['type'] == 'error':\n            score -= 15\n        elif insight['type'] == 'warning':\n            score -= 8\n        elif insight['type'] == 'success':\n            score += 5\n        # info insights não afetam o score\n    \n    return max(0, min(100, score))\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":33766},"pages/6_📄_Relatórios.py":{"content":"\"\"\"Página de Relatórios Avançados\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nfrom utils.pdf_reports import PDFReportGenerator\nfrom utils.data_analyzer import DataAnalyzer\nimport os\n\nst.set_page_config(page_title=\"Relatórios\", page_icon=\"📄\", layout=\"wide\")\nst.title(\"📄 Relatórios Avançados\")\nst.markdown(\"*Sistema completo de geração de relatórios com dados consolidados de todos os painéis*\")\n\n# Carregar dados diretamente da base de dados\ndf_inicial = DatabaseManager.get_dashboard_data()\nif df_inicial.empty:\n    st.warning(\"⚠️ Nenhum dado encontrado. Faça upload de arquivos CSV primeiro.\")\n    st.stop()\nelse:\n    st.success(f\"✅ Dados carregados: {len(df_inicial):,} registros para geração de relatórios\")\n\n# Carregar dados com cache para performance\n@st.cache_data(ttl=300)  # Cache por 5 minutos\ndef load_report_data():\n    \"\"\"Carrega dados otimizados para relatórios\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        summary = DatabaseManager.get_fleet_summary()\n        \n        if df.empty:\n            return None, None\n            \n        return df, summary\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {str(e)}\")\n        return None, None\n\n# Configurações do relatório\nst.sidebar.header(\"⚙️ Configurações do Relatório\")\n\n# Filtros de período\nperiodo_opcoes = {\n    \"Últimos 7 dias\": 7,\n    \"Últimos 30 dias\": 30,\n    \"Últimos 90 dias\": 90,\n    \"Todos os dados\": None\n}\n\nperiodo_selecionado = st.sidebar.selectbox(\n    \"📅 Período:\",\n    options=list(periodo_opcoes.keys()),\n    index=1  # Padrão: últimos 30 dias\n)\n\n# Tipos de relatório\ntipo_relatorio = st.sidebar.selectbox(\n    \"📊 Tipo de Relatório:\",\n    [\n        \"📋 Relatório Executivo Completo\",\n        \"🚗 Análise Detalhada por Veículo\", \n        \"⚡ Relatório de Performance\",\n        \"🚨 Relatório de Conformidade\",\n        \"📈 Análise de Tendências\",\n        \"🔍 Relatório Personalizado\"\n    ]\n)\n\n# Opções de formato\nformato_saida = st.sidebar.selectbox(\n    \"📄 Formato:\",\n    [\"PDF Profissional\", \"Visualização Web\", \"Dados CSV\", \"Relatório Completo (PDF + CSV)\"]\n)\n\n# Incluir gráficos\nincluir_graficos = st.sidebar.checkbox(\"📊 Incluir Gráficos\", value=True)\nincluir_mapas = st.sidebar.checkbox(\"🗺️ Incluir Mapas\", value=False)\n\n# Carregar dados\ndf, summary = load_report_data()\n\nif df is None or df.empty:\n    st.error(\"❌ Não foi possível carregar os dados para o relatório.\")\n    st.stop()\n\n# Validar colunas essenciais\nrequired_columns = ['data', 'placa', 'cliente', 'velocidade_km', 'gps']\nmissing_columns = [col for col in required_columns if col not in df.columns]\nif missing_columns:\n    st.error(f\"❌ Colunas essenciais ausentes nos dados: {', '.join(missing_columns)}\")\n    st.stop()\n\n# Filtrar dados por período se especificado - com validação de tipo\ndf_filtered = df.copy()\n\n# Converter coluna de data para datetime de forma segura\nif 'data' in df_filtered.columns:\n    df_filtered['data'] = pd.to_datetime(df_filtered['data'], errors='coerce')\n    df_filtered = df_filtered.dropna(subset=['data'])\n\nif periodo_opcoes[periodo_selecionado] is not None:\n    dias = periodo_opcoes[periodo_selecionado]\n    cutoff_date = datetime.now() - timedelta(days=dias)\n    \n    # Filtrar apenas se a coluna de data está disponível e foi convertida com sucesso\n    if 'data' in df_filtered.columns and not df_filtered.empty:\n        # Verificar se a coluna de data tem timezone e ajustar a comparação\n        if hasattr(df_filtered['data'].dtype, 'tz') and df_filtered['data'].dtype.tz is not None:\n            # Se dados têm timezone, converter cutoff_date para timezone-aware\n            from datetime import timezone\n            cutoff_date = cutoff_date.replace(tzinfo=timezone.utc)\n        else:\n            # Se dados são naive, garantir que cutoff_date também seja naive\n            cutoff_date = cutoff_date.replace(tzinfo=None)\n        \n        df_filtered = df_filtered[df_filtered['data'] >= cutoff_date]\n        st.info(f\"📅 Dados filtrados: {len(df_filtered):,} registros dos últimos {dias} dias\")\n    else:\n        st.warning(\"⚠️ Não foi possível filtrar por período - dados de data indisponíveis\")\nelse:\n    st.info(f\"📅 Usando todos os dados: {len(df_filtered):,} registros\")\n\n# Análise preliminar para o relatório\nanalyzer = DataAnalyzer(df_filtered)\n\n# ========== DEFINIÇÕES DAS FUNÇÕES ==========\n\ndef show_report_preview(df, summary, analyzer, tipo_relatorio):\n    \"\"\"Mostra pré-visualização do relatório\"\"\"\n    st.subheader(\"👁️ Pré-visualização do Relatório\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para o período selecionado.\")\n        col1, col2, col3, col4 = st.columns(4)\n        with col1:\n            st.metric(\"📊 Total de Registros\", \"0\")\n        with col2:\n            st.metric(\"🚗 Veículos\", \"0\")\n        with col3:\n            st.metric(\"🏢 Clientes\", \"0\")\n        with col4:\n            st.metric(\"📅 Período\", \"0 dias\")\n        return\n    \n    # Estatísticas gerais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"📊 Total de Registros\", f\"{len(df):,}\")\n    with col2:\n        st.metric(\"🚗 Veículos\", f\"{df['placa'].nunique()}\")\n    with col3:\n        st.metric(\"🏢 Clientes\", f\"{df['cliente'].nunique()}\")\n    with col4:\n        # Calcular período de forma segura\n        try:\n            periodo_dias = (df['data'].max() - df['data'].min()).days\n            if pd.isna(periodo_dias):\n                periodo_dias = 0\n        except:\n            periodo_dias = 0\n        st.metric(\"📅 Período\", f\"{periodo_dias} dias\")\n    \n    # Conteúdo baseado no tipo de relatório\n    if \"Executivo\" in tipo_relatorio:\n        show_executive_preview(df, summary, analyzer)\n    elif \"Veículo\" in tipo_relatorio:\n        show_vehicle_preview(df, analyzer)\n    elif \"Performance\" in tipo_relatorio:\n        show_performance_preview(df, analyzer)\n    elif \"Conformidade\" in tipo_relatorio:\n        show_compliance_preview(df, analyzer)\n    elif \"Tendências\" in tipo_relatorio:\n        show_trends_preview(df, analyzer)\n    else:\n        show_custom_preview(df, analyzer)\n\ndef show_executive_preview(df, summary, analyzer):\n    \"\"\"Preview do relatório executivo\"\"\"\n    st.markdown(\"### 📋 Resumo Executivo\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para o resumo executivo.\")\n        return\n    \n    # KPIs principais\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        velocidade_media = df['velocidade_km'].mean()\n        st.metric(\"⚡ Velocidade Média\", f\"{velocidade_media:.1f} km/h\")\n        \n    with col2:\n        gps_coverage = (df['gps'].sum() / len(df)) * 100 if len(df) > 0 else 0\n        st.metric(\"📡 Cobertura GPS\", f\"{gps_coverage:.1f}%\")\n        \n    with col3:\n        utilizacao = len(df[df['velocidade_km'] > 0]) / len(df) * 100 if len(df) > 0 else 0\n        st.metric(\"🚗 Taxa de Utilização\", f\"{utilizacao:.1f}%\")\n    \n    # Gráfico de utilização por veículo\n    vehicle_usage = df.groupby('placa').agg({\n        'velocidade_km': 'mean',\n        'data': 'count'\n    }).reset_index()\n    vehicle_usage.columns = ['Veículo', 'Velocidade Média', 'Registros']\n    \n    fig = px.bar(vehicle_usage, x='Veículo', y='Registros', \n                 title=\"📊 Registros por Veículo\")\n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_vehicle_preview(df, analyzer):\n    \"\"\"Preview da análise por veículo\"\"\"\n    st.markdown(\"### 🚗 Análise por Veículo\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para análise por veículo.\")\n        return\n    \n    # Seleção de veículo\n    veiculos = sorted(df['placa'].unique())\n    veiculo_selecionado = st.selectbox(\"Selecione um veículo:\", veiculos)\n    \n    df_veiculo = df[df['placa'] == veiculo_selecionado]\n    \n    # Verificar se há dados para o veículo selecionado\n    if df_veiculo.empty:\n        st.warning(f\"⚠️ Nenhum dado encontrado para o veículo {veiculo_selecionado}.\")\n        return\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"📊 Registros\", f\"{len(df_veiculo):,}\")\n    with col2:\n        vel_media = df_veiculo['velocidade_km'].mean()\n        st.metric(\"⚡ Velocidade Média\", f\"{vel_media:.1f} km/h\")\n    with col3:\n        vel_max = df_veiculo['velocidade_km'].max()\n        st.metric(\"🏎️ Velocidade Máxima\", f\"{vel_max:.1f} km/h\")\n    with col4:\n        utilizacao = len(df_veiculo[df_veiculo['velocidade_km'] > 0]) / len(df_veiculo) * 100 if len(df_veiculo) > 0 else 0\n        st.metric(\"📈 Utilização\", f\"{utilizacao:.1f}%\")\n    \n    # Gráfico de velocidade ao longo do tempo\n    df_veiculo_sample = df_veiculo.sample(min(500, len(df_veiculo)))\n    fig = px.line(df_veiculo_sample, x='data', y='velocidade_km',\n                  title=f\"📈 Velocidade ao Longo do Tempo - {veiculo_selecionado}\")\n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_performance_preview(df, analyzer):\n    \"\"\"Preview do relatório de performance\"\"\"\n    st.markdown(\"### ⚡ Análise de Performance\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para análise de performance.\")\n        return\n    \n    # Top performers\n    performance = df.groupby('placa').agg({\n        'velocidade_km': 'mean',\n        'odometro_periodo_km': 'sum',\n        'data': 'count'\n    }).reset_index()\n    performance.columns = ['Veículo', 'Velocidade Média', 'KM Total', 'Registros']\n    performance = performance.sort_values('KM Total', ascending=False)\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.markdown(\"**🏆 Top 5 - Maior Quilometragem**\")\n        top_km = performance.head(5)\n        st.dataframe(top_km[['Veículo', 'KM Total']], hide_index=True)\n        \n    with col2:\n        st.markdown(\"**⚡ Top 5 - Maior Velocidade Média**\")\n        top_speed = performance.sort_values('Velocidade Média', ascending=False).head(5)\n        st.dataframe(top_speed[['Veículo', 'Velocidade Média']], hide_index=True)\n    \n    # Gráfico de eficiência\n    fig = px.scatter(performance, x='Velocidade Média', y='KM Total', \n                     size='Registros', text='Veículo',\n                     title=\"📊 Matriz de Performance: Velocidade vs Quilometragem\")\n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_compliance_preview(df, analyzer):\n    \"\"\"Preview do relatório de conformidade\"\"\"\n    st.markdown(\"### 🚨 Análise de Conformidade\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para análise de conformidade.\")\n        return\n    \n    # Análise de velocidade\n    excesso_velocidade = df[df['velocidade_km'] > 80]\n    violacoes_criticas = df[df['velocidade_km'] > 100]\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\"⚠️ Excessos de Velocidade\", len(excesso_velocidade))\n    with col2:\n        st.metric(\"🚨 Violações Críticas\", len(violacoes_criticas))\n    with col3:\n        conformidade = (1 - len(excesso_velocidade) / len(df)) * 100 if len(df) > 0 else 100\n        st.metric(\"✅ Taxa de Conformidade\", f\"{conformidade:.1f}%\")\n    \n    if len(excesso_velocidade) > 0:\n        # Veículos com mais violações\n        violacoes_por_veiculo = excesso_velocidade.groupby('placa').size().reset_index(name='Violações')\n        violacoes_por_veiculo = violacoes_por_veiculo.sort_values('Violações', ascending=False)\n        \n        fig = px.bar(violacoes_por_veiculo.head(10), x='placa', y='Violações',\n                     title=\"🚨 Veículos com Mais Violações de Velocidade\")\n        st.plotly_chart(fig, use_container_width=True)\n\ndef show_trends_preview(df, analyzer):\n    \"\"\"Preview da análise de tendências\"\"\"\n    st.markdown(\"### 📈 Análise de Tendências\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para análise de tendências.\")\n        return\n    \n    # Tendências por dia\n    df['dia'] = df['data'].dt.date\n    tendencias_diarias = df.groupby('dia').agg({\n        'velocidade_km': 'mean',\n        'data': 'count'\n    }).reset_index()\n    tendencias_diarias.columns = ['Data', 'Velocidade Média', 'Registros']\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        fig1 = px.line(tendencias_diarias, x='Data', y='Velocidade Média',\n                       title=\"📈 Tendência de Velocidade Média\")\n        st.plotly_chart(fig1, use_container_width=True)\n        \n    with col2:\n        fig2 = px.line(tendencias_diarias, x='Data', y='Registros',\n                       title=\"📊 Tendência de Atividade\")\n        st.plotly_chart(fig2, use_container_width=True)\n\ndef show_custom_preview(df, analyzer):\n    \"\"\"Preview do relatório personalizado\"\"\"\n    st.markdown(\"### 🔍 Relatório Personalizado\")\n    st.info(\"Configure suas métricas personalizadas na aba Configurações\")\n\ndef show_report_dashboard(df, analyzer):\n    \"\"\"Dashboard interativo do relatório\"\"\"\n    st.subheader(\"📊 Dashboard Interativo\")\n    \n    # Verificar se há dados\n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado disponível para o dashboard.\")\n        return\n    \n    # Filtros interativos\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        veiculos_selecionados = st.multiselect(\n            \"🚗 Veículos:\",\n            options=sorted(df['placa'].unique()),\n            default=sorted(df['placa'].unique())[:5]  # Primeiros 5 por padrão\n        )\n    \n    with col2:\n        # Tratamento seguro para datas\n        try:\n            min_date = df['data'].min()\n            if pd.isna(min_date):\n                min_date = datetime.now().date() - timedelta(days=30)\n            else:\n                min_date = min_date.date()\n            data_inicio = st.date_input(\"📅 Data Início:\", value=min_date)\n        except:\n            data_inicio = st.date_input(\"📅 Data Início:\", value=datetime.now().date() - timedelta(days=30))\n    \n    with col3:\n        # Tratamento seguro para datas\n        try:\n            max_date = df['data'].max()\n            if pd.isna(max_date):\n                max_date = datetime.now().date()\n            else:\n                max_date = max_date.date()\n            data_fim = st.date_input(\"📅 Data Fim:\", value=max_date)\n        except:\n            data_fim = st.date_input(\"📅 Data Fim:\", value=datetime.now().date())\n    \n    # Filtrar dados\n    df_filtered = df[\n        (df['placa'].isin(veiculos_selecionados)) &\n        (df['data'].dt.date >= data_inicio) &\n        (df['data'].dt.date <= data_fim)\n    ]\n    \n    if df_filtered.empty:\n        st.warning(\"⚠️ Nenhum dado encontrado com os filtros aplicados\")\n        return\n    \n    # Métricas principais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"📊 Registros Filtrados\", f\"{len(df_filtered):,}\")\n    with col2:\n        vel_media = df_filtered['velocidade_km'].mean()\n        st.metric(\"⚡ Velocidade Média\", f\"{vel_media:.1f} km/h\")\n    with col3:\n        km_total = df_filtered['odometro_periodo_km'].sum()\n        st.metric(\"🛣️ KM Total\", f\"{km_total:.1f}\")\n    with col4:\n        gps_coverage = (df_filtered['gps'].sum() / len(df_filtered)) * 100\n        st.metric(\"📡 GPS Coverage\", f\"{gps_coverage:.1f}%\")\n    \n    # Gráficos principais\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        # Distribuição de velocidade\n        fig1 = px.histogram(df_filtered, x='velocidade_km', nbins=30,\n                           title=\"📊 Distribuição de Velocidade\")\n        st.plotly_chart(fig1, use_container_width=True)\n    \n    with col2:\n        # Atividade por hora\n        df_filtered['hora'] = df_filtered['data'].dt.hour\n        atividade_hora = df_filtered.groupby('hora').size().reset_index(name='Registros')\n        fig2 = px.bar(atividade_hora, x='hora', y='Registros',\n                      title=\"📈 Atividade por Hora do Dia\")\n        st.plotly_chart(fig2, use_container_width=True)\n\ndef show_advanced_settings():\n    \"\"\"Configurações avançadas do relatório\"\"\"\n    st.subheader(\"⚙️ Configurações Avançadas\")\n    \n    # Configurações de métricas\n    st.markdown(\"### 📊 Métricas Personalizadas\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.checkbox(\"📈 Incluir análise de tendências\", value=True)\n        st.checkbox(\"🚨 Incluir alertas de segurança\", value=True)\n        st.checkbox(\"⚡ Incluir métricas de performance\", value=True)\n        st.checkbox(\"🗺️ Incluir análise geográfica\", value=False)\n    \n    with col2:\n        st.number_input(\"🏎️ Limite de velocidade (km/h):\", min_value=50, max_value=120, value=80)\n        st.number_input(\"🔋 Bateria baixa (V):\", min_value=10.0, max_value=15.0, value=12.0)\n        st.slider(\"📊 Número de veículos no top ranking:\", 5, 20, 10)\n        \n    # Configurações de formato\n    st.markdown(\"### 📄 Formatação do Relatório\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.selectbox(\"🎨 Tema do relatório:\", [\"Profissional\", \"Moderno\", \"Clássico\"])\n        st.selectbox(\"📊 Estilo dos gráficos:\", [\"Plotly\", \"Matplotlib\", \"Seaborn\"])\n    \n    with col2:\n        st.selectbox(\"📄 Tamanho da página:\", [\"A4\", \"Letter\", \"A3\"])\n        st.selectbox(\"🔤 Idioma:\", [\"Português\", \"English\", \"Español\"])\n\ndef show_download_options(df, summary, analyzer, tipo_relatorio, formato_saida, incluir_graficos):\n    \"\"\"Opções de download do relatório\"\"\"\n    st.subheader(\"📥 Download do Relatório\")\n    \n    # Informações do relatório a ser gerado\n    st.markdown(f\"**📊 Tipo:** {tipo_relatorio}\")\n    st.markdown(f\"**📄 Formato:** {formato_saida}\")\n    st.markdown(f\"**📅 Período:** {len(df):,} registros\")\n    st.markdown(f\"**🚗 Veículos:** {df['placa'].nunique()}\")\n    \n    # Botões de geração\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        if st.button(\"📄 Gerar PDF Profissional\", type=\"primary\"):\n            with st.spinner(\"🔄 Gerando relatório PDF...\"):\n                try:\n                    generator = PDFReportGenerator()\n                    # Usar método existente até implementar o avançado\n                    pdf_path = generator.generate_fleet_report()\n                    \n                    if os.path.exists(pdf_path):\n                        with open(pdf_path, \"rb\") as file:\n                            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                            filename = f\"relatorio_{tipo_relatorio.split()[1].lower()}_{timestamp}.pdf\"\n                            \n                            st.download_button(\n                                label=\"⬇️ Baixar PDF\",\n                                data=file.read(),\n                                file_name=filename,\n                                mime=\"application/pdf\"\n                            )\n                        st.success(\"✅ Relatório PDF gerado com sucesso!\")\n                    else:\n                        st.error(\"❌ Erro ao gerar relatório PDF\")\n                except Exception as e:\n                    st.error(f\"❌ Erro: {str(e)}\")\n    \n    with col2:\n        if st.button(\"📊 Exportar Dados CSV\"):\n            csv_data = df.to_csv(index=False)\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filename = f\"dados_frota_{timestamp}.csv\"\n            \n            st.download_button(\n                label=\"⬇️ Baixar CSV\",\n                data=csv_data,\n                file_name=filename,\n                mime=\"text/csv\"\n            )\n            st.success(\"✅ Dados CSV prontos para download!\")\n    \n    with col3:\n        if st.button(\"📈 Relatório Completo\"):\n            st.info(\"🚧 Gerando relatório completo (PDF + CSV + Gráficos)...\")\n            st.markdown(\"*Funcionalidade em desenvolvimento*\")\n    \n    # Histórico de relatórios\n    st.markdown(\"### 📋 Histórico de Relatórios\")\n    st.info(\"📁 Últimos relatórios gerados aparecerão aqui\")\n\n# ========== SEÇÃO PRINCIPAL - EXECUTADA APÓS DEFINIÇÕES ==========\n# Seção principal de geração de relatórios\nst.header(\"📊 Geração de Relatórios\")\n\n# Tabs para diferentes visualizações\ntab1, tab2, tab3, tab4 = st.tabs([\"📋 Pré-visualização\", \"📊 Dashboard\", \"⚙️ Configurações\", \"📥 Download\"])\n\nwith tab1:\n    show_report_preview(df_filtered, summary, analyzer, tipo_relatorio)\n\nwith tab2:\n    show_report_dashboard(df_filtered, analyzer)\n\nwith tab3:\n    show_advanced_settings()\n\nwith tab4:\n    show_download_options(df_filtered, summary, analyzer, tipo_relatorio, formato_saida, incluir_graficos)","size_bytes":21086},"utils/alert_system.py":{"content":"\"\"\"Sistema de Alertas em Tempo Real\"\"\"\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any\nfrom database.db_manager import DatabaseManager\n\nclass AlertSystem:\n    def __init__(self):\n        self.alert_configs = {\n            'velocidade_maxima': 80,\n            'velocidade_critica': 100,\n            'bateria_baixa': 12,\n            'bateria_critica': 11,\n            'tempo_parado_max': 180,  # minutos\n            'uso_noturno_inicio': 22,\n            'uso_noturno_fim': 6\n        }\n    \n    def check_realtime_alerts(self) -> List[Dict[str, Any]]:\n        \"\"\"Verifica alertas em tempo real\"\"\"\n        alerts = []\n        df = DatabaseManager.get_dashboard_data()\n        \n        if df.empty:\n            return alerts\n        \n        # Validar e converter coluna de data\n        if 'data' not in df.columns:\n            return alerts\n            \n        # Converter para datetime de forma segura\n        df = df.copy()\n        df['data'] = pd.to_datetime(df['data'], errors='coerce')\n        \n        # Remover registros com data inválida\n        df = df.dropna(subset=['data'])\n        \n        if df.empty:\n            return alerts\n        \n        # Filtrar últimas 24h - timezone correto\n        from datetime import timezone\n        cutoff = datetime.now(timezone.utc) - timedelta(hours=24)\n        \n        # Verificar se dados têm timezone\n        if df['data'].dt.tz is not None:\n            # Se dados não estão em UTC, converter\n            if df['data'].dt.tz != timezone.utc:\n                cutoff = cutoff.astimezone(df['data'].dt.tz)\n        else:\n            # Se dados são naive, usar cutoff naive\n            cutoff = cutoff.replace(tzinfo=None)\n        \n        df_recent = df[df['data'] >= cutoff]\n        \n        # Alertas de velocidade\n        speed_alerts = self._check_speed_alerts(df_recent)\n        alerts.extend(speed_alerts)\n        \n        # Alertas de bateria\n        battery_alerts = self._check_battery_alerts(df_recent)\n        alerts.extend(battery_alerts)\n        \n        # Alertas de uso noturno\n        night_alerts = self._check_night_usage(df_recent)\n        alerts.extend(night_alerts)\n        \n        return alerts\n    \n    def _check_speed_alerts(self, df: pd.DataFrame) -> List[Dict]:\n        alerts = []\n        if 'velocidade_km' not in df.columns:\n            return alerts\n            \n        # Filtrar apenas registros com velocidade > 0 para alertas\n        df_moving = df[df['velocidade_km'] > 0]\n        \n        for _, row in df_moving.iterrows():\n            vel = float(row['velocidade_km'])\n            if vel > self.alert_configs['velocidade_critica']:\n                alerts.append({\n                    'tipo': 'Velocidade Crítica',\n                    'severidade': 'Alta',\n                    'veiculo': row['placa'],\n                    'valor': f\"{vel:.1f} km/h\",\n                    'timestamp': row['data'],\n                    'localizacao': row.get('endereco', 'Localização não disponível')\n                })\n            elif vel > self.alert_configs['velocidade_maxima']:\n                alerts.append({\n                    'tipo': 'Excesso de Velocidade',\n                    'severidade': 'Média',\n                    'veiculo': row['placa'],\n                    'valor': f\"{vel:.1f} km/h\",\n                    'timestamp': row['data'],\n                    'localizacao': row.get('endereco', 'Localização não disponível')\n                })\n        return alerts\n    \n    def _check_battery_alerts(self, df: pd.DataFrame) -> List[Dict]:\n        alerts = []\n        if 'battery_level' in df.columns:\n            # Converter battery_level para numérico (vem como string)\n            df_copy = df.copy()\n            df_copy['bateria_num'] = pd.to_numeric(df_copy['battery_level'], errors='coerce')\n            \n            # Filtrar apenas valores válidos\n            df_copy = df_copy.dropna(subset=['bateria_num'])\n            \n            if not df_copy.empty:\n                # Alertas de bateria baixa (abaixo de 12V)\n                low_battery = df_copy[df_copy['bateria_num'] < self.alert_configs['bateria_baixa']]\n                \n                for _, row in low_battery.iterrows():\n                    bat = row['bateria_num']\n                    severity = 'Alta' if bat < self.alert_configs['bateria_critica'] else 'Média'\n                    alerts.append({\n                        'tipo': 'Bateria Baixa',\n                        'severidade': severity,\n                        'veiculo': row['placa'],\n                        'valor': f\"{bat:.1f}V\",\n                        'timestamp': row['data'],\n                        'localizacao': row.get('endereco', 'Localização não disponível')\n                    })\n        return alerts\n    \n    def _check_night_usage(self, df: pd.DataFrame) -> List[Dict]:\n        alerts = []\n        if 'data' not in df.columns:\n            return alerts\n            \n        # Trabalhar com cópia para não modificar original\n        df_copy = df.copy()\n        \n        # Converter para datetime se necessário\n        if not pd.api.types.is_datetime64_any_dtype(df_copy['data']):\n            df_copy['data'] = pd.to_datetime(df_copy['data'], errors='coerce')\n            \n        # Remover registros com data inválida\n        df_copy = df_copy.dropna(subset=['data'])\n        \n        if df_copy.empty:\n            return alerts\n            \n        df_copy['hora'] = df_copy['data'].dt.hour\n        night_usage = df_copy[\n            (df_copy['hora'] >= self.alert_configs['uso_noturno_inicio']) | \n            (df_copy['hora'] <= self.alert_configs['uso_noturno_fim'])\n        ]\n        \n        if len(night_usage) > 10:  # Mais de 10 registros noturnos\n            for veiculo in night_usage['placa'].unique():\n                count = len(night_usage[night_usage['placa'] == veiculo])\n                if count > 5:\n                    alerts.append({\n                        'tipo': 'Uso Noturno Frequente',\n                        'severidade': 'Baixa',\n                        'veiculo': veiculo,\n                        'valor': f\"{count} ocorrências\",\n                        'timestamp': night_usage['data'].max(),\n                        'localizacao': 'Múltiplas'\n                    })\n        return alerts\n    \n    def get_alert_summary(self) -> Dict[str, Any]:\n        \"\"\"Resumo dos alertas\"\"\"\n        alerts = self.check_realtime_alerts()\n        summary = {\n            'total_alerts': len(alerts),\n            'high_severity': len([a for a in alerts if a['severidade'] == 'Alta']),\n            'medium_severity': len([a for a in alerts if a['severidade'] == 'Média']),\n            'low_severity': len([a for a in alerts if a['severidade'] == 'Baixa']),\n            'by_type': {}\n        }\n        \n        for alert in alerts:\n            tipo = alert['tipo']\n            if tipo not in summary['by_type']:\n                summary['by_type'][tipo] = 0\n            summary['by_type'][tipo] += 1\n            \n        return summary","size_bytes":7053},"utils/csv_processor.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport streamlit as st\nimport os\nimport re\nimport sys\nsys.path.append('.')\nfrom database.db_manager import DatabaseManager\n\nclass CSVProcessor:\n    \"\"\"Classe para processar arquivos CSV de dados telemáticos\"\"\"\n    \n    # Campos obrigatórios do CSV\n    REQUIRED_FIELDS = [\n        'Cliente', 'Placa', 'Ativo', 'Data', 'Data (GPRS)',\n        'Velocidade (Km)', 'Ignição', 'Motorista', 'GPS', 'Gprs',\n        'Localização', 'Endereço', 'Tipo do Evento', 'Cerca',\n        'Saida', 'Entrada', 'Pacote', 'Odômetro do período (Km)',\n        'Horímetro do período', 'Horímetro embarcado',\n        'Odômetro embarcado (Km)', 'Bateria', 'Imagem', 'Tensão', 'Bloqueado'\n    ]\n    \n    def __init__(self):\n        \"\"\"Inicializa o processador de CSV\"\"\"\n        self.data = None\n        self.validation_errors = []\n        self.use_database = True  # Enable database integration\n        \n    def normalize_column_name(self, col_name):\n        \"\"\"Normaliza nome da coluna removendo espaços extras e caracteres especiais\"\"\"\n        # Remover espaços extras e normalizar\n        normalized = re.sub(r'\\s+', ' ', str(col_name).strip())\n        return normalized\n    \n    def validate_csv_structure(self, df):\n        \"\"\"Valida a estrutura do CSV\"\"\"\n        self.validation_errors = []\n        \n        # Normalizar nomes das colunas - handle double spaces and irregular spacing\n        df.columns = [' '.join(col.strip().split()) for col in df.columns]\n        \n        # Criar mapeamento de colunas normalizadas\n        normalized_columns = {self.normalize_column_name(col): col for col in df.columns}\n        \n        # Verificar se todos os campos obrigatórios estão presentes\n        missing_fields = []\n        for field in self.REQUIRED_FIELDS:\n            field_normalized = self.normalize_column_name(field)\n            if field_normalized not in normalized_columns:\n                missing_fields.append(field)\n        \n        if missing_fields:\n            self.validation_errors.append(f\"Campos obrigatórios ausentes: {', '.join(missing_fields)}\")\n        \n        # Verificar se há dados\n        if df.empty:\n            self.validation_errors.append(\"O arquivo CSV está vazio\")\n        \n        # Verificar tipos de dados básicos\n        try:\n            # Tentar converter datas (formato brasileiro)\n            if 'Data' in df.columns:\n                pd.to_datetime(df['Data'], errors='coerce', dayfirst=True)\n            if 'Data (GPRS)' in df.columns:\n                pd.to_datetime(df['Data (GPRS)'], errors='coerce', dayfirst=True)\n        except Exception as e:\n            self.validation_errors.append(f\"Erro na validação de datas: {str(e)}\")\n        \n        return len(self.validation_errors) == 0\n    \n    def clean_and_standardize_data(self, df):\n        \"\"\"Limpa e padroniza os dados do CSV\"\"\"\n        try:\n            # Criar cópia do DataFrame\n            clean_df = df.copy()\n            \n            # Padronizar nomes das colunas\n            column_mapping = {\n                'Cliente': 'cliente',\n                'Placa': 'placa',\n                'Ativo': 'ativo',\n                'Data': 'data',\n                'Data (GPRS)': 'data_gprs',\n                'Velocidade (Km)': 'velocidade_km',\n                'Ignição': 'ignicao',\n                'Motorista': 'motorista',\n                'GPS': 'gps',\n                'Gprs': 'gprs',\n                'Localização': 'localizacao',\n                'Endereço': 'endereco',\n                'Tipo do Evento': 'tipo_evento',\n                'Cerca': 'cerca',\n                'Saida': 'saida',\n                'Entrada': 'entrada',\n                'Pacote': 'pacote',\n                'Odômetro do período (Km)': 'odometro_periodo_km',\n                'Odômetro do período  (Km)': 'odometro_periodo_km',  # Handle double space variant\n                'Horímetro do período': 'horimetro_periodo',\n                'Horímetro embarcado': 'horimetro_embarcado',\n                'Odômetro embarcado (Km)': 'odometro_embarcado_km',\n                'Bateria': 'bateria',\n                'Imagem': 'imagem',\n                'Tensão': 'tensao',\n                'Bloqueado': 'bloqueado'\n            }\n            \n            clean_df = clean_df.rename(columns=column_mapping)\n            \n            # Converter datas com formato brasileiro (DD/MM/YYYY)\n            clean_df['data'] = pd.to_datetime(clean_df['data'], errors='coerce', dayfirst=True)\n            clean_df['data_gprs'] = pd.to_datetime(clean_df['data_gprs'], errors='coerce', dayfirst=True)\n            \n            # Converter velocidade para numérico\n            clean_df['velocidade_km'] = pd.to_numeric(clean_df['velocidade_km'], errors='coerce')\n            \n            # Converter GPS e GPRS para booleano\n            clean_df['gps'] = clean_df['gps'].astype(str).str.strip() == '1'\n            clean_df['gprs'] = clean_df['gprs'].astype(str).str.strip() == '1'\n            \n            # Converter bloqueado para booleano\n            clean_df['bloqueado'] = clean_df['bloqueado'].astype(str).str.strip() == '1'\n            \n            # Converter odômetros para numérico\n            clean_df['odometro_periodo_km'] = pd.to_numeric(clean_df['odometro_periodo_km'], errors='coerce')\n            clean_df['odometro_embarcado_km'] = pd.to_numeric(clean_df['odometro_embarcado_km'], errors='coerce')\n            \n            # Converter tensão para numérico\n            clean_df['tensao'] = pd.to_numeric(clean_df['tensao'], errors='coerce')\n            \n            # Limpar campos de texto\n            text_fields = ['cliente', 'placa', 'ativo', 'motorista', 'endereco', 'tipo_evento']\n            for field in text_fields:\n                if field in clean_df.columns:\n                    clean_df[field] = clean_df[field].astype(str).str.strip()\n            \n            # Padronizar placas (remover espaços e converter para maiúsculas)\n            clean_df['placa'] = clean_df['placa'].str.upper().str.replace(' ', '')\n            \n            # Adicionar timestamp de processamento\n            clean_df['processed_at'] = datetime.now()\n            \n            return clean_df\n            \n        except Exception as e:\n            self.validation_errors.append(f\"Erro na limpeza dos dados: {str(e)}\")\n            return None\n    \n    def convert_time_to_hours(self, time_str):\n        \"\"\"Converte string de tempo (HH:MM:SS) para horas decimais\"\"\"\n        if pd.isna(time_str) or time_str == '':\n            return 0.0\n        \n        try:\n            time_str = str(time_str).strip()\n            if ':' in time_str:\n                parts = time_str.split(':')\n                hours = int(parts[0])\n                minutes = int(parts[1]) if len(parts) > 1 else 0\n                seconds = int(parts[2]) if len(parts) > 2 else 0\n                return hours + minutes/60 + seconds/3600\n            else:\n                return float(time_str)\n        except:\n            return 0.0\n    \n    def process_csv_file(self, uploaded_file):\n        \"\"\"Processa arquivo CSV completo\"\"\"\n        try:\n            # Detectar separador e encoding do arquivo\n            separator = ','\n            encoding = 'utf-8'\n            \n            # Tentar diferentes separadores e encodings\n            sample = uploaded_file.read(1024)\n            uploaded_file.seek(0)\n            \n            if isinstance(sample, bytes):\n                sample_str = sample.decode('utf-8', errors='ignore')\n            else:\n                sample_str = sample\n            \n            # Detectar separador (vírgula vs ponto e vírgula)\n            if sample_str.count(';') > sample_str.count(','):\n                separator = ';'\n            \n            # Tentar ler com diferentes encodings\n            df = None\n            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for enc in encodings_to_try:\n                try:\n                    uploaded_file.seek(0)\n                    df = pd.read_csv(uploaded_file, sep=separator, encoding=enc)\n                    encoding = enc\n                    break\n                except:\n                    continue\n            \n            if df is None:\n                # Fall back para leitura padrão\n                uploaded_file.seek(0)\n                df = pd.read_csv(uploaded_file, sep=separator)\n            \n            st.info(f\"📄 Arquivo lido com separador '{separator}' e encoding '{encoding}'\")\n            \n            # Validar estrutura\n            if not self.validate_csv_structure(df):\n                return None, self.validation_errors\n            \n            # Limpar e padronizar dados\n            clean_df = self.clean_and_standardize_data(df)\n            \n            if clean_df is None:\n                return None, self.validation_errors\n            \n            # Converter horímetros\n            if 'horimetro_periodo' in clean_df.columns:\n                clean_df['horimetro_periodo_horas'] = clean_df['horimetro_periodo'].apply(\n                    self.convert_time_to_hours\n                )\n            \n            # Salvar dados processados\n            filename = getattr(uploaded_file, 'name', 'uploaded_file.csv')\n            self.save_processed_data(clean_df, filename)\n            \n            self.data = clean_df\n            return clean_df, []\n            \n        except Exception as e:\n            error_msg = f\"Erro no processamento do arquivo: {str(e)}\"\n            return None, [error_msg]\n    \n    def save_processed_data(self, df, filename=None):\n        \"\"\"Salva dados processados na base de dados e como backup\"\"\"\n        try:\n            # Salvar na base de dados se habilitado\n            if self.use_database:\n                result = DatabaseManager.migrate_csv_to_database_from_df(df, filename)\n                if not result['success']:\n                    st.error(f\"Erro ao salvar na base de dados: {result['error']}\")\n                    # Fall back to file save\n                    self.use_database = False\n            \n            # Sempre manter backup em arquivo\n            os.makedirs('data', exist_ok=True)\n            df.to_csv('data/processed_data.csv', index=False)\n            \n            # Salvar backup com timestamp\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            df.to_csv(f'data/backup_{timestamp}.csv', index=False)\n            \n        except Exception as e:\n            st.error(f\"Erro ao salvar dados: {str(e)}\")\n    \n    def get_data_summary(self, df):\n        \"\"\"Gera resumo dos dados processados\"\"\"\n        if df is None or df.empty:\n            return {}\n        \n        summary = {\n            'total_registros': len(df),\n            'total_veiculos': df['placa'].nunique(),\n            'total_clientes': df['cliente'].nunique(),\n            'periodo_inicio': df['data'].min(),\n            'periodo_fim': df['data'].max(),\n            'velocidade_media': df['velocidade_km'].mean(),\n            'velocidade_maxima': df['velocidade_km'].max(),\n            'total_km_periodo': df['odometro_periodo_km'].sum(),\n            'registros_com_gps': df['gps'].sum(),\n            'registros_sem_gps': (~df['gps']).sum(),\n            'veiculos_bloqueados': df['bloqueado'].sum()\n        }\n        \n        return summary\n","size_bytes":11356},"utils/data_analyzer.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport sys\nsys.path.append('.')\nfrom database.db_manager import DatabaseManager\n\nclass DataAnalyzer:\n    \"\"\"Classe para análise de dados de frota\"\"\"\n    \n    def __init__(self, df):\n        \"\"\"Inicializa o analisador com DataFrame\"\"\"\n        self.df = df\n        self.filtered_df = df.copy()\n    \n    @classmethod\n    def from_database(cls, cliente=None, placa=None, data_inicio=None, data_fim=None):\n        \"\"\"Cria uma instância do analisador usando dados da base de dados\"\"\"\n        try:\n            # Buscar dados da base de dados com filtros\n            df = DatabaseManager.get_dashboard_data(\n                client_filter=cliente if cliente != \"Todos\" else None,\n                vehicle_filter=placa if placa != \"Todos\" else None,\n                start_date=data_inicio,\n                end_date=data_fim\n            )\n            \n            if not df.empty:\n                print(f\"✅ DataAnalyzer: {len(df):,} registros carregados da base PostgreSQL\")\n            \n            return cls(df)\n        except Exception as e:\n            print(f\"❌ Erro ao carregar dados: {str(e)}\")\n            # Em caso de erro, retornar analisador com DataFrame vazio\n            empty_df = pd.DataFrame(columns=[\n                'cliente', 'placa', 'data', 'velocidade_km', 'odometro_periodo_km',\n                'gps', 'bloqueado', 'engine_hours_period'\n            ])\n            return cls(empty_df)\n    \n    def apply_filters(self, cliente=None, placa=None, data_inicio=None, data_fim=None):\n        \"\"\"Aplica filtros aos dados com tratamento robusto para 'TODOS'\"\"\"\n        try:\n            filtered = self.df.copy()\n            \n            # Filtro por cliente - garantir que \"Todos\" não cause problemas\n            if cliente and cliente not in [\"Todos\", \"TODOS\", None]:\n                filtered = filtered[filtered['cliente'] == cliente]\n            \n            # Filtro por placa - garantir que \"Todos\" não cause problemas  \n            if placa and placa not in [\"Todos\", \"TODOS\", None]:\n                filtered = filtered[filtered['placa'] == placa]\n        \n        except Exception as e:\n            # Em caso de erro, retornar dados originais sem filtros\n            print(f\"Erro ao aplicar filtros: {str(e)}\")\n            filtered = self.df.copy()\n        \n        # Verificar se há dados e se a coluna 'data' existe e é datetime\n        if not filtered.empty and 'data' in filtered.columns:\n            try:\n                # Garantir que a coluna 'data' é datetime\n                if not pd.api.types.is_datetime64_any_dtype(filtered['data']):\n                    filtered['data'] = pd.to_datetime(filtered['data'], errors='coerce')\n                \n                if data_inicio:\n                    # Converter data_inicio para o mesmo timezone dos dados\n                    try:\n                        if hasattr(filtered['data'].dtype, 'tz') and filtered['data'].dt.tz is not None:\n                            # Dados têm timezone, converter data_inicio\n                            data_inicio_tz = pd.Timestamp(data_inicio).tz_localize('UTC')\n                        else:\n                            # Dados sem timezone, usar timestamp simples\n                            data_inicio_tz = pd.Timestamp(data_inicio)\n                        filtered = filtered[filtered['data'] >= data_inicio_tz]\n                    except Exception:\n                        # Em caso de erro, usar comparação simples\n                        data_inicio_tz = pd.Timestamp(data_inicio)\n                        filtered = filtered[filtered['data'] >= data_inicio_tz]\n                \n                if data_fim:\n                    # Converter data_fim para o mesmo timezone dos dados\n                    try:\n                        if hasattr(filtered['data'].dtype, 'tz') and filtered['data'].dt.tz is not None:\n                            # Dados têm timezone, converter data_fim\n                            data_fim_tz = pd.Timestamp(data_fim).tz_localize('UTC')\n                        else:\n                            # Dados sem timezone, usar timestamp simples\n                            data_fim_tz = pd.Timestamp(data_fim)\n                        \n                        # CORREÇÃO: Se mesmo dia, incluir todo o dia até 23:59:59\n                        if data_inicio and pd.Timestamp(data_inicio).date() == pd.Timestamp(data_fim).date():\n                            data_fim_tz = data_fim_tz.replace(hour=23, minute=59, second=59, microsecond=999999)\n                        \n                        filtered = filtered[filtered['data'] <= data_fim_tz]\n                    except Exception:\n                        # Em caso de erro, usar comparação simples\n                        data_fim_tz = pd.Timestamp(data_fim)\n                        \n                        # CORREÇÃO: Se mesmo dia, incluir todo o dia até 23:59:59\n                        if data_inicio and pd.Timestamp(data_inicio).date() == pd.Timestamp(data_fim).date():\n                            data_fim_tz = data_fim_tz.replace(hour=23, minute=59, second=59, microsecond=999999)\n                        \n                        filtered = filtered[filtered['data'] <= data_fim_tz]\n                        \n            except Exception:\n                # Se houver erro com datetime, não aplicar filtros de data\n                pass\n        \n        self.filtered_df = filtered\n        return filtered\n    \n    def _calculate_total_hours(self, time_series):\n        \"\"\"Converte série de tempo (HH:MM:SS) para total de horas\"\"\"\n        total_hours = 0.0\n        for time_str in time_series:\n            if pd.isna(time_str) or time_str == '' or time_str == '0':\n                continue\n            try:\n                # Parse formato HH:MM:SS\n                parts = str(time_str).split(':')\n                if len(parts) >= 3:\n                    hours = int(parts[0])\n                    minutes = int(parts[1])\n                    seconds = int(parts[2])\n                    total_hours += hours + (minutes / 60.0) + (seconds / 3600.0)\n                elif len(parts) == 2:  # HH:MM\n                    hours = int(parts[0])\n                    minutes = int(parts[1])\n                    total_hours += hours + (minutes / 60.0)\n                elif len(parts) == 1:  # Apenas horas\n                    total_hours += float(parts[0])\n            except (ValueError, IndexError):\n                # Ignorar valores inválidos\n                continue\n        return float(total_hours)\n    \n    def get_kpis(self):\n        \"\"\"Calcula KPIs principais\"\"\"\n        if self.filtered_df.empty:\n            return {}\n        \n        df = self.filtered_df\n        \n        kpis = {\n            'total_veiculos': int(df['placa'].nunique()),\n            'total_registros': int(len(df)),\n            'velocidade_media': float(df['velocidade_km'].mean()),\n            'velocidade_maxima': float(df['velocidade_km'].max()),\n            'distancia_total': float(df['odometro_periodo_km'].sum()),\n            'tempo_ativo_horas': self._calculate_total_hours(df['engine_hours_period']) if 'engine_hours_period' in df.columns else 0.0,\n            'cobertura_gps': float((df['gps'].sum() / len(df)) * 100),\n            'veiculos_bloqueados': int(df['bloqueado'].sum()),\n            'periodo_dias': int((df['data'].max() - df['data'].min()).days + 1) if len(df) > 0 else 0\n        }\n        \n        return kpis\n    \n    def get_speed_analysis(self):\n        \"\"\"Análise de velocidade\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Definir faixas de velocidade\n        conditions = [\n            (df['velocidade_km'] == 0),\n            (df['velocidade_km'] > 0) & (df['velocidade_km'] <= 40),\n            (df['velocidade_km'] > 40) & (df['velocidade_km'] <= 60),\n            (df['velocidade_km'] > 60) & (df['velocidade_km'] <= 80),\n            (df['velocidade_km'] > 80)\n        ]\n        choices = ['Parado', 'Baixa (1-40)', 'Moderada (41-60)', 'Alta (61-80)', 'Muito Alta (80+)']\n        \n        df['faixa_velocidade'] = np.select(conditions, choices, default='Indefinido')\n        \n        speed_dist = df['faixa_velocidade'].value_counts()\n        \n        return {\n            'distribuicao': speed_dist,\n            'velocidade_media_por_veiculo': df.groupby('placa')['velocidade_km'].mean().sort_values(ascending=False),\n            'velocidade_maxima_por_veiculo': df.groupby('placa')['velocidade_km'].max().sort_values(ascending=False),\n            'velocidade_por_hora': df.groupby(df['data'].dt.hour)['velocidade_km'].mean()\n        }\n    \n    def get_operational_analysis(self):\n        \"\"\"Análise operacional\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Análise por veículo\n        agg_dict = {\n            'velocidade_km': ['mean', 'max', 'count'],\n            'odometro_periodo_km': 'sum',\n            'gps': 'mean',\n            'bloqueado': 'any'\n        }\n        \n        # Adicionar engine_hours_period apenas se existir na coluna\n        if 'engine_hours_period' in df.columns:\n            agg_dict['engine_hours_period'] = 'sum'\n        \n        vehicle_stats = df.groupby('placa').agg(agg_dict).round(2)\n        \n        # Achatamento do MultiIndex\n        vehicle_stats.columns = ['_'.join(col) for col in vehicle_stats.columns]\n        vehicle_stats = vehicle_stats.reset_index()\n        \n        # Análise temporal\n        daily_stats = df.groupby(df['data'].dt.date).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean',\n            'odometro_periodo_km': 'sum'\n        }).reset_index()\n        \n        # Análise de utilização por hora\n        hourly_usage = df.groupby(df['data'].dt.hour).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean'\n        }).reset_index()\n        \n        return {\n            'estatisticas_por_veiculo': vehicle_stats,\n            'estatisticas_diarias': daily_stats,\n            'utilizacao_por_hora': hourly_usage,\n            'total_km_por_veiculo': df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False)\n        }\n    \n    def get_compliance_analysis(self):\n        \"\"\"Análise de compliance/conformidade\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Definir limites de compliance\n        SPEED_LIMIT = 80  # km/h\n        MIN_GPS_COVERAGE = 95  # %\n        \n        # Análise de excesso de velocidade\n        speed_violations = df[df['velocidade_km'] > SPEED_LIMIT]\n        \n        # Análise de cobertura GPS\n        gps_coverage_by_vehicle = df.groupby('placa')['gps'].mean() * 100\n        low_gps_vehicles = gps_coverage_by_vehicle[gps_coverage_by_vehicle < MIN_GPS_COVERAGE]\n        \n        # Veículos com problemas\n        blocked_vehicles = df[df['bloqueado'] == True]['placa'].unique()\n        \n        # Score de compliance por veículo\n        compliance_scores = {}\n        for placa in df['placa'].unique():\n            vehicle_data = df[df['placa'] == placa]\n            \n            # Pontuação baseada em critérios\n            speed_score = 100 - (len(vehicle_data[vehicle_data['velocidade_km'] > SPEED_LIMIT]) / len(vehicle_data)) * 100\n            gps_score = (vehicle_data['gps'].mean() * 100)\n            block_score = 100 if not vehicle_data['bloqueado'].any() else 0\n            \n            overall_score = (speed_score * 0.4 + gps_score * 0.4 + block_score * 0.2)\n            compliance_scores[placa] = round(overall_score, 2)\n        \n        return {\n            'violacoes_velocidade': len(speed_violations),\n            'veiculos_baixo_gps': len(low_gps_vehicles),\n            'veiculos_bloqueados': len(blocked_vehicles),\n            'score_compliance': compliance_scores,\n            'detalhes_violacoes': speed_violations.groupby('placa').size().sort_values(ascending=False),\n            'cobertura_gps_por_veiculo': gps_coverage_by_vehicle.sort_values(ascending=True)\n        }\n    \n    def compare_vehicles(self, placas_list):\n        \"\"\"Compara múltiplos veículos\"\"\"\n        if not placas_list or len(placas_list) < 2:\n            return {}\n        \n        comparison_data = {}\n        \n        for placa in placas_list:\n            vehicle_data = self.filtered_df[self.filtered_df['placa'] == placa]\n            \n            if not vehicle_data.empty:\n                comparison_data[placa] = {\n                    'total_registros': len(vehicle_data),\n                    'velocidade_media': vehicle_data['velocidade_km'].mean(),\n                    'velocidade_maxima': vehicle_data['velocidade_km'].max(),\n                    'distancia_total': vehicle_data['odometro_periodo_km'].sum(),\n                    'tempo_ativo': vehicle_data['engine_hours_period'].sum() if 'engine_hours_period' in vehicle_data.columns else 0,\n                    'cobertura_gps': (vehicle_data['gps'].mean() * 100),\n                    'violacoes_velocidade': len(vehicle_data[vehicle_data['velocidade_km'] > 80]),\n                    'bloqueios': vehicle_data['bloqueado'].sum()\n                }\n        \n        return comparison_data\n    \n    def get_temporal_patterns(self):\n        \"\"\"Análise de padrões temporais\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Padrões por dia da semana\n        df['dia_semana'] = df['data'].dt.day_name()\n        weekly_patterns = df.groupby('dia_semana').agg({\n            'velocidade_km': 'mean',\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        })\n        \n        # Padrões por hora do dia\n        hourly_patterns = df.groupby(df['data'].dt.hour).agg({\n            'velocidade_km': 'mean',\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        })\n        \n        # Padrões mensais - usando year+month para evitar warning de timezone\n        monthly_patterns = df.groupby([df['data'].dt.year, df['data'].dt.month]).agg({\n            'velocidade_km': 'mean',\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        })\n        \n        return {\n            'padroes_semanais': weekly_patterns,\n            'padroes_por_hora': hourly_patterns,\n            'padroes_mensais': monthly_patterns\n        }\n    \n    def get_efficiency_metrics(self):\n        \"\"\"Métricas de eficiência\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Eficiência por veículo\n        vehicle_efficiency = df.groupby('placa').apply(\n            lambda x: {\n                'km_por_dia': x['odometro_periodo_km'].sum() / max(1, (x['data'].max() - x['data'].min()).days + 1),\n                'utilizacao_diaria': len(x) / max(1, (x['data'].max() - x['data'].min()).days + 1),\n                'velocidade_media': x['velocidade_km'].mean(),\n                'tempo_parado_pct': (len(x[x['velocidade_km'] == 0]) / len(x)) * 100\n            }\n        )\n        \n        return {\n            'eficiencia_por_veiculo': vehicle_efficiency,\n            'top_veiculos_km': df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False).head(10),\n            'veiculos_mais_utilizados': df.groupby('placa').size().sort_values(ascending=False).head(10)\n        }\n","size_bytes":15500},"utils/insights_generator.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport streamlit as st\n\nclass InsightsGenerator:\n    \"\"\"Gerador de insights automáticos para dados de frota\"\"\"\n    \n    def __init__(self, analyzer):\n        \"\"\"Inicializa com um analisador de dados\"\"\"\n        self.analyzer = analyzer\n        self.insights = []\n    \n    def generate_all_insights(self):\n        \"\"\"Gera todos os tipos de insights\"\"\"\n        self.insights = []\n        \n        # Gerar diferentes tipos de insights\n        self.generate_performance_insights()\n        self.generate_compliance_insights()\n        self.generate_efficiency_insights()\n        self.generate_operational_insights()\n        self.generate_predictive_insights()\n        \n        return self.insights\n    \n    def generate_performance_insights(self):\n        \"\"\"Gera insights de performance\"\"\"\n        kpis = self.analyzer.get_kpis()\n        speed_analysis = self.analyzer.get_speed_analysis()\n        \n        if not kpis:\n            return\n        \n        # Insight sobre velocidade média\n        if kpis['velocidade_media'] > 60:\n            self.add_insight(\n                \"⚠️ Velocidade Elevada\",\n                f\"A velocidade média da frota é de {kpis['velocidade_media']:.1f} km/h, acima do recomendado.\",\n                \"Considere implementar políticas de condução mais rigorosas.\",\n                \"warning\"\n            )\n        elif kpis['velocidade_media'] < 25:\n            self.add_insight(\n                \"🐌 Velocidade Baixa\",\n                f\"A velocidade média da frota é de {kpis['velocidade_media']:.1f} km/h, possivelmente indicando trânsito urbano intenso.\",\n                \"Avalie otimização de rotas em horários de menor movimento.\",\n                \"info\"\n            )\n        \n        # Insight sobre cobertura GPS\n        if kpis['cobertura_gps'] < 90:\n            self.add_insight(\n                \"📡 Problemas de GPS\",\n                f\"Cobertura GPS de apenas {kpis['cobertura_gps']:.1f}%, abaixo do ideal (>95%).\",\n                \"Verifique equipamentos GPS e áreas de cobertura de sinal.\",\n                \"error\"\n            )\n        else:\n            self.add_insight(\n                \"✅ Excelente Cobertura GPS\",\n                f\"Cobertura GPS de {kpis['cobertura_gps']:.1f}%, dentro do padrão de qualidade.\",\n                \"Sistema de rastreamento funcionando adequadamente.\",\n                \"success\"\n            )\n    \n    def generate_compliance_insights(self):\n        \"\"\"Gera insights de compliance\"\"\"\n        compliance = self.analyzer.get_compliance_analysis()\n        \n        if not compliance:\n            return\n        \n        # Insights sobre violações de velocidade\n        if compliance['violacoes_velocidade'] > 0:\n            self.add_insight(\n                \"🚨 Violações de Velocidade\",\n                f\"{compliance['violacoes_velocidade']} registros de excesso de velocidade detectados.\",\n                \"Implemente treinamento de condutores e monitoramento rigoroso.\",\n                \"error\"\n            )\n        \n        # Insights sobre veículos bloqueados\n        if compliance['veiculos_bloqueados'] > 0:\n            self.add_insight(\n                \"🔒 Veículos Bloqueados\",\n                f\"{compliance['veiculos_bloqueados']} veículos com status de bloqueio ativo.\",\n                \"Verifique motivos do bloqueio e normalize situação se necessário.\",\n                \"warning\"\n            )\n        \n        # Score de compliance geral\n        if compliance['score_compliance']:\n            avg_score = np.mean(list(compliance['score_compliance'].values()))\n            if avg_score >= 90:\n                self.add_insight(\n                    \"🏆 Excelente Compliance\",\n                    f\"Score médio de compliance de {avg_score:.1f}%, indicando ótima conformidade.\",\n                    \"Mantenha os padrões de operação atuais.\",\n                    \"success\"\n                )\n            elif avg_score < 70:\n                self.add_insight(\n                    \"⚠️ Compliance Baixo\",\n                    f\"Score médio de compliance de {avg_score:.1f}%, requer atenção.\",\n                    \"Implemente ações corretivas urgentes para melhorar conformidade.\",\n                    \"error\"\n                )\n    \n    def generate_efficiency_insights(self):\n        \"\"\"Gera insights de eficiência\"\"\"\n        kpis = self.analyzer.get_kpis()\n        efficiency = self.analyzer.get_efficiency_metrics()\n        \n        if not kpis or not efficiency:\n            return\n        \n        # Insight sobre utilização da frota\n        if kpis['periodo_dias'] > 0:\n            utilizacao_media = kpis['total_registros'] / (kpis['total_veiculos'] * kpis['periodo_dias'])\n            \n            if utilizacao_media < 10:\n                self.add_insight(\n                    \"📉 Baixa Utilização\",\n                    f\"Utilização média de {utilizacao_media:.1f} registros por veículo/dia.\",\n                    \"Considere otimizar a distribuição e uso dos veículos.\",\n                    \"warning\"\n                )\n            elif utilizacao_media > 50:\n                self.add_insight(\n                    \"📈 Alta Utilização\",\n                    f\"Utilização média de {utilizacao_media:.1f} registros por veículo/dia.\",\n                    \"Frota sendo bem utilizada, monitore sobrecarga.\",\n                    \"success\"\n                )\n        \n        # Insight sobre distância percorrida\n        if kpis['distancia_total'] > 0 and kpis['tempo_ativo_horas'] > 0:\n            km_por_hora = kpis['distancia_total'] / kpis['tempo_ativo_horas']\n            if km_por_hora < 15:\n                self.add_insight(\n                    \"🐢 Baixa Produtividade\",\n                    f\"Média de {km_por_hora:.1f} km/hora de operação, indicando possível ineficiência.\",\n                    \"Analise rotas e otimize deslocamentos.\",\n                    \"warning\"\n                )\n    \n    def generate_operational_insights(self):\n        \"\"\"Gera insights operacionais\"\"\"\n        operational = self.analyzer.get_operational_analysis()\n        patterns = self.analyzer.get_temporal_patterns()\n        \n        if not operational or not patterns:\n            return\n        \n        # Insight sobre padrões de uso por hora\n        if 'padroes_por_hora' in patterns and patterns['padroes_por_hora'] is not None:\n            hourly_data = patterns['padroes_por_hora']\n            \n            # Se hourly_data é um DataFrame com dados válidos\n            if isinstance(hourly_data, pd.DataFrame) and not hourly_data.empty:\n                # Verificar se tem índice ou coluna de hora\n                if hourly_data.index.name == 'hora' or 'hora' in hourly_data.columns:\n                    # Usar a primeira coluna numérica disponível como proxy para atividade\n                    numeric_cols = hourly_data.select_dtypes(include=[np.number]).columns\n                    if len(numeric_cols) > 0:\n                        activity_col = numeric_cols[0]\n                        peak_hour = hourly_data[activity_col].idxmax()\n                        peak_value = hourly_data.loc[peak_hour, activity_col]\n                        \n                        self.add_insight(\n                            \"⏰ Pico de Utilização\",\n                            f\"Maior atividade registrada às {peak_hour}h.\",\n                            \"Considere balanceamento de carga em outros horários.\",\n                            \"info\"\n                        )\n                        return\n        \n        # Insight sobre veículos inativos\n        if 'estatisticas_por_veiculo' in operational:\n            vehicle_stats = operational['estatisticas_por_veiculo']\n            if len(vehicle_stats) > 0:\n                inactive_threshold = vehicle_stats['velocidade_km_count'].quantile(0.25)\n                inactive_vehicles = len(vehicle_stats[vehicle_stats['velocidade_km_count'] < inactive_threshold])\n                \n                if inactive_vehicles > 0:\n                    self.add_insight(\n                        \"😴 Veículos Pouco Ativos\",\n                        f\"{inactive_vehicles} veículos com baixa atividade registrada.\",\n                        \"Verifique se estes veículos estão sendo subutilizados.\",\n                        \"info\"\n                    )\n    \n    def generate_predictive_insights(self):\n        \"\"\"Gera insights preditivos\"\"\"\n        df = self.analyzer.filtered_df\n        \n        if df.empty:\n            return\n        \n        # Análise de tendências - usar período dinâmico baseado nos dados filtrados\n        if len(df) > 7:  # Pelo menos uma semana de dados\n            # Calcular período de análise baseado no range total dos dados filtrados\n            date_range = (df['data'].max() - df['data'].min()).days\n            \n            # Usar 25% do período total para análise recente, mínimo 1 dia, máximo 30 dias\n            analysis_days = max(1, min(30, round(date_range * 0.25)))\n            \n            recent_data = df[df['data'] >= df['data'].max() - timedelta(days=analysis_days)]\n            older_data = df[df['data'] < df['data'].max() - timedelta(days=analysis_days)]\n            \n            if not recent_data.empty and not older_data.empty:\n                recent_avg_speed = recent_data['velocidade_km'].mean()\n                older_avg_speed = older_data['velocidade_km'].mean()\n                \n                # Evitar divisão por zero e valores inválidos\n                if pd.notna(older_avg_speed) and pd.notna(recent_avg_speed) and older_avg_speed > 0:\n                    speed_change = ((recent_avg_speed - older_avg_speed) / older_avg_speed) * 100\n                    \n                    if abs(speed_change) > 10:\n                        trend = \"aumento\" if speed_change > 0 else \"redução\"\n                        period_text = f\"últimos {analysis_days} dia{'s' if analysis_days > 1 else ''}\"\n                        self.add_insight(\n                            f\"📈 Tendência de Velocidade\",\n                            f\"Detectado {trend} de {abs(speed_change):.1f}% na velocidade média nos {period_text}.\",\n                            f\"Monitore esta tendência para identificar padrões sazonais ou operacionais.\",\n                            \"info\"\n                        )\n        \n        # Predição de manutenção baseada em uso\n        agg_map = {'odometro_periodo_km': 'sum'}\n        if 'engine_hours_period' in df.columns:\n            agg_map['engine_hours_period'] = 'sum'\n        \n        vehicle_usage = df.groupby('placa').agg(agg_map)\n        \n        for placa, data in vehicle_usage.iterrows():\n            total_km = data['odometro_periodo_km']\n            if total_km > 1000:  # Veículos com alta quilometragem\n                self.add_insight(\n                    \"🔧 Manutenção Preventiva\",\n                    f\"Veículo {placa} percorreu {total_km:.0f}km no período analisado.\",\n                    \"Agende revisão preventiva para manter performance e segurança.\",\n                    \"info\"\n                )\n    \n    def add_insight(self, title, description, recommendation, type=\"info\"):\n        \"\"\"Adiciona um insight à lista\"\"\"\n        insight = {\n            'title': title,\n            'description': description,\n            'recommendation': recommendation,\n            'type': type,\n            'timestamp': datetime.now(),\n            'priority': self.get_priority_by_type(type)\n        }\n        self.insights.append(insight)\n    \n    def get_priority_by_type(self, type):\n        \"\"\"Retorna prioridade baseada no tipo\"\"\"\n        priority_map = {\n            'error': 1,\n            'warning': 2,\n            'info': 3,\n            'success': 4\n        }\n        return priority_map.get(type, 3)\n    \n    def get_insights_by_priority(self):\n        \"\"\"Retorna insights ordenados por prioridade\"\"\"\n        return sorted(self.insights, key=lambda x: x['priority'])\n    \n    def get_insights_by_type(self, type):\n        \"\"\"Retorna insights de um tipo específico\"\"\"\n        return [insight for insight in self.insights if insight['type'] == type]\n    \n    def export_insights_to_text(self):\n        \"\"\"Exporta insights para texto\"\"\"\n        output = f\"# Relatório de Insights - {datetime.now().strftime('%d/%m/%Y %H:%M')}\\n\\n\"\n        \n        for insight in self.get_insights_by_priority():\n            output += f\"## {insight['title']}\\n\"\n            output += f\"**Descrição:** {insight['description']}\\n\"\n            output += f\"**Recomendação:** {insight['recommendation']}\\n\"\n            output += f\"**Tipo:** {insight['type'].upper()}\\n\\n\"\n        \n        return output\n","size_bytes":12764},"utils/ml_predictive.py":{"content":"\"\"\"\nAnálise de Manutenção Preditiva usando Machine Learning\nDetecta anomalias e prevê necessidades de manutenção\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Tuple\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass PredictiveMaintenanceAnalyzer:\n    \"\"\"Análise de manutenção preditiva para frota\"\"\"\n    \n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.anomaly_detector = IsolationForest(\n            contamination=0.1,\n            random_state=42,\n            n_estimators=100\n        )\n        self.clusterer = DBSCAN(eps=0.5, min_samples=5)\n        \n    def analyze_vehicle_health(self, df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Análise completa de saúde do veículo\"\"\"\n        if df.empty:\n            return {'status': 'error', 'message': 'Sem dados disponíveis'}\n            \n        # Preparar features para ML\n        features = self._prepare_features(df)\n        if features.empty:\n            return {'status': 'error', 'message': 'Dados insuficientes para análise'}\n            \n        # Detectar anomalias\n        anomalies = self._detect_anomalies(features)\n        \n        # Análise de padrões\n        patterns = self._analyze_patterns(df)\n        \n        # Previsões de manutenção\n        maintenance_alerts = self._predict_maintenance_needs(df, anomalies)\n        \n        # Scores de saúde\n        health_scores = self._calculate_health_scores(df, anomalies)\n        \n        return {\n            'status': 'success',\n            'health_scores': health_scores,\n            'anomalies': anomalies,\n            'patterns': patterns,\n            'maintenance_alerts': maintenance_alerts,\n            'recommendations': self._generate_recommendations(health_scores, maintenance_alerts)\n        }\n    \n    def _prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Preparar features para análise ML\"\"\"\n        try:\n            features_data = []\n            \n            # Converter campos numéricos primeiro\n            numeric_columns = ['velocidade_km', 'battery_level', 'tensao', 'odometro_periodo_km', 'engine_hours_period']\n            for col in numeric_columns:\n                if col in df.columns:\n                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n            \n            # Agrupar por hora para análise temporal\n            df_hourly = df.groupby(df['data'].dt.floor('H')).agg({\n                'velocidade_km': ['mean', 'max', 'std'],\n                'battery_level': 'mean',\n                'tensao': 'mean',\n                'odometro_periodo_km': 'sum',\n                'engine_hours_period': 'sum',\n                'ignicao': lambda x: (x == 'Ligada').sum() / len(x) if len(x) > 0 else 0\n            }).reset_index()\n            \n            # Flatten column names\n            df_hourly.columns = ['timestamp', 'vel_media', 'vel_max', 'vel_std', \n                                'bateria_media', 'tensao_media', 'km_periodo', \n                                'engine_hours_periodo', 'ignicao_ratio']\n            \n            # Adicionar features temporais\n            df_hourly['hora'] = df_hourly['timestamp'].dt.hour\n            df_hourly['dia_semana'] = df_hourly['timestamp'].dt.dayofweek\n            \n            # Preencher NaNs\n            numeric_cols = ['vel_media', 'vel_max', 'vel_std', 'bateria_media', \n                           'tensao_media', 'km_periodo', 'engine_hours_periodo', 'ignicao_ratio']\n            for col in numeric_cols:\n                if col in df_hourly.columns:\n                    df_hourly[col] = pd.to_numeric(df_hourly[col], errors='coerce').fillna(0)\n            \n            return df_hourly[numeric_cols + ['hora', 'dia_semana']].dropna()\n            \n        except Exception as e:\n            print(f\"Erro ao preparar features: {e}\")\n            return pd.DataFrame()\n    \n    def _detect_anomalies(self, features: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Detectar anomalias usando Isolation Forest\"\"\"\n        try:\n            if len(features) < 10:\n                return {'count': 0, 'indices': [], 'scores': []}\n            \n            # Normalizar dados\n            features_scaled = self.scaler.fit_transform(features)\n            \n            # Detectar anomalias\n            outliers = self.anomaly_detector.fit_predict(features_scaled)\n            anomaly_scores = self.anomaly_detector.score_samples(features_scaled)\n            \n            anomaly_indices = np.where(outliers == -1)[0]\n            \n            return {\n                'count': len(anomaly_indices),\n                'indices': anomaly_indices.tolist(),\n                'scores': anomaly_scores.tolist(),\n                'severity': self._classify_anomaly_severity(anomaly_scores[anomaly_indices])\n            }\n            \n        except Exception as e:\n            print(f\"Erro na detecção de anomalias: {e}\")\n            return {'count': 0, 'indices': [], 'scores': []}\n    \n    def _analyze_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Análise de padrões de uso\"\"\"\n        try:\n            patterns = {}\n            \n            # Padrões de velocidade\n            vel_stats = df['velocidade_km'].describe()\n            patterns['velocidade'] = {\n                'media': round(vel_stats['mean'], 1),\n                'maxima': round(vel_stats['max'], 1),\n                'excessos': len(df[df['velocidade_km'] > 80]),  # Velocidade excessiva\n                'paradas': len(df[df['velocidade_km'] == 0])\n            }\n            \n            # Padrões temporais\n            df['hora'] = df['data'].dt.hour\n            uso_por_hora = df.groupby('hora').size()\n            patterns['uso_temporal'] = {\n                'horario_pico': uso_por_hora.idxmax(),\n                'horario_baixo': uso_por_hora.idxmin(),\n                'uso_noturno': len(df[(df['hora'] >= 22) | (df['hora'] <= 6)])\n            }\n            \n            # Padrões de manutenção\n            bateria_baixa = df[pd.to_numeric(df['bateria'], errors='coerce') < 12].shape[0] if 'bateria' in df.columns else 0\n            patterns['manutencao'] = {\n                'bateria_baixa_count': bateria_baixa,\n                'tensao_problemas': len(df[pd.to_numeric(df['tensao'], errors='coerce') < 11]) if 'tensao' in df.columns else 0\n            }\n            \n            return patterns\n            \n        except Exception as e:\n            print(f\"Erro na análise de padrões: {e}\")\n            return {}\n    \n    def _predict_maintenance_needs(self, df: pd.DataFrame, anomalies: Dict) -> List[Dict]:\n        \"\"\"Prever necessidades de manutenção\"\"\"\n        alerts = []\n        \n        try:\n            # Análise de bateria\n            if 'bateria' in df.columns:\n                bateria_values = pd.to_numeric(df['bateria'], errors='coerce').dropna()\n                if not bateria_values.empty:\n                    bateria_media = bateria_values.mean()\n                    if bateria_media < 12:\n                        alerts.append({\n                            'tipo': 'Bateria',\n                            'severidade': 'Alta' if bateria_media < 11 else 'Média',\n                            'descricao': f'Bateria baixa (média: {bateria_media:.1f}V)',\n                            'prazo': '3-7 dias' if bateria_media < 11 else '1-2 semanas'\n                        })\n            \n            # Análise de velocidade excessiva\n            vel_excessiva = len(df[df['velocidade_km'] > 80])\n            if vel_excessiva > len(df) * 0.1:  # Mais de 10% do tempo em velocidade alta\n                alerts.append({\n                    'tipo': 'Desgaste',\n                    'severidade': 'Média',\n                    'descricao': f'Uso intensivo detectado ({vel_excessiva} ocorrências)',\n                    'prazo': '2-4 semanas'\n                })\n            \n            # Anomalias frequentes\n            if anomalies.get('count', 0) > len(df) * 0.05:  # Mais de 5% anomalias\n                alerts.append({\n                    'tipo': 'Comportamento Anômalo',\n                    'severidade': 'Média',\n                    'descricao': f'{anomalies[\"count\"]} anomalias detectadas',\n                    'prazo': '1-3 semanas'\n                })\n                \n        except Exception as e:\n            print(f\"Erro na previsão de manutenção: {e}\")\n        \n        return alerts\n    \n    def _calculate_health_scores(self, df: pd.DataFrame, anomalies: Dict) -> Dict[str, float]:\n        \"\"\"Calcular scores de saúde do veículo\"\"\"\n        scores = {}\n        \n        try:\n            # Score da bateria (0-100)\n            if 'bateria' in df.columns:\n                bateria_values = pd.to_numeric(df['bateria'], errors='coerce').dropna()\n                if not bateria_values.empty:\n                    bateria_media = bateria_values.mean()\n                    scores['bateria'] = max(0, min(100, (bateria_media - 10) * 10))  # 10V = 0%, 20V = 100%\n                else:\n                    scores['bateria'] = 50\n            else:\n                scores['bateria'] = 50\n            \n            # Score do comportamento (baseado em anomalias)\n            total_registros = len(df)\n            anomalia_ratio = anomalies.get('count', 0) / total_registros if total_registros > 0 else 0\n            scores['comportamento'] = max(0, 100 - (anomalia_ratio * 500))  # Penaliza anomalias\n            \n            # Score de velocidade (uso responsável)\n            vel_excessiva_ratio = len(df[df['velocidade_km'] > 80]) / total_registros if total_registros > 0 else 0\n            scores['velocidade'] = max(0, 100 - (vel_excessiva_ratio * 200))\n            \n            # Score geral (média ponderada)\n            scores['geral'] = (\n                scores['bateria'] * 0.4 + \n                scores['comportamento'] * 0.35 + \n                scores['velocidade'] * 0.25\n            )\n            \n        except Exception as e:\n            print(f\"Erro no cálculo de scores: {e}\")\n            scores = {'bateria': 50, 'comportamento': 50, 'velocidade': 50, 'geral': 50}\n        \n        return {k: round(v, 1) for k, v in scores.items()}\n    \n    def _classify_anomaly_severity(self, anomaly_scores: np.ndarray) -> str:\n        \"\"\"Classificar severidade das anomalias\"\"\"\n        if len(anomaly_scores) == 0:\n            return 'Baixa'\n        \n        mean_score = np.mean(anomaly_scores)\n        if mean_score < -0.3:\n            return 'Alta'\n        elif mean_score < -0.1:\n            return 'Média'\n        else:\n            return 'Baixa'\n    \n    def _generate_recommendations(self, health_scores: Dict, alerts: List[Dict]) -> List[str]:\n        \"\"\"Gerar recomendações baseadas na análise\"\"\"\n        recommendations = []\n        \n        try:\n            # Recomendações baseadas nos scores\n            if health_scores.get('bateria', 50) < 70:\n                recommendations.append(\"🔋 Verificar sistema elétrico e bateria\")\n            \n            if health_scores.get('comportamento', 50) < 70:\n                recommendations.append(\"⚠️ Revisar padrões de condução e treinamento\")\n            \n            if health_scores.get('velocidade', 50) < 70:\n                recommendations.append(\"🚗 Implementar controle de velocidade\")\n            \n            # Recomendações baseadas nos alertas\n            if any(alert['severidade'] == 'Alta' for alert in alerts):\n                recommendations.append(\"🚨 Manutenção urgente necessária\")\n            \n            if len(alerts) > 2:\n                recommendations.append(\"📋 Revisar plano de manutenção preventiva\")\n            \n            if not recommendations:\n                recommendations.append(\"✅ Veículo em bom estado - manter rotina\")\n                \n        except Exception as e:\n            print(f\"Erro ao gerar recomendações: {e}\")\n            recommendations = [\"📋 Manter rotina de manutenção\"]\n        \n        return recommendations","size_bytes":12153},"utils/monthly_data_manager.py":{"content":"\"\"\"\nGerenciador de dados mensais - preparado para uploads recorrentes\n\"\"\"\n\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\nfrom database.db_manager import DatabaseManager\n\nclass MonthlyDataManager:\n    \"\"\"Gerencia uploads de dados mensais com a mesma estrutura\"\"\"\n    \n    @staticmethod\n    def process_monthly_upload(csv_files: List[str]) -> Dict[str, Any]:\n        \"\"\"Processa upload mensal de múltiplos arquivos CSV\"\"\"\n        results = {\n            'success': True,\n            'processed_files': 0,\n            'total_records': 0,\n            'new_vehicles': 0,\n            'errors': []\n        }\n        \n        for csv_file in csv_files:\n            try:\n                if not os.path.exists(csv_file):\n                    results['errors'].append(f\"Arquivo não encontrado: {csv_file}\")\n                    continue\n                \n                # Processar arquivo\n                result = DatabaseManager.migrate_csv_to_database(csv_file)\n                \n                if result['success']:\n                    results['processed_files'] += 1\n                    results['total_records'] += result.get('records_processed', 0)\n                    results['new_vehicles'] += result.get('unique_vehicles', 0)\n                else:\n                    results['errors'].append(f\"{os.path.basename(csv_file)}: {result.get('error', 'Erro desconhecido')}\")\n                    \n            except Exception as e:\n                results['errors'].append(f\"{os.path.basename(csv_file)}: {str(e)}\")\n        \n        # Se houve erros mas alguns sucessos, ainda é parcialmente bem-sucedido\n        if results['errors'] and results['processed_files'] == 0:\n            results['success'] = False\n        \n        return results\n    \n    @staticmethod\n    def get_monthly_summary() -> Dict[str, Any]:\n        \"\"\"Resumo dos dados mensais na base\"\"\"\n        summary = DatabaseManager.get_fleet_summary()\n        \n        # Adicionar informações específicas mensais\n        df = DatabaseManager.get_dashboard_data()\n        \n        if not df.empty:\n            # Análise temporal\n            df['mes'] = df['data'].dt.to_period('M')\n            monthly_data = df.groupby('mes').agg({\n                'placa': 'nunique',\n                'velocidade_km': 'mean',\n                'data': 'count'\n            }).reset_index()\n            \n            summary['monthly_breakdown'] = monthly_data.to_dict('records')\n            summary['data_period'] = {\n                'start': df['data'].min().strftime('%d/%m/%Y'),\n                'end': df['data'].max().strftime('%d/%m/%Y'),\n                'days': (df['data'].max() - df['data'].min()).days\n            }\n        \n        return summary\n    \n    @staticmethod\n    def prepare_for_next_month() -> Dict[str, Any]:\n        \"\"\"Prepara sistema para próximo upload mensal\"\"\"\n        current_summary = MonthlyDataManager.get_monthly_summary()\n        \n        return {\n            'current_data': current_summary,\n            'ready_for_upload': True,\n            'upload_instructions': {\n                'format': 'CSV com separador ; (ponto e vírgula)',\n                'encoding': 'Latin-1 (ISO 8859-1)',\n                'required_columns': [\n                    'Cliente', 'Placa', 'Ativo', 'Data', 'Data (GPRS)',\n                    'Velocidade (Km)', 'Ignição', 'Motorista', 'GPS', 'Gprs',\n                    'Localização', 'Endereço', 'Tipo do Evento', 'Cerca',\n                    'Saida', 'Entrada', 'Pacote', 'Odômetro do período  (Km)',\n                    'Horímetro do período', 'Horímetro embarcado',\n                    'Odômetro embarcado (Km)', 'Bateria', 'Imagem', 'Tensão', 'Bloqueado'\n                ],\n                'date_format': 'DD/MM/AAAA HH:MM:SS (formato brasileiro)',\n                'notes': 'O sistema detecta automaticamente o formato e processa múltiplos arquivos'\n            }\n        }","size_bytes":3940},"utils/pdf_reports.py":{"content":"\"\"\"Gerador de Relatórios PDF Avançado\"\"\"\nfrom fpdf import FPDF\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nimport os\nimport numpy as np\n\nclass PDFReportGenerator:\n    def __init__(self):\n        self.pdf = FPDF()\n        self.pdf.set_auto_page_break(auto=True, margin=15)\n        self.pdf.add_page()\n        self.pdf.set_font('Arial', 'B', 16)\n    \n    def add_header(self, title):\n        \"\"\"Adiciona cabeçalho da seção\"\"\"\n        self.pdf.set_fill_color(230, 230, 230)\n        self.pdf.set_font('Arial', 'B', 14)\n        self.pdf.cell(0, 10, title, 0, 1, 'L', True)\n        self.pdf.ln(5)\n    \n    def add_subsection(self, title):\n        \"\"\"Adiciona subtítulo\"\"\"\n        self.pdf.set_font('Arial', 'B', 12)\n        self.pdf.cell(0, 8, title, 0, 1, 'L')\n        self.pdf.ln(3)\n    \n    def add_metric(self, label, value, unit=\"\"):\n        \"\"\"Adiciona métrica formatada\"\"\"\n        self.pdf.set_font('Arial', '', 10)\n        self.pdf.cell(100, 6, f'{label}:', 0, 0, 'L')\n        self.pdf.set_font('Arial', 'B', 10)\n        self.pdf.cell(0, 6, f'{value} {unit}', 0, 1, 'L')\n    \n    def add_table_header(self, headers):\n        \"\"\"Adiciona cabeçalho de tabela\"\"\"\n        self.pdf.set_font('Arial', 'B', 9)\n        self.pdf.set_fill_color(200, 200, 200)\n        col_width = 190 / len(headers)\n        for header in headers:\n            self.pdf.cell(col_width, 8, str(header), 1, 0, 'C', True)\n        self.pdf.ln()\n    \n    def add_table_row(self, data):\n        \"\"\"Adiciona linha de tabela\"\"\"\n        self.pdf.set_font('Arial', '', 8)\n        col_width = 190 / len(data)\n        for item in data:\n            self.pdf.cell(col_width, 6, str(item), 1, 0, 'C')\n        self.pdf.ln()\n    \n    def generate_comprehensive_report(self, output_path: str = \"relatorio_completo_frota.pdf\") -> str:\n        \"\"\"Gera relatório completo e detalhado da frota\"\"\"\n        try:\n            # Carregar dados\n            df = DatabaseManager.get_dashboard_data()\n            summary = DatabaseManager.get_fleet_summary()\n            \n            if df.empty:\n                self.pdf.cell(0, 10, 'ERRO: Nenhum dado disponível para gerar relatório', 0, 1, 'C')\n                self.pdf.output(output_path)\n                return output_path\n            \n            # Cabeçalho principal\n            self.pdf.set_font('Arial', 'B', 18)\n            self.pdf.cell(0, 15, 'RELATÓRIO EXECUTIVO DE FROTA', 0, 1, 'C')\n            self.pdf.set_font('Arial', 'I', 12)\n            self.pdf.cell(0, 8, 'Insight Hub - Sistema de Monitoramento Municipal', 0, 1, 'C')\n            self.pdf.ln(10)\n            \n            # Informações do relatório\n            self.pdf.set_font('Arial', '', 10)\n            self.pdf.cell(0, 6, f'Data de Geração: {datetime.now().strftime(\"%d/%m/%Y às %H:%M:%S\")}', 0, 1)\n            self.pdf.cell(0, 6, f'Período Analisado: {df[\"data\"].min().strftime(\"%d/%m/%Y\")} a {df[\"data\"].max().strftime(\"%d/%m/%Y\")}', 0, 1)\n            self.pdf.ln(10)\n            \n            # 1. RESUMO EXECUTIVO\n            self.add_header('1. RESUMO EXECUTIVO')\n            \n            total_records = len(df)\n            total_vehicles = df['placa'].nunique()\n            total_clients = df['cliente'].nunique()\n            avg_speed = df['velocidade_km'].mean()\n            max_speed = df['velocidade_km'].max()\n            total_distance = df['odometro_periodo_km'].sum()\n            \n            # Métricas principais\n            self.add_metric('Total de Registros Processados', f'{total_records:,}')\n            self.add_metric('Veículos Monitorados', f'{total_vehicles}')\n            self.add_metric('Clientes Atendidos', f'{total_clients}')\n            self.add_metric('Velocidade Média da Frota', f'{avg_speed:.1f}', 'km/h')\n            self.add_metric('Velocidade Máxima Registrada', f'{max_speed:.1f}', 'km/h')\n            self.add_metric('Distância Total Percorrida', f'{total_distance:.1f}', 'km')\n            \n            # Cobertura GPS\n            gps_quality = df['gps'].value_counts()\n            gps_coverage = (gps_quality.get('Ativo', 0) / total_records * 100) if total_records > 0 else 0\n            self.add_metric('Cobertura GPS Ativa', f'{gps_coverage:.1f}', '%')\n            self.pdf.ln(5)\n            \n            # 2. ANÁLISE POR CLIENTE\n            self.add_header('2. ANÁLISE POR CLIENTE')\n            \n            client_stats = df.groupby('cliente').agg({\n                'placa': 'nunique',\n                'velocidade_km': ['mean', 'max'],\n                'odometro_periodo_km': 'sum'\n            }).round(2)\n            \n            self.add_table_header(['Cliente', 'Veículos', 'Vel. Média', 'Vel. Máx', 'Dist. Total'])\n            for cliente in client_stats.index:\n                data = [\n                    cliente,\n                    client_stats.loc[cliente, ('placa', 'nunique')],\n                    f\"{client_stats.loc[cliente, ('velocidade_km', 'mean')]:.1f}\",\n                    f\"{client_stats.loc[cliente, ('velocidade_km', 'max')]:.1f}\",\n                    f\"{client_stats.loc[cliente, ('odometro_periodo_km', 'sum')]:.1f}\"\n                ]\n                self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 3. ANÁLISE DETALHADA POR VEÍCULO\n            self.add_header('3. ANÁLISE DETALHADA POR VEÍCULO')\n            \n            vehicle_stats = df.groupby(['cliente', 'placa']).agg({\n                'velocidade_km': ['mean', 'max', 'std'],\n                'odometro_periodo_km': 'sum',\n                'data': 'count'\n            }).round(2)\n            \n            self.add_table_header(['Cliente', 'Placa', 'Registros', 'Vel. Média', 'Vel. Máx', 'Distância'])\n            for (cliente, placa) in vehicle_stats.index:\n                data = [\n                    cliente[:15],  # Truncar se muito longo\n                    placa,\n                    vehicle_stats.loc[(cliente, placa), ('data', 'count')],\n                    f\"{vehicle_stats.loc[(cliente, placa), ('velocidade_km', 'mean')]:.1f}\",\n                    f\"{vehicle_stats.loc[(cliente, placa), ('velocidade_km', 'max')]:.1f}\",\n                    f\"{vehicle_stats.loc[(cliente, placa), ('odometro_periodo_km', 'sum')]:.1f}\"\n                ]\n                self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 4. ANÁLISE DE VELOCIDADE E SEGURANÇA\n            self.add_header('4. ANÁLISE DE VELOCIDADE E SEGURANÇA')\n            \n            # Faixas de velocidade\n            speed_ranges = {\n                '0-30 km/h': len(df[df['velocidade_km'] <= 30]),\n                '31-60 km/h': len(df[(df['velocidade_km'] > 30) & (df['velocidade_km'] <= 60)]),\n                '61-80 km/h': len(df[(df['velocidade_km'] > 60) & (df['velocidade_km'] <= 80)]),\n                '81-100 km/h': len(df[(df['velocidade_km'] > 80) & (df['velocidade_km'] <= 100)]),\n                'Acima de 100 km/h': len(df[df['velocidade_km'] > 100])\n            }\n            \n            self.add_subsection('Distribuição de Velocidades:')\n            for faixa, count in speed_ranges.items():\n                percentage = (count / total_records * 100) if total_records > 0 else 0\n                self.add_metric(faixa, f'{count:,} ({percentage:.1f}%)')\n            \n            # Violações de velocidade (acima de 80 km/h)\n            violations = df[df['velocidade_km'] > 80]\n            self.add_subsection('Alertas de Velocidade (acima de 80 km/h):')\n            self.add_metric('Total de Violações', f'{len(violations):,}')\n            self.add_metric('Percentual da Frota', f'{(len(violations)/total_records*100):.1f}%')\n            \n            if not violations.empty:\n                worst_violations = violations.nlargest(5, 'velocidade_km')\n                self.add_subsection('Top 5 Maiores Velocidades:')\n                self.add_table_header(['Placa', 'Cliente', 'Velocidade', 'Data/Hora'])\n                for _, row in worst_violations.iterrows():\n                    data = [\n                        row['placa'],\n                        row['cliente'][:15],\n                        f\"{row['velocidade_km']:.1f} km/h\",\n                        row['data'].strftime('%d/%m %H:%M')\n                    ]\n                    self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 5. ANÁLISE OPERACIONAL\n            self.add_header('5. ANÁLISE OPERACIONAL')\n            \n            # Status operacional\n            ignition_stats = df['ignicao'].value_counts() if 'ignicao' in df.columns else {}\n            blocked_stats = df['bloqueado'].value_counts() if 'bloqueado' in df.columns else {}\n            \n            self.add_subsection('Status dos Veículos:')\n            if 'ligado' in ignition_stats.index or 'desligado' in ignition_stats.index:\n                self.add_metric('Veículos com Ignição Ligada', f'{ignition_stats.get(\"ligado\", 0):,}')\n                self.add_metric('Veículos com Ignição Desligada', f'{ignition_stats.get(\"desligado\", 0):,}')\n            \n            if 'sim' in blocked_stats.index or 'não' in blocked_stats.index:\n                self.add_metric('Veículos Bloqueados', f'{blocked_stats.get(\"sim\", 0):,}')\n                self.add_metric('Veículos Desbloqueados', f'{blocked_stats.get(\"não\", 0):,}')\n            \n            # Análise temporal\n            df['hora'] = df['data'].dt.hour\n            peak_hour = df.groupby('hora').size().idxmax()\n            peak_count = df.groupby('hora').size().max()\n            \n            self.add_subsection('Padrões de Uso:')\n            self.add_metric('Horário de Pico', f'{peak_hour}:00 - {peak_hour+1}:00')\n            self.add_metric('Atividade no Pico', f'{peak_count:,} registros')\n            self.pdf.ln(5)\n            \n            # 6. ANÁLISE DE EFICIÊNCIA\n            self.add_header('6. ANÁLISE DE EFICIÊNCIA ENERGÉTICA')\n            \n            if 'bateria' in df.columns:\n                battery_avg = df['bateria'].mean()\n                battery_low = len(df[df['bateria'] < 12.0])\n                self.add_metric('Nível Médio de Bateria', f'{battery_avg:.1f}V')\n                self.add_metric('Alertas de Bateria Baixa', f'{battery_low:,}')\n            \n            # Eficiência por veículo\n            efficiency_stats = df.groupby('placa').agg({\n                'velocidade_km': 'mean',\n                'odometro_periodo_km': 'sum'\n            }).round(2)\n            \n            # Top 5 mais eficientes (maior distância, menor velocidade média)\n            efficiency_stats['score'] = efficiency_stats['odometro_periodo_km'] / (efficiency_stats['velocidade_km'] + 1)\n            top_efficient = efficiency_stats.nlargest(5, 'score')\n            \n            self.add_subsection('Top 5 Veículos Mais Eficientes:')\n            self.add_table_header(['Placa', 'Distância Total', 'Vel. Média', 'Score'])\n            for placa in top_efficient.index:\n                data = [\n                    placa,\n                    f\"{top_efficient.loc[placa, 'odometro_periodo_km']:.1f} km\",\n                    f\"{top_efficient.loc[placa, 'velocidade_km']:.1f} km/h\",\n                    f\"{top_efficient.loc[placa, 'score']:.2f}\"\n                ]\n                self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 7. RECOMENDAÇÕES\n            self.add_header('7. RECOMENDAÇÕES E ALERTAS')\n            \n            self.pdf.set_font('Arial', '', 10)\n            recommendations = []\n            \n            if len(violations) > total_records * 0.1:\n                recommendations.append(\"• Implementar treinamento de condução defensiva para reduzir violações de velocidade\")\n            \n            if gps_coverage < 95:\n                recommendations.append(\"• Verificar sistema GPS dos veículos com baixa cobertura\")\n            \n            low_activity_vehicles = vehicle_stats[vehicle_stats[('data', 'count')] < 50].index\n            if len(low_activity_vehicles) > 0:\n                recommendations.append(f\"• Investigar {len(low_activity_vehicles)} veículos com baixa atividade\")\n            \n            if not recommendations:\n                recommendations.append(\"• Frota operando dentro dos parâmetros normais\")\n                recommendations.append(\"• Continuar monitoramento regular\")\n            \n            for rec in recommendations:\n                self.pdf.cell(0, 6, rec.encode('latin-1', 'replace').decode('latin-1'), 0, 1)\n            \n            self.pdf.ln(10)\n            \n            # Rodapé\n            self.pdf.set_font('Arial', 'I', 8)\n            self.pdf.cell(0, 5, f'Relatório gerado automaticamente pelo Insight Hub em {datetime.now().strftime(\"%d/%m/%Y às %H:%M:%S\")}', 0, 1, 'C')\n            \n            # Salvar PDF\n            self.pdf.output(output_path)\n            return output_path\n            \n        except Exception as e:\n            # Em caso de erro, criar relatório básico\n            self.pdf.cell(0, 10, f'Erro ao gerar relatório detalhado: {str(e)}', 0, 1)\n            self.pdf.output(output_path)\n            return output_path\n    \n    def generate_fleet_report(self, output_path: str = \"relatorio_frota.pdf\") -> str:\n        \"\"\"Mantém compatibilidade - chama relatório completo\"\"\"\n        return self.generate_comprehensive_report(output_path)","size_bytes":13365},"utils/visualizations.py":{"content":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\n\nclass FleetVisualizations:\n    \"\"\"Classe para criar visualizações de dados de frota\"\"\"\n    \n    def __init__(self, analyzer):\n        \"\"\"Inicializa com um analisador de dados\"\"\"\n        self.analyzer = analyzer\n        self.colors = {\n            'primary': '#1f77b4',\n            'secondary': '#ff7f0e',\n            'success': '#2ca02c',\n            'warning': '#d62728',\n            'info': '#9467bd'\n        }\n    \n    def create_kpi_charts(self):\n        \"\"\"Cria gráficos de KPIs\"\"\"\n        kpis = self.analyzer.get_kpis()\n        \n        if not kpis:\n            return {}\n        \n        charts = {}\n        \n        # Gráfico de velocidade média por veículo\n        speed_by_vehicle = self.analyzer.filtered_df.groupby('placa')['velocidade_km'].mean().sort_values(ascending=False)\n        \n        speed_chart = px.bar(\n            x=speed_by_vehicle.values[:15],  # Top 15\n            y=speed_by_vehicle.index[:15],\n            orientation='h',\n            title='Velocidade Média por Veículo (Top 15)',\n            labels={'x': 'Velocidade Média (km/h)', 'y': 'Placa'}\n        )\n        speed_chart.update_traces(hovertemplate='<b>Veículo:</b> %{y}<br><b>Velocidade Média:</b> %{x:.1f} km/h<extra></extra>')\n        charts['speed_by_vehicle'] = speed_chart\n        \n        # Gráfico de distribuição de velocidade\n        dist_chart = px.histogram(\n            self.analyzer.filtered_df,\n            x='velocidade_km',\n            nbins=30,\n            title='Distribuição de Velocidade',\n            labels={'x': 'Velocidade (km/h)', 'y': 'Frequência'}\n        )\n        dist_chart.update_traces(hovertemplate='<b>Velocidade:</b> %{x:.1f} km/h<br><b>Frequência:</b> %{y}<extra></extra>')\n        charts['speed_distribution'] = dist_chart\n        \n        return charts\n    \n    def create_temporal_charts(self):\n        \"\"\"Cria gráficos temporais\"\"\"\n        df = self.analyzer.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        charts = {}\n        \n        # Atividade por hora do dia\n        hourly_activity = df.groupby(df['data'].dt.hour).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean'\n        }).reset_index()\n        \n        activity_chart = px.line(\n            hourly_activity,\n            x='data',\n            y='placa',\n            title='Veículos Ativos por Hora do Dia',\n            labels={'data': 'Hora', 'placa': 'Número de Veículos Ativos'}\n        )\n        activity_chart.update_traces(hovertemplate='<b>Hora:</b> %{x}h<br><b>Veículos Ativos:</b> %{y}<extra></extra>')\n        charts['hourly_activity'] = activity_chart\n        \n        # Velocidade média por hora\n        speed_hourly_chart = px.line(\n            hourly_activity,\n            x='data',\n            y='velocidade_km',\n            title='Velocidade Média por Hora do Dia',\n            labels={'data': 'Hora', 'velocidade_km': 'Velocidade Média (km/h)'}\n        )\n        speed_hourly_chart.update_traces(hovertemplate='<b>Hora:</b> %{x}h<br><b>Velocidade Média:</b> %{y:.1f} km/h<extra></extra>')\n        charts['hourly_speed'] = speed_hourly_chart\n        \n        # Atividade diária\n        daily_activity = df.groupby(df['data'].dt.date).agg({\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum',\n            'velocidade_km': 'mean'\n        }).reset_index()\n        \n        charts['daily_activity'] = px.line(\n            daily_activity,\n            x='data',\n            y='placa',\n            title='Veículos Ativos por Dia',\n            labels={'data': 'Data', 'placa': 'Número de Veículos Ativos'}\n        )\n        \n        return charts\n    \n    def create_compliance_charts(self):\n        \"\"\"Cria gráficos de compliance\"\"\"\n        compliance = self.analyzer.get_compliance_analysis()\n        \n        if not compliance:\n            return {}\n        \n        charts = {}\n        \n        # Score de compliance por veículo\n        if compliance['score_compliance']:\n            scores_df = pd.DataFrame(list(compliance['score_compliance'].items()), \n                                   columns=['Placa', 'Score'])\n            scores_df = scores_df.sort_values('Score', ascending=False)\n            \n            # Definir cores baseadas no score\n            colors = ['#2ca02c' if score >= 90 else '#ff7f0e' if score >= 70 else '#d62728' \n                     for score in scores_df['Score']]\n            \n            charts['compliance_scores'] = go.Figure(data=[\n                go.Bar(\n                    x=scores_df['Placa'],\n                    y=scores_df['Score'],\n                    marker_color=colors,\n                    text=scores_df['Score'].round(1),\n                    textposition='outside'\n                )\n            ])\n            charts['compliance_scores'].update_layout(\n                title='Score de Compliance por Veículo',\n                xaxis_title='Placa',\n                yaxis_title='Score (%)',\n                yaxis=dict(range=[0, 100])\n            )\n        \n        # Violações de velocidade\n        if 'detalhes_violacoes' in compliance and not compliance['detalhes_violacoes'].empty:\n            violations_df = compliance['detalhes_violacoes'].head(10).reset_index()\n            \n            charts['speed_violations'] = px.bar(\n                violations_df,\n                x='placa',\n                y=0,  # A coluna de contagem\n                title='Top 10 Veículos com Violações de Velocidade',\n                labels={'placa': 'Placa', '0': 'Número de Violações'}\n            )\n        \n        return charts\n    \n    def create_comparison_chart(self, comparison_data):\n        \"\"\"Cria gráfico de comparação entre veículos\"\"\"\n        if not comparison_data:\n            return None\n        \n        # Converter dados para DataFrame\n        df_comparison = pd.DataFrame(comparison_data).T.reset_index()\n        df_comparison.columns = ['Placa'] + list(df_comparison.columns[1:])\n        \n        # Criar gráfico de radar/spider\n        categories = ['Velocidade Média', 'Distância Total', 'Tempo Ativo', 'Cobertura GPS']\n        \n        fig = go.Figure()\n        \n        for _, row in df_comparison.iterrows():\n            fig.add_trace(go.Scatterpolar(\n                r=[\n                    row['velocidade_media'],\n                    row['distancia_total'] / 100,  # Normalizar\n                    row['tempo_ativo'] / 10,  # Normalizar\n                    row['cobertura_gps']\n                ],\n                theta=categories,\n                fill='toself',\n                name=row['Placa']\n            ))\n        \n        fig.update_layout(\n            polar=dict(\n                radialaxis=dict(\n                    visible=True,\n                    range=[0, 100]\n                )),\n            showlegend=True,\n            title=\"Comparação de Performance entre Veículos\"\n        )\n        \n        return fig\n    \n    def create_efficiency_charts(self):\n        \"\"\"Cria gráficos de eficiência\"\"\"\n        efficiency = self.analyzer.get_efficiency_metrics()\n        \n        if not efficiency:\n            return {}\n        \n        charts = {}\n        \n        # Top veículos por quilometragem\n        if 'top_veiculos_km' in efficiency:\n            top_km = efficiency['top_veiculos_km'].head(10)\n            \n            charts['top_km'] = px.bar(\n                x=top_km.values,\n                y=top_km.index,\n                orientation='h',\n                title='Top 10 Veículos por Quilometragem Total',\n                labels={'x': 'Quilometragem Total (km)', 'y': 'Placa'}\n            )\n        \n        # Veículos mais utilizados\n        if 'veiculos_mais_utilizados' in efficiency:\n            most_used = efficiency['veiculos_mais_utilizados'].head(10)\n            \n            charts['most_used'] = px.bar(\n                x=most_used.values,\n                y=most_used.index,\n                orientation='h',\n                title='Top 10 Veículos Mais Utilizados',\n                labels={'x': 'Número de Registros', 'y': 'Placa'}\n            )\n        \n        return charts\n    \n    def create_speed_analysis_charts(self):\n        \"\"\"Cria gráficos de análise de velocidade\"\"\"\n        speed_analysis = self.analyzer.get_speed_analysis()\n        \n        if not speed_analysis:\n            return {}\n        \n        charts = {}\n        \n        # Distribuição por faixas de velocidade\n        if 'distribuicao' in speed_analysis:\n            dist_data = speed_analysis['distribuicao']\n            \n            charts['speed_ranges'] = px.pie(\n                values=dist_data.values,\n                names=dist_data.index,\n                title='Distribuição por Faixas de Velocidade'\n            )\n        \n        # Velocidade por hora do dia\n        if 'velocidade_por_hora' in speed_analysis:\n            hourly_speed = speed_analysis['velocidade_por_hora']\n            \n            charts['speed_by_hour'] = px.line(\n                x=hourly_speed.index,\n                y=hourly_speed.values,\n                title='Velocidade Média por Hora do Dia',\n                labels={'x': 'Hora', 'y': 'Velocidade Média (km/h)'}\n            )\n        \n        return charts\n    \n    def create_map_visualization(self, sample_size=1000):\n        \"\"\"Cria visualização de mapa com as localizações\"\"\"\n        df = self.analyzer.filtered_df\n        \n        if df.empty or 'localizacao' not in df.columns:\n            return None\n        \n        # Filtrar apenas uma amostra para performance\n        if len(df) > sample_size:\n            df_sample = df.sample(n=sample_size)\n        else:\n            df_sample = df\n        \n        # Extrair coordenadas (assumindo formato \"lat,lng\")\n        coords_data = []\n        for _, row in df_sample.iterrows():\n            try:\n                if pd.notna(row['localizacao']) and ',' in str(row['localizacao']):\n                    lat, lng = map(float, str(row['localizacao']).split(','))\n                    coords_data.append({\n                        'lat': lat,\n                        'lng': lng,\n                        'placa': row['placa'],\n                        'velocidade': row['velocidade_km'],\n                        'data': row['data']\n                    })\n            except:\n                continue\n        \n        if not coords_data:\n            return None\n        \n        coords_df = pd.DataFrame(coords_data)\n        \n        # Criar mapa\n        fig = px.scatter_mapbox(\n            coords_df,\n            lat='lat',\n            lon='lng',\n            color='velocidade',\n            size='velocidade',\n            hover_data=['placa', 'data'],\n            color_continuous_scale='Viridis',\n            mapbox_style='open-street-map',\n            title='Localização dos Veículos (Amostra)',\n            zoom=10\n        )\n        \n        fig.update_layout(height=500)\n        \n        return fig\n    \n    def create_dashboard_summary(self):\n        \"\"\"Cria resumo visual para dashboard\"\"\"\n        kpis = self.analyzer.get_kpis()\n        \n        if not kpis:\n            return None\n        \n        # Criar gráfico de gauge para métricas principais\n        fig = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('Cobertura GPS (%)', 'Utilização da Frota', \n                          'Velocidade Média', 'Eficiência Operacional'),\n            specs=[[{\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n                   [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}]]\n        )\n        \n        # Gauge para cobertura GPS\n        fig.add_trace(go.Indicator(\n            mode = \"gauge+number\",\n            value = kpis['cobertura_gps'],\n            domain = {'x': [0, 1], 'y': [0, 1]},\n            title = {'text': \"GPS\"},\n            gauge = {\n                'axis': {'range': [None, 100]},\n                'bar': {'color': \"darkblue\"},\n                'steps': [\n                    {'range': [0, 50], 'color': \"lightgray\"},\n                    {'range': [50, 80], 'color': \"gray\"}],\n                'threshold': {\n                    'line': {'color': \"red\", 'width': 4},\n                    'thickness': 0.75,\n                    'value': 90}}\n        ), row=1, col=1)\n        \n        # Mais indicadores podem ser adicionados aqui...\n        \n        fig.update_layout(height=400)\n        \n        return fig\n","size_bytes":12494},".streamlit/config.toml":{"content":"[server]\nheadless = true\naddress = \"0.0.0.0\"\nport = 5000\n\n[theme]\nbase = \"light\"\nprimaryColor = \"#1f77b4\"\nbackgroundColor = \"#ffffff\"\nsecondaryBackgroundColor = \"#f0f2f6\"\n","size_bytes":171},"pages/8_🚨_Controle_Operacional.py":{"content":"import streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime, time, timedelta\nimport pytz\nfrom database.db_manager import DatabaseManager\nfrom utils.data_analyzer import DataAnalyzer\n\ndef main():\n    st.title(\"🚨 Controle Operacional\")\n    st.markdown(\"**Monitoramento de conformidade operacional das vans da prefeitura**\")\n    \n    # Carregar dados diretamente\n    df_inicial = DatabaseManager.get_dashboard_data()\n    if df_inicial.empty:\n        st.warning(\"⚠️ Não há dados carregados. Faça upload de arquivos CSV primeiro.\")\n        return\n    else:\n        st.success(f\"✅ Dados carregados: {len(df_inicial):,} registros para controle operacional\")\n    \n    # Sidebar com filtros\n    with st.sidebar:\n        st.header(\"🔍 Filtros\")\n        \n        # Filtro de cliente\n        clients = get_client_list()\n        selected_client = st.selectbox(\n            \"Cliente:\",\n            [\"Todos\"] + clients,\n            index=0\n        )\n        \n        # Filtro de veículo\n        vehicles = get_vehicle_list(selected_client if selected_client != \"Todos\" else None)\n        selected_vehicle = st.selectbox(\n            \"Veículo:\",\n            [\"Todos\"] + vehicles,\n            index=0\n        )\n        \n        # Filtro de período\n        st.subheader(\"📅 Período de Análise\")\n        col1, col2 = st.columns(2)\n        with col1:\n            start_date = st.date_input(\n                \"Data Início:\",\n                value=datetime(2025, 8, 6).date()  # Início dos dados disponíveis\n            )\n        with col2:\n            end_date = st.date_input(\n                \"Data Fim:\",\n                value=datetime(2025, 8, 31).date()  # Fim dos dados disponíveis\n            )\n        \n        # Filtros de horário autorizado\n        st.subheader(\"⏰ Filtros de Horário\")\n        \n        # Botões de preset para períodos autorizados\n        st.markdown(\"**Períodos Autorizados:**\")\n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if st.button(\"🌅 Manhã\\n04:00-07:00\", key=\"morning\"):\n                st.session_state.time_filter = \"morning\"\n        with col2:\n            if st.button(\"🍽️ Almoço\\n10:50-13:00\", key=\"lunch\"):\n                st.session_state.time_filter = \"lunch\"\n        with col3:\n            if st.button(\"🌇 Tarde\\n16:50-19:00\", key=\"afternoon\"):\n                st.session_state.time_filter = \"afternoon\"\n        \n        # Filtro personalizado de horário\n        time_filter_mode = st.selectbox(\n            \"Filtrar por período:\",\n            [\"Todos os horários\", \"Apenas horários autorizados\", \"Apenas violações de horário\", \"Período personalizado\"],\n            index=0\n        )\n        \n        custom_start_time = None\n        custom_end_time = None\n        if time_filter_mode == \"Período personalizado\":\n            col1, col2 = st.columns(2)\n            with col1:\n                custom_start_time = st.time_input(\"Hora início:\", value=time(0, 0))\n            with col2:\n                custom_end_time = st.time_input(\"Hora fim:\", value=time(23, 59))\n        \n        # Critérios de movimento\n        st.subheader(\"🚗 Critérios de Movimento\")\n        include_stationary = st.checkbox(\n            \"Incluir veículos parados na análise de violações\",\n            value=False,\n            help=\"Quando desmarcado, apenas veículos em movimento (velocidade > 0 ou ignição ligada) serão considerados para violações\"\n        )\n        \n        # Limites de velocidade\n        st.subheader(\"⚡ Configurações\")\n        speed_violation_threshold = st.slider(\n            \"Limite de velocidade (km/h):\",\n            min_value=10,\n            max_value=120,\n            value=80,\n            step=5,\n            help=\"Velocidades acima deste valor serão destacadas como picos\"\n        )\n    \n    # Aplicar filtros aos dados já carregados\n    # Usar método centralizado de filtros do DataAnalyzer\n    analyzer = DataAnalyzer(df_inicial)\n    \n    # Aplicar filtros com método centralizado do DataAnalyzer\n    df = analyzer.apply_filters(\n        cliente=selected_client,\n        placa=selected_vehicle,\n        data_inicio=start_date,\n        data_fim=end_date\n    )\n    \n    # Aplicar filtros de horário se especificado\n    if time_filter_mode != \"Todos os horários\" and not df.empty:\n        df = apply_time_filters(df, time_filter_mode, custom_start_time, custom_end_time)\n    \n    if df.empty:\n        st.warning(\"⚠️ Nenhum dado encontrado para os filtros selecionados.\")\n        return\n    \n    # Processar dados para análise operacional\n    df = process_operational_data(df, include_stationary=include_stationary)\n    \n    # Abas principais\n    tab1, tab2, tab3, tab4 = st.tabs([\n        \"📊 Resumo Operacional\", \n        \"⚠️ Violações Detectadas\", \n        \"🗺️ Mapa de Trajetos\", \n        \"📋 Relatório Detalhado\"\n    ])\n    \n    with tab1:\n        show_operational_summary(df)\n    \n    with tab2:\n        show_violations(df)\n    \n    with tab3:\n        show_trajectory_map(df)\n    \n    with tab4:\n        show_detailed_report(df)\n\n@st.cache_data(ttl=300)\ndef get_client_list():\n    \"\"\"Busca lista de clientes com cache para melhor performance\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty and 'cliente' in df.columns:\n            return sorted(df['cliente'].unique().tolist())\n        return []\n    except Exception as e:\n        st.error(f\"Erro ao carregar clientes: {str(e)}\")\n        return []\n\n@st.cache_data(ttl=300)\ndef get_vehicle_list(client_filter=None):\n    \"\"\"Busca lista de veículos com cache para melhor performance\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty and 'placa' in df.columns:\n            # Filtrar por cliente se especificado\n            if client_filter and client_filter != \"Todos\":\n                df = df[df['cliente'] == client_filter]\n            return sorted(df['placa'].unique().tolist())\n        return []\n    except Exception as e:\n        st.error(f\"Erro ao carregar veículos: {str(e)}\")\n        return []\n\n# Função removida - agora usando método centralizado DataAnalyzer.apply_filters()\n\ndef load_filtered_data(client_filter, vehicle_filter, start_date, end_date):\n    \"\"\"Função mantida para compatibilidade - Carrega dados filtrados\"\"\"\n    try:\n        client_f = None if client_filter == \"Todos\" else client_filter\n        vehicle_f = None if vehicle_filter == \"Todos\" else vehicle_filter\n        \n        start_datetime = datetime.combine(start_date, datetime.min.time())\n        end_datetime = datetime.combine(end_date, datetime.max.time())\n        \n        df = DatabaseManager.get_dashboard_data(\n            client_filter=client_f,\n            vehicle_filter=vehicle_f,\n            start_date=start_datetime,\n            end_date=end_datetime\n        )\n        \n        return df\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {str(e)}\")\n        return pd.DataFrame()\n\ndef apply_time_filters(df, time_filter_mode, custom_start_time=None, custom_end_time=None):\n    \"\"\"Aplica filtros de horário aos dados\"\"\"\n    if df.empty:\n        return df\n        \n    df_filtered = df.copy()\n    \n    # Verificar se colunas necessárias existem\n    if 'data' not in df_filtered.columns:\n        return df_filtered\n    \n    # Converter para datetime se necessário\n    if not pd.api.types.is_datetime64_any_dtype(df_filtered['data']):\n        df_filtered['data'] = pd.to_datetime(df_filtered['data'], errors='coerce')\n    \n    # Filtrar por período autorizado ou violações\n    if time_filter_mode == \"Apenas horários autorizados\":\n        # Criar lógica de horário autorizado se não existir\n        if 'operacao_autorizada' not in df_filtered.columns:\n            df_filtered = create_authorization_logic(df_filtered)\n        df_filtered = df_filtered[df_filtered['operacao_autorizada'] == True]\n    elif time_filter_mode == \"Apenas violações de horário\":\n        # Criar lógica de horário autorizado se não existir\n        if 'operacao_autorizada' not in df_filtered.columns:\n            df_filtered = create_authorization_logic(df_filtered)\n        df_filtered = df_filtered[df_filtered['operacao_autorizada'] == False]\n    elif time_filter_mode == \"Período personalizado\" and custom_start_time and custom_end_time:\n        # Filtrar por horário personalizado\n        hour_filter = (\n            (df_filtered['data'].dt.time >= custom_start_time) & \n            (df_filtered['data'].dt.time <= custom_end_time)\n        )\n        df_filtered = df_filtered[hour_filter]\n    \n    return df_filtered\n\ndef create_authorization_logic(df):\n    \"\"\"Cria a lógica de autorização para o DataFrame\"\"\"\n    if df.empty:\n        return df\n    \n    df = df.copy()\n    \n    # Extrair informações de tempo\n    df['hora'] = df['data'].dt.hour\n    df['minuto'] = df['data'].dt.minute\n    df['dia_semana'] = df['data'].dt.dayofweek  # 0=Segunda, 6=Domingo\n    \n    # Determinar se o horário está dentro dos períodos autorizados\n    df['horario_permitido'] = df.apply(lambda row: is_authorized_time_simple(row['hora'], row['minuto']), axis=1)\n    df['dia_util'] = df['dia_semana'] < 5  # Segunda a Sexta (0-4)\n    df['operacao_autorizada'] = df['horario_permitido'] & df['dia_util']\n    \n    return df\n\ndef is_authorized_time_simple(hora, minuto):\n    \"\"\"Verifica se o horário está dentro dos períodos autorizados - versão simplificada\"\"\"\n    time_current = time(hora, minuto)\n    \n    # Horários permitidos pela prefeitura\n    morning_start = time(4, 0)   # 04:00\n    morning_end = time(7, 0)     # 07:00\n    \n    lunch_start = time(10, 50)   # 10:50\n    lunch_end = time(13, 0)      # 13:00\n    \n    afternoon_start = time(16, 50)  # 16:50\n    afternoon_end = time(19, 0)     # 19:00\n    \n    # Verificar se está em algum período permitido\n    is_morning = morning_start <= time_current <= morning_end\n    is_lunch = lunch_start <= time_current <= lunch_end\n    is_afternoon = afternoon_start <= time_current <= afternoon_end\n    \n    return is_morning or is_lunch or is_afternoon\n\ndef safe_column_access(df, column, default_value=None, numeric=False):\n    \"\"\"Acesso seguro a colunas do DataFrame com valores padrão\"\"\"\n    if column not in df.columns:\n        if numeric:\n            return pd.Series([0] * len(df), index=df.index)\n        else:\n            return pd.Series([default_value] * len(df), index=df.index)\n    \n    series = df[column]\n    if numeric:\n        return pd.to_numeric(series, errors='coerce').fillna(0)\n    else:\n        return series.fillna(default_value if default_value is not None else '')\n\ndef process_operational_data(df, include_stationary=False):\n    \"\"\"Processa dados para análise operacional com critérios de movimento\"\"\"\n    if df.empty:\n        return df\n    \n    df = df.copy()\n    \n    # Verificar e converter colunas essenciais com acesso seguro\n    if 'data' in df.columns:\n        df['data'] = pd.to_datetime(df['data'], errors='coerce')\n    else:\n        # Se não há coluna de data, criar uma padrão\n        df['data'] = pd.Timestamp.now()\n    \n    # Acessar colunas de forma segura\n    df['velocidade_km'] = safe_column_access(df, 'velocidade_km', 0, numeric=True)\n    df['ignicao'] = safe_column_access(df, 'ignicao', 'D')\n    df['latitude'] = safe_column_access(df, 'latitude', 0, numeric=True)\n    df['longitude'] = safe_column_access(df, 'longitude', 0, numeric=True)\n    df['endereco'] = safe_column_access(df, 'endereco', 'Local não informado')\n    df['placa'] = safe_column_access(df, 'placa', 'Placa não informada')\n    \n    # Extrair informações de tempo\n    df['hora'] = df['data'].dt.hour\n    df['minuto'] = df['data'].dt.minute\n    df['dia_semana'] = df['data'].dt.dayofweek  # 0=Segunda, 6=Domingo\n    df['dia_semana_nome'] = df['data'].dt.strftime('%A')\n    df['data_date'] = df['data'].dt.date\n    df['hora_minuto'] = df['data'].dt.time\n    \n    # Determinar se o veículo está em movimento\n    df['em_movimento'] = (\n        (df['velocidade_km'] > 0) | \n        (df['ignicao'].isin(['D', 'L', 'Dirigindo', 'Ligado']))\n    )\n    \n    # Definir horários permitidos pela prefeitura\n    df['horario_permitido'] = df.apply(is_authorized_time, axis=1)\n    df['dia_util'] = df['dia_semana'] < 5  # Segunda a Sexta (0-4)\n    df['operacao_autorizada'] = df['horario_permitido'] & df['dia_util']\n    \n    # Aplicar critério de movimento para violações (se configurado)\n    if not include_stationary:\n        # Apenas considerar violações quando o veículo está em movimento\n        df['violacao_considerada'] = ~df['operacao_autorizada'] & df['em_movimento']\n    else:\n        # Considerar todas as violações, independente do movimento\n        df['violacao_considerada'] = ~df['operacao_autorizada']\n    \n    # Classificar violações considerando movimento\n    df['tipo_violacao'] = df.apply(lambda row: classify_violation(row, include_stationary), axis=1)\n    \n    return df\n\ndef is_authorized_time(row):\n    \"\"\"Verifica se o horário está dentro dos períodos autorizados\"\"\"\n    hora = row['hora']\n    minuto = row['minuto']\n    time_current = time(hora, minuto)\n    \n    # Horários permitidos pela prefeitura\n    morning_start = time(4, 0)   # 04:00\n    morning_end = time(7, 0)     # 07:00\n    \n    lunch_start = time(10, 50)   # 10:50\n    lunch_end = time(13, 0)      # 13:00\n    \n    afternoon_start = time(16, 50)  # 16:50\n    afternoon_end = time(19, 0)     # 19:00\n    \n    # Verificar se está em algum período permitido\n    is_morning = morning_start <= time_current <= morning_end\n    is_lunch = lunch_start <= time_current <= lunch_end\n    is_afternoon = afternoon_start <= time_current <= afternoon_end\n    \n    return is_morning or is_lunch or is_afternoon\n\ndef classify_violation(row, include_stationary=False):\n    \"\"\"Classifica o tipo de violação considerando critérios de movimento\"\"\"\n    # Se a operação é autorizada, sempre é válida\n    if row['operacao_autorizada']:\n        return \"✅ Autorizada\"\n    \n    # Se não devemos incluir veículos parados e o veículo não está em movimento\n    if not include_stationary and not row.get('em_movimento', True):\n        return \"🚙 Parado (Não Analisado)\"\n    \n    # Classificar tipos de violação\n    if not row['dia_util']:\n        return \"🚫 Final de Semana\"\n    elif not row['horario_permitido']:\n        return \"⏰ Horário Não Autorizado\"\n    else:\n        return \"❓ Outros\"\n\ndef show_operational_summary(df):\n    \"\"\"Mostra resumo operacional\"\"\"\n    st.markdown(\"### 📊 Resumo Operacional\")\n    \n    # Métricas principais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    total_records = len(df)\n    authorized_records = len(df[df['operacao_autorizada']])\n    violation_records = len(df[~df['operacao_autorizada']])\n    compliance_rate = (authorized_records / total_records * 100) if total_records > 0 else 0\n    \n    with col1:\n        st.metric(\n            \"📊 Total de Registros\",\n            f\"{total_records:,}\",\n            help=\"Total de registros no período selecionado\"\n        )\n    \n    with col2:\n        st.metric(\n            \"✅ Operações Autorizadas\",\n            f\"{authorized_records:,}\",\n            delta=f\"{(authorized_records/total_records*100):.1f}%\" if total_records > 0 else \"0%\"\n        )\n    \n    with col3:\n        st.metric(\n            \"⚠️ Violações Detectadas\",\n            f\"{violation_records:,}\",\n            delta=f\"-{(violation_records/total_records*100):.1f}%\" if total_records > 0 else \"0%\",\n            delta_color=\"inverse\"\n        )\n    \n    with col4:\n        st.metric(\n            \"📈 Taxa de Conformidade\",\n            f\"{compliance_rate:.1f}%\",\n            delta=\"Meta: 100%\",\n            delta_color=\"normal\" if compliance_rate >= 95 else \"inverse\"\n        )\n    \n    # Gráfico de distribuição por tipo de operação\n    st.markdown(\"### 📈 Distribuição de Operações\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        # Gráfico de pizza - Autorizada vs Violação\n        violation_counts = df['operacao_autorizada'].value_counts()\n        \n        fig_pie = px.pie(\n            values=violation_counts.values,\n            names=['✅ Autorizadas' if x else '⚠️ Violações' for x in violation_counts.index],\n            title=\"Proporção de Operações\",\n            color_discrete_map={\n                '✅ Autorizadas': '#2E8B57',\n                '⚠️ Violações': '#DC143C'\n            }\n        )\n        st.plotly_chart(fig_pie, use_container_width=True)\n    \n    with col2:\n        # Gráfico de barras - Tipos de violação\n        violation_types = df['tipo_violacao'].value_counts()\n        \n        fig_bar = px.bar(\n            x=violation_types.index,\n            y=violation_types.values,\n            title=\"Tipos de Violação\",\n            color=violation_types.index,\n            color_discrete_map={\n                '✅ Autorizada': '#2E8B57',\n                '🚫 Final de Semana': '#FF4500',\n                '⏰ Horário Não Autorizado': '#DC143C',\n                '❓ Outros': '#696969'\n            }\n        )\n        fig_bar.update_layout(showlegend=False)\n        st.plotly_chart(fig_bar, use_container_width=True)\n\ndef show_violations(df):\n    \"\"\"Mostra violações detectadas\"\"\"\n    st.markdown(\"### ⚠️ Violações Operacionais Detectadas\")\n    \n    # Filtrar apenas violações\n    violations_df = df[~df['operacao_autorizada']].copy()\n    \n    if violations_df.empty:\n        st.success(\"🎉 Nenhuma violação detectada no período selecionado!\")\n        return\n    \n    # Resumo das violações\n    col1, col2, col3 = st.columns(3)\n    \n    weekend_violations = len(violations_df[violations_df['tipo_violacao'] == '🚫 Final de Semana'])\n    time_violations = len(violations_df[violations_df['tipo_violacao'] == '⏰ Horário Não Autorizado'])\n    \n    with col1:\n        st.metric(\"🚫 Final de Semana\", f\"{weekend_violations:,}\")\n    with col2:\n        st.metric(\"⏰ Horário Irregular\", f\"{time_violations:,}\")\n    with col3:\n        unique_vehicles = violations_df['placa'].nunique()\n        st.metric(\"🚗 Veículos Envolvidos\", unique_vehicles)\n    \n    # Análise temporal das violações\n    st.markdown(\"#### 📅 Violações por Dia\")\n    \n    daily_violations = violations_df.groupby(['data_date', 'tipo_violacao']).size().reset_index(name='count')\n    daily_violations['data_date'] = pd.to_datetime(daily_violations['data_date'])\n    \n    fig_daily = px.bar(\n        daily_violations,\n        x='data_date',\n        y='count',\n        color='tipo_violacao',\n        title=\"Violações Diárias por Tipo\",\n        color_discrete_map={\n            '🚫 Final de Semana': '#FF4500',\n            '⏰ Horário Não Autorizado': '#DC143C',\n            '❓ Outros': '#696969'\n        }\n    )\n    st.plotly_chart(fig_daily, use_container_width=True)\n    \n    # Tabela detalhada das violações\n    st.markdown(\"#### 📋 Detalhes das Violações\")\n    \n    # Preparar dados para tabela\n    display_df = violations_df[[\n        'data', 'placa', 'tipo_violacao', 'velocidade_km', 'endereco'\n    ]].copy()\n    \n    display_df['data'] = display_df['data'].dt.strftime('%d/%m/%Y %H:%M:%S')\n    display_df = display_df.rename(columns={\n        'data': 'Data/Hora',\n        'placa': 'Veículo',\n        'tipo_violacao': 'Tipo de Violação',\n        'velocidade_km': 'Velocidade (km/h)',\n        'endereco': 'Local'\n    })\n    \n    # Ordenar por data mais recente\n    display_df = display_df.sort_values('Data/Hora', ascending=False)\n    \n    st.dataframe(\n        display_df.head(100),  # Limitar a 100 registros para performance\n        use_container_width=True,\n        hide_index=True\n    )\n    \n    if len(display_df) > 100:\n        st.info(f\"📋 Mostrando os 100 registros mais recentes de {len(display_df):,} violações totais.\")\n\ndef show_trajectory_map(df):\n    \"\"\"Mostra mapa de trajetos e picos de velocidade\"\"\"\n    st.markdown(\"### 🗺️ Mapa de Trajetos e Velocidade\")\n    \n    # Verificar se há coordenadas\n    map_df = df[(df['latitude'].notna()) & (df['longitude'].notna()) & \n                (df['latitude'] != 0) & (df['longitude'] != 0)].copy()\n    \n    if map_df.empty:\n        st.warning(\"⚠️ Nenhum dado com coordenadas GPS válidas encontrado.\")\n        return\n    \n    # Filtros para o mapa\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        map_filter = st.selectbox(\n            \"Mostrar no mapa:\",\n            [\"Todas as operações\", \"Apenas violações\", \"Apenas autorizadas\"]\n        )\n    \n    with col2:\n        speed_threshold = st.number_input(\n            \"Pico de velocidade acima de (km/h):\",\n            min_value=0,\n            max_value=200,\n            value=80,\n            step=5\n        )\n    \n    with col3:\n        if st.button(\"🔄 Atualizar Mapa\"):\n            st.rerun()\n    \n    # Filtrar dados do mapa\n    if map_filter == \"Apenas violações\":\n        map_df = map_df[~map_df['operacao_autorizada']]\n    elif map_filter == \"Apenas autorizadas\":\n        map_df = map_df[map_df['operacao_autorizada']]\n    \n    if map_df.empty:\n        st.warning(\"⚠️ Nenhum dado para exibir no mapa com os filtros selecionados.\")\n        return\n    \n    # Identificar picos de velocidade\n    map_df['pico_velocidade'] = map_df['velocidade_km'] > speed_threshold\n    \n    # Preparar dados para o mapa com cores por status\n    map_df['cor_status'] = map_df.apply(lambda row: \n        '#FF0000' if row['pico_velocidade'] else  # Vermelho para picos de velocidade\n        '#DC143C' if row['tipo_violacao'] == '⏰ Horário Não Autorizado' else  # Vermelho escuro para violações de horário\n        '#FF4500' if row['tipo_violacao'] == '🚫 Final de Semana' else  # Laranja para final de semana\n        '#2E8B57' if row['tipo_violacao'] == '✅ Autorizada' else  # Verde para autorizadas\n        '#696969', axis=1  # Cinza para outros\n    )\n    \n    map_df['tamanho_ponto'] = map_df['pico_velocidade'].apply(lambda x: 12 if x else 6)\n    map_df['hover_info'] = map_df.apply(lambda row: \n        f\"<b>{row['placa']}</b><br>\" +\n        f\"Data: {row['data'].strftime('%d/%m/%Y %H:%M')}<br>\" +\n        f\"Velocidade: {row['velocidade_km']:.1f} km/h<br>\" +\n        f\"Status: {row['tipo_violacao']}<br>\" +\n        f\"Local: {row['endereco'][:50]}...\", axis=1\n    )\n    \n    # Calcular centro do mapa\n    center_lat = map_df['latitude'].mean()\n    center_lon = map_df['longitude'].mean()\n    \n    # Criar mapa real com Mapbox e OpenStreetMap\n    try:\n        # Usar scatter_mapbox para mapa real\n        fig_map = px.scatter_mapbox(\n            map_df,\n            lat='latitude',\n            lon='longitude',\n            color='tipo_violacao',\n            size='tamanho_ponto',\n            hover_data={\n                'placa': True,\n                'velocidade_km': ':.1f',\n                'data': True,\n                'endereco': True,\n                'latitude': False,\n                'longitude': False,\n                'tamanho_ponto': False\n            },\n            color_discrete_map={\n                '✅ Autorizada': '#2E8B57',\n                '🚫 Final de Semana': '#FF4500', \n                '⏰ Horário Não Autorizado': '#DC143C',\n                '🚗 Parado (Não Analisado)': '#808080',\n                '❓ Outros': '#696969'\n            },\n            title=\"🗺️ Mapa Real de Trajetos e Operações\",\n            mapbox_style=\"open-street-map\",  # Usar OpenStreetMap\n            height=600,\n            zoom=12\n        )\n        \n        # Configurar layout do mapa\n        fig_map.update_layout(\n            mapbox=dict(\n                center=dict(lat=center_lat, lon=center_lon),\n                zoom=12\n            ),\n            title={\n                'text': \"🗺️ Mapa Real de Trajetos e Operações\",\n                'x': 0.5,\n                'xanchor': 'center'\n            },\n            showlegend=True,\n            height=600\n        )\n        \n        # Adicionar linhas de trajeto por veículo\n        vehicles = map_df['placa'].unique()\n        colors = px.colors.qualitative.Set1\n        \n        for i, vehicle in enumerate(vehicles[:5]):  # Limitar a 5 veículos para performance\n            vehicle_df = map_df[map_df['placa'] == vehicle].sort_values('data')\n            if len(vehicle_df) > 1:  # Só adicionar linha se houver múltiplos pontos\n                color = colors[i % len(colors)]\n                \n                # Adicionar linha de trajeto\n                fig_map.add_trace(\n                    go.Scattermapbox(\n                        lat=vehicle_df['latitude'],\n                        lon=vehicle_df['longitude'],\n                        mode='lines',\n                        line=dict(width=2, color=color),\n                        name=f'Trajeto {vehicle}',\n                        showlegend=True,\n                        opacity=0.7\n                    )\n                )\n        \n        st.plotly_chart(fig_map, use_container_width=True)\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar mapa: {str(e)}\")\n        st.info(\"🗺️ Usando visualização alternativa...\")\n        \n        # Fallback: usar pydeck se mapbox falhar\n        try:\n            import pydeck as pdk\n            \n            # Configurar layer do pydeck\n            layer = pdk.Layer(\n                'ScatterplotLayer',\n                data=map_df,\n                get_position='[longitude, latitude]',\n                get_color='[255, 0, 0, 160]',  # Vermelho semi-transparente\n                get_radius=50,\n                pickable=True\n            )\n            \n            # Configurar viewport\n            view_state = pdk.ViewState(\n                latitude=center_lat,\n                longitude=center_lon,\n                zoom=12,\n                pitch=0\n            )\n            \n            # Criar deck\n            deck = pdk.Deck(\n                layers=[layer],\n                initial_view_state=view_state,\n                tooltip={\n                    'text': 'Veículo: {placa}\\nVelocidade: {velocidade_km} km/h\\nStatus: {tipo_violacao}'\n                }\n            )\n            \n            st.pydeck_chart(deck)\n            \n        except Exception as e2:\n            st.error(f\"Erro ao carregar visualização alternativa: {str(e2)}\")\n            st.warning(\"⚠️ Mapa não pôde ser carregado. Verifique os dados de coordenadas.\")\n    \n    # Estatísticas do mapa\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"📍 Pontos no Mapa\", f\"{len(map_df):,}\")\n    with col2:\n        st.metric(\"⭐ Picos de Velocidade\", f\"{len(map_df[map_df['pico_velocidade']]):,}\")\n    with col3:\n        st.metric(\"🚗 Veículos Únicos\", f\"{map_df['placa'].nunique()}\")\n    with col4:\n        avg_speed = map_df['velocidade_km'].mean()\n        st.metric(\"⚡ Velocidade Média\", f\"{avg_speed:.1f} km/h\")\n\ndef show_detailed_report(df):\n    \"\"\"Mostra relatório detalhado\"\"\"\n    st.markdown(\"### 📋 Relatório Detalhado de Conformidade\")\n    \n    # Relatório por veículo\n    st.markdown(\"#### 🚗 Análise por Veículo\")\n    \n    vehicle_summary = df.groupby('placa').agg({\n        'data': 'count',\n        'operacao_autorizada': ['sum', 'mean'],\n        'velocidade_km': ['mean', 'max'],\n        'tipo_violacao': lambda x: (x != '✅ Autorizada').sum()\n    }).round(2)\n    \n    vehicle_summary.columns = ['Total Registros', 'Operações Autorizadas', 'Taxa Conformidade (%)', \n                              'Velocidade Média', 'Velocidade Máxima', 'Total Violações']\n    vehicle_summary['Taxa Conformidade (%)'] = (vehicle_summary['Taxa Conformidade (%)'] * 100).round(1)\n    \n    # Colorir células baseado na conformidade\n    def color_compliance(val):\n        if isinstance(val, (int, float)):\n            if val >= 95:\n                return 'color: green'\n            elif val >= 80:\n                return 'color: orange'\n            else:\n                return 'color: red'\n        return ''\n    \n    styled_df = vehicle_summary.style.applymap(\n        color_compliance, \n        subset=['Taxa Conformidade (%)']\n    )\n    \n    st.dataframe(styled_df, use_container_width=True)\n    \n    # Relatório por dia da semana\n    st.markdown(\"#### 📅 Análise por Dia da Semana\")\n    \n    weekday_names = {\n        0: 'Segunda', 1: 'Terça', 2: 'Quarta', 3: 'Quinta', \n        4: 'Sexta', 5: 'Sábado', 6: 'Domingo'\n    }\n    \n    df['dia_semana_nome'] = df['dia_semana'].map(weekday_names)\n    \n    weekday_summary = df.groupby('dia_semana_nome').agg({\n        'data': 'count',\n        'operacao_autorizada': ['sum', 'mean'],\n        'tipo_violacao': lambda x: (x != '✅ Autorizada').sum()\n    }).round(2)\n    \n    weekday_summary.columns = ['Total Registros', 'Operações Autorizadas', 'Taxa Conformidade', 'Total Violações']\n    weekday_summary['Taxa Conformidade'] = (weekday_summary['Taxa Conformidade'] * 100).round(1)\n    \n    # Reordenar dias da semana\n    day_order = ['Segunda', 'Terça', 'Quarta', 'Quinta', 'Sexta', 'Sábado', 'Domingo']\n    weekday_summary = weekday_summary.reindex([day for day in day_order if day in weekday_summary.index])\n    \n    fig_weekday = px.bar(\n        weekday_summary.reset_index(),\n        x='dia_semana_nome',\n        y='Taxa Conformidade',\n        title=\"Taxa de Conformidade por Dia da Semana\",\n        color='Taxa Conformidade',\n        color_continuous_scale='RdYlGn'\n    )\n    fig_weekday.update_layout(showlegend=False)\n    st.plotly_chart(fig_weekday, use_container_width=True)\n    \n    # Exportar relatório\n    if st.button(\"📄 Exportar Relatório Completo\"):\n        # Aqui você pode implementar exportação para PDF ou Excel\n        st.success(\"🎉 Funcionalidade de exportação será implementada em breve!\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":29902},"pages/5_🗺️_Mapa_de_Rotas.py":{"content":"\"\"\"Painel de Mapa de Rotas - Análise de Trajetos e Velocidade\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nfrom utils.data_analyzer import DataAnalyzer\nimport folium\nfrom streamlit_folium import st_folium\n\nst.set_page_config(page_title=\"Mapa de Rotas\", page_icon=\"🗺️\", layout=\"wide\")\nst.title(\"🗺️ Mapa de Rotas\")\nst.markdown(\"**Análise de trajetos, frequência de rotas, desvios e picos de velocidade**\")\n\n# Carregar dados\ndf = DatabaseManager.get_dashboard_data()\nif df.empty:\n    st.warning(\"⚠️ Nenhum dado encontrado. Faça o upload de um arquivo CSV primeiro.\")\n    st.stop()\nelse:\n    st.success(f\"✅ Dados carregados: {len(df):,} registros para análise de rotas\")\n\n# Filtros na sidebar\nwith st.sidebar:\n    st.header(\"🔍 Filtros de Análise\")\n    \n    # Filtro de cliente\n    clients = [\"Todos\"] + sorted(df['cliente'].unique().tolist())\n    selected_client = st.selectbox(\"Cliente:\", clients, index=0)\n    \n    # Filtrar veículos baseado no cliente\n    if selected_client != \"Todos\":\n        vehicles_df = df[df['cliente'] == selected_client]\n    else:\n        vehicles_df = df\n    \n    vehicles = [\"Todos\"] + sorted(vehicles_df['placa'].unique().tolist())\n    selected_vehicle = st.selectbox(\"Veículo:\", vehicles, index=0)\n    \n    # Filtro de período\n    st.subheader(\"📅 Período\")\n    col1, col2 = st.columns(2)\n    with col1:\n        start_date = st.date_input(\n            \"Data Início:\",\n            value=datetime.now().date() - timedelta(days=7)\n        )\n    with col2:\n        end_date = st.date_input(\n            \"Data Fim:\",\n            value=datetime.now().date()\n        )\n    \n    # Filtros de análise\n    st.subheader(\"⚡ Parâmetros de Análise\")\n    speed_threshold = st.slider(\n        \"Limite de velocidade para picos (km/h):\",\n        min_value=50,\n        max_value=120,\n        value=80,\n        step=5\n    )\n    \n    deviation_radius = st.slider(\n        \"Raio para detecção de desvios (km):\",\n        min_value=0.5,\n        max_value=5.0,\n        value=2.0,\n        step=0.5\n    )\n\n# Aplicar filtros usando método centralizado do DataAnalyzer\nanalyzer = DataAnalyzer(df)\nfiltered_df = analyzer.apply_filters(\n    cliente=selected_client,\n    placa=selected_vehicle,\n    data_inicio=start_date,\n    data_fim=end_date\n)\n\nif filtered_df.empty:\n    st.warning(\"⚠️ Nenhum dado encontrado para os filtros selecionados.\")\n    \n    # Ajudar o usuário a entender o problema\n    st.info(\"💡 **Dicas para encontrar dados:**\")\n    st.markdown(\"\"\"\n    - ✅ Verifique se o **período de datas** corresponde aos dados disponíveis\n    - ✅ Experimente ampliar o **intervalo de datas** \n    - ✅ Tente selecionar **\"Todos\"** para cliente ou veículo\n    - ✅ Use filtros mais amplos para ver dados disponíveis\n    \"\"\")\n    st.stop()\n\n# Preparar dados para análise de rotas\ndef prepare_route_data(df):\n    \"\"\"Prepara dados para análise de rotas\"\"\"\n    route_df = df.copy()\n    \n    # Garantir colunas necessárias\n    required_cols = ['latitude', 'longitude', 'velocidade_km', 'data', 'placa']\n    available_cols = [col for col in required_cols if col in route_df.columns]\n    \n    if len(available_cols) < 3:\n        return pd.DataFrame()\n    \n    # Remover registros sem coordenadas válidas\n    route_df = route_df.dropna(subset=['latitude', 'longitude'])\n    route_df = route_df[(route_df['latitude'] != 0) & (route_df['longitude'] != 0)]\n    \n    # Converter velocidade para numérico\n    if 'velocidade_km' in route_df.columns:\n        route_df['velocidade_km'] = pd.to_numeric(route_df['velocidade_km'], errors='coerce')\n        route_df['velocidade_km'] = route_df['velocidade_km'].fillna(0)\n    \n    return route_df\n\nroute_data = prepare_route_data(filtered_df)\n\nif route_data.empty:\n    st.warning(\"⚠️ Dados insuficientes para análise de rotas (faltam coordenadas válidas).\")\n    st.stop()\n\n# Tabs principais\ntab1, tab2, tab3, tab4, tab5 = st.tabs([\n    \"🗺️ Mapa Interativo\", \n    \"📊 Análise de Velocidade\", \n    \"🛣️ Rotas Frequentes\",\n    \"⚠️ Desvios Detectados\",\n    \"📈 Padrões Temporais\"\n])\n\nwith tab1:\n    st.header(\"🗺️ Mapa Interativo de Trajetos\")\n    \n    col1, col2 = st.columns([3, 1])\n    \n    with col1:\n        if not route_data.empty and len(route_data) > 0:\n            # Criar mapa Folium\n            center_lat = route_data['latitude'].mean()\n            center_lon = route_data['longitude'].mean()\n            \n            m = folium.Map(\n                location=[center_lat, center_lon],\n                zoom_start=12,\n                tiles='OpenStreetMap'\n            )\n            \n            # Adicionar pontos coloridos por velocidade\n            for idx, row in route_data.iterrows():\n                if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n                    color = 'red' if row.get('velocidade_km', 0) > speed_threshold else 'green'\n                    popup_text = f\"\"\"\n                    Veículo: {row.get('placa', 'N/A')}<br>\n                    Velocidade: {row.get('velocidade_km', 0):.1f} km/h<br>\n                    Data: {row.get('data', 'N/A')}<br>\n                    Coordenadas: {row['latitude']:.6f}, {row['longitude']:.6f}\n                    \"\"\"\n                    \n                    folium.CircleMarker(\n                        location=[row['latitude'], row['longitude']],\n                        radius=3,\n                        popup=popup_text,\n                        color=color,\n                        fill=True,\n                        fillOpacity=0.7\n                    ).add_to(m)\n            \n            # Exibir mapa\n            map_data = st_folium(m, width=700, height=400)\n        else:\n            st.warning(\"Não há dados de coordenadas suficientes para exibir o mapa.\")\n    \n    with col2:\n        st.subheader(\"📊 Resumo da Rota\")\n        \n        if not route_data.empty:\n            total_points = len(route_data)\n            speed_violations = len(route_data[route_data['velocidade_km'] > speed_threshold])\n            avg_speed = route_data['velocidade_km'].mean()\n            max_speed = route_data['velocidade_km'].max()\n            \n            st.metric(\"Total de Pontos\", f\"{total_points:,}\")\n            st.metric(\"Velocidade Média\", f\"{avg_speed:.1f} km/h\")\n            st.metric(\"Velocidade Máxima\", f\"{max_speed:.1f} km/h\")\n            st.metric(\"Picos de Velocidade\", f\"{speed_violations:,}\", \n                     delta=f\"{(speed_violations/total_points*100):.1f}% do total\")\n\nwith tab2:\n    st.header(\"📊 Análise de Velocidade\")\n    \n    if not route_data.empty and 'velocidade_km' in route_data.columns:\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Histograma de velocidades\n            fig_hist = px.histogram(\n                route_data, \n                x='velocidade_km',\n                nbins=30,\n                title=\"Distribuição de Velocidades\",\n                labels={'velocidade_km': 'Velocidade (km/h)', 'count': 'Frequência'}\n            )\n            fig_hist.add_vline(x=speed_threshold, line_dash=\"dash\", line_color=\"red\", \n                              annotation_text=f\"Limite: {speed_threshold} km/h\")\n            st.plotly_chart(fig_hist, use_container_width=True)\n        \n        with col2:\n            # Velocidade ao longo do tempo\n            if 'data' in route_data.columns:\n                route_data_sorted = route_data.sort_values('data')\n                fig_time = px.line(\n                    route_data_sorted,\n                    x='data',\n                    y='velocidade_km',\n                    title=\"Velocidade ao Longo do Tempo\",\n                    labels={'data': 'Data/Hora', 'velocidade_km': 'Velocidade (km/h)'}\n                )\n                fig_time.add_hline(y=speed_threshold, line_dash=\"dash\", line_color=\"red\")\n                st.plotly_chart(fig_time, use_container_width=True)\n        \n        # Tabela de picos de velocidade\n        speed_violations_df = route_data[route_data['velocidade_km'] > speed_threshold]\n        if not speed_violations_df.empty:\n            st.subheader(\"⚠️ Picos de Velocidade Detectados\")\n            speed_violations_display = speed_violations_df[['data', 'placa', 'velocidade_km', 'latitude', 'longitude']].copy()\n            speed_violations_display = speed_violations_display.sort_values('velocidade_km', ascending=False)\n            st.dataframe(speed_violations_display.head(20), use_container_width=True)\n\nwith tab3:\n    st.header(\"🛣️ Análise de Rotas Frequentes\")\n    \n    if not route_data.empty:\n        # Agrupar pontos próximos para identificar rotas frequentes\n        st.subheader(\"📍 Pontos Mais Visitados\")\n        \n        # Criar grid de coordenadas para agrupar pontos próximos\n        precision = 0.01  # Precisão do grid (aproximadamente 1km)\n        route_data['lat_grid'] = (route_data['latitude'] / precision).round() * precision\n        route_data['lon_grid'] = (route_data['longitude'] / precision).round() * precision\n        \n        # Contar frequência de cada ponto\n        point_frequency = route_data.groupby(['lat_grid', 'lon_grid']).size().reset_index(name='frequencia')\n        point_frequency = point_frequency.sort_values('frequencia', ascending=False)\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Top 10 pontos mais frequentes\n            top_points = point_frequency.head(10)\n            fig_freq = px.bar(\n                top_points,\n                x='frequencia',\n                y=range(len(top_points)),\n                orientation='h',\n                title=\"Top 10 Locais Mais Visitados\",\n                labels={'frequencia': 'Número de Visitas', 'y': 'Ranking'}\n            )\n            st.plotly_chart(fig_freq, use_container_width=True)\n        \n        with col2:\n            # Análise por horário\n            if 'data' in route_data.columns:\n                route_data['hora'] = route_data['data'].dt.hour\n                hourly_activity = route_data.groupby('hora').size().reset_index(name='atividade')\n                \n                fig_hourly = px.bar(\n                    hourly_activity,\n                    x='hora',\n                    y='atividade',\n                    title=\"Atividade por Horário do Dia\",\n                    labels={'hora': 'Hora do Dia', 'atividade': 'Número de Registros'}\n                )\n                st.plotly_chart(fig_hourly, use_container_width=True)\n\nwith tab4:\n    st.header(\"⚠️ Análise de Desvios de Rota\")\n    \n    if not route_data.empty:\n        st.subheader(\"🔍 Detecção de Desvios\")\n        \n        # Calcular centro geográfico das rotas\n        center_lat = route_data['latitude'].mean()\n        center_lon = route_data['longitude'].mean()\n        \n        # Calcular distância do centro para cada ponto\n        def haversine_distance(lat1, lon1, lat2, lon2):\n            from math import radians, cos, sin, asin, sqrt\n            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n            dlon = lon2 - lon1\n            dlat = lat2 - lat1\n            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n            return 2 * asin(sqrt(a)) * 6371  # Raio da Terra em km\n        \n        route_data['distancia_centro'] = route_data.apply(\n            lambda row: haversine_distance(center_lat, center_lon, row['latitude'], row['longitude']),\n            axis=1\n        )\n        \n        # Identificar desvios (pontos muito distantes do centro)\n        desvios = route_data[route_data['distancia_centro'] > deviation_radius]\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.metric(\"Total de Desvios Detectados\", len(desvios))\n            st.metric(\"Raio de Análise\", f\"{deviation_radius} km\")\n            \n            if not desvios.empty:\n                st.subheader(\"📋 Lista de Desvios\")\n                desvios_display = desvios[['data', 'placa', 'distancia_centro', 'velocidade_km']].copy()\n                desvios_display['distancia_centro'] = desvios_display['distancia_centro'].round(2)\n                st.dataframe(desvios_display.head(10), use_container_width=True)\n        \n        with col2:\n            if not route_data.empty:\n                # Gráfico de dispersão das distâncias\n                fig_scatter = px.scatter(\n                    route_data,\n                    x='distancia_centro',\n                    y='velocidade_km',\n                    color='placa' if 'placa' in route_data.columns else None,\n                    title=\"Desvios vs Velocidade\",\n                    labels={'distancia_centro': 'Distância do Centro (km)', 'velocidade_km': 'Velocidade (km/h)'}\n                )\n                fig_scatter.add_vline(x=deviation_radius, line_dash=\"dash\", line_color=\"red\",\n                                    annotation_text=f\"Limite: {deviation_radius} km\")\n                st.plotly_chart(fig_scatter, use_container_width=True)\n\nwith tab5:\n    st.header(\"📈 Padrões Temporais de Movimento\")\n    \n    if not route_data.empty and 'data' in route_data.columns:\n        # Análise por dia da semana\n        route_data['dia_semana'] = route_data['data'].dt.day_name()\n        route_data['dia_semana_num'] = route_data['data'].dt.dayofweek\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Atividade por dia da semana\n            daily_activity = route_data.groupby('dia_semana').size().reset_index(name='atividade')\n            # Ordenar por dia da semana\n            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n            daily_activity['dia_semana'] = pd.Categorical(daily_activity['dia_semana'], categories=day_order, ordered=True)\n            daily_activity = daily_activity.sort_values('dia_semana')\n            \n            fig_daily = px.bar(\n                daily_activity,\n                x='dia_semana',\n                y='atividade',\n                title=\"Atividade por Dia da Semana\",\n                labels={'dia_semana': 'Dia da Semana', 'atividade': 'Número de Registros'}\n            )\n            st.plotly_chart(fig_daily, use_container_width=True)\n        \n        with col2:\n            # Heatmap de atividade por hora e dia\n            route_data['hora'] = route_data['data'].dt.hour\n            heatmap_data = route_data.groupby(['dia_semana_num', 'hora']).size().reset_index(name='atividade')\n            \n            # Criar pivot table para heatmap\n            pivot_data = heatmap_data.pivot(index='dia_semana_num', columns='hora', values='atividade').fillna(0)\n            \n            fig_heatmap = px.imshow(\n                pivot_data,\n                labels=dict(x=\"Hora do Dia\", y=\"Dia da Semana\", color=\"Atividade\"),\n                title=\"Padrão de Atividade (Heatmap)\",\n                aspect=\"auto\"\n            )\n            st.plotly_chart(fig_heatmap, use_container_width=True)\n        \n        # Análise de velocidade média por período\n        st.subheader(\"🚗 Velocidade Média por Período\")\n        route_data['periodo'] = route_data['hora'].apply(\n            lambda x: 'Madrugada (00-06)' if x < 6 \n            else 'Manhã (06-12)' if x < 12 \n            else 'Tarde (12-18)' if x < 18 \n            else 'Noite (18-24)'\n        )\n        \n        period_stats = route_data.groupby('periodo')['velocidade_km'].agg(['mean', 'max', 'count']).reset_index()\n        period_stats.columns = ['periodo', 'velocidade_media', 'velocidade_maxima', 'total_registros']\n        \n        fig_period = px.bar(\n            period_stats,\n            x='periodo',\n            y='velocidade_media',\n            title=\"Velocidade Média por Período do Dia\",\n            labels={'periodo': 'Período', 'velocidade_media': 'Velocidade Média (km/h)'}\n        )\n        st.plotly_chart(fig_period, use_container_width=True)\n        \n        # Tabela resumo\n        st.subheader(\"📊 Resumo Estatístico por Período\")\n        st.dataframe(period_stats, use_container_width=True)\n\n# Footer com informações técnicas\nst.markdown(\"---\")\nst.markdown(\"\"\"\n<div style='text-align: center; color: gray; font-size: 0.8em;'>\n    <p>🗺️ Mapa de Rotas - Sistema de Análise de Trajetos</p>\n    <p>Dados processados: {total_records:,} registros | Última atualização: {timestamp}</p>\n</div>\n\"\"\".format(\n    total_records=len(route_data),\n    timestamp=datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n), unsafe_allow_html=True)","size_bytes":16674}},"version":1}