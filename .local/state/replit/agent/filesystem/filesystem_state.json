{"file_contents":{"app.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom database.db_manager import DatabaseManager\n\n# Configura√ß√£o da p√°gina\nst.set_page_config(\n    page_title=\"Insight Hub - Monitoramento de Frotas\",\n    page_icon=\"üöõ\",\n    layout=\"wide\",\n    initial_sidebar_state=\"expanded\"\n)\n\n# CSS customizado para melhor apar√™ncia\nst.markdown(\"\"\"\n    <style>\n    .main-header {\n        font-size: 2.5rem;\n        font-weight: bold;\n        color: #1f77b4;\n        text-align: center;\n        margin-bottom: 2rem;\n    }\n    .metric-card {\n        background-color: #f0f2f6;\n        padding: 1rem;\n        border-radius: 0.5rem;\n        margin: 0.5rem 0;\n    }\n    </style>\n\"\"\", unsafe_allow_html=True)\n\ndef load_processed_data():\n    \"\"\"Carrega dados da base PostgreSQL\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty:\n            st.success(f\"‚úÖ Dados carregados: {len(df):,} registros da base PostgreSQL\")\n            return df\n        return pd.DataFrame()\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados da base: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    # Header principal\n    st.markdown('<h1 class=\"main-header\">üöõ Insight Hub</h1>', unsafe_allow_html=True)\n    st.markdown('<p style=\"text-align: center; font-size: 1.2rem; color: #666;\">Plataforma de Monitoramento e An√°lise de Frotas Municipais</p>', unsafe_allow_html=True)\n    \n    # Verificar se h√° dados processados\n    df = load_processed_data()\n    \n    if df.empty:\n        st.warning(\"üìÅ Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV na p√°gina 'Upload CSV' para come√ßar.\")\n        \n        # Informa√ß√µes sobre o sistema\n        st.markdown(\"---\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"\"\"\n            ### üéØ **Funcionalidades Principais**\n            \n            - **üìä An√°lise Profissional**: KPIs em tempo real  \n            - **üìÅ Upload de CSV**: Processamento de dados telem√°ticos\n            - **üîç An√°lise Detalhada**: Filtros avan√ßados por cliente e per√≠odo\n            - **üìà Compara√ß√£o de Ve√≠culos**: An√°lise comparativa de performance\n            - **üîß Manuten√ß√£o Preditiva**: Alertas e previs√µes\n            - **üö® Alertas**: Monitoramento em tempo real\n            - **üß† Insights Autom√°ticos**: Gera√ß√£o inteligente de relat√≥rios\n            - **üìÑ Relat√≥rios Avan√ßados**: Documenta√ß√£o completa\n            - **üö® Controle Operacional**: Conformidade municipal\n            \"\"\")\n        \n        with col2:\n            st.markdown(\"\"\"\n            ### üìã **Campos Obrigat√≥rios do CSV**\n            \n            - Cliente, Placa, Ativo, Data, Data (GPRS)\n            - Velocidade (Km/h), Igni√ß√£o, Motorista\n            - GPS, GPRS, Localiza√ß√£o, Endere√ßo\n            - Tipo do Evento, Cerca, Sa√≠da, Entrada\n            - Pacote, Od√¥metro do per√≠odo (Km)\n            - Hor√≠metro do per√≠odo, Hor√≠metro embarcado\n            - Od√¥metro embarcado (Km), Bateria (%)\n            - Imagem, Tens√£o (V), Bloqueado\n            - Latitude, Longitude (coordenadas GPS)\n            \"\"\")\n        \n        st.markdown(\"---\")\n        st.info(\"üí° **Dica**: Comece fazendo o upload de um arquivo CSV com dados de frota na aba lateral.\")\n        \n    else:\n        # Mostrar resumo dos dados carregados\n        st.success(f\"‚úÖ Dados carregados: {len(df):,} registros processados\")\n        \n        # M√©tricas gerais\n        st.markdown(\"### üìà **Vis√£o Geral dos Dados**\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            total_vehicles = df['placa'].nunique()\n            st.metric(\"üöó Total de Ve√≠culos\", f\"{total_vehicles:,}\")\n        \n        with col2:\n            total_clients = df['cliente'].nunique()\n            st.metric(\"üè¢ Total de Clientes\", f\"{total_clients:,}\")\n        \n        with col3:\n            date_range = df['data'].max() - df['data'].min()\n            st.metric(\"üìÖ Per√≠odo dos Dados\", f\"{date_range.days} dias\")\n        \n        with col4:\n            total_records = len(df)\n            st.metric(\"üìä Total de Registros\", f\"{total_records:,}\")\n        \n        # Gr√°fico de distribui√ß√£o temporal\n        st.markdown(\"### üìä **Distribui√ß√£o Temporal dos Dados**\")\n        \n        # Agrupar por data\n        daily_data = df.groupby(df['data'].dt.date).size().reset_index()\n        daily_data.columns = ['data', 'registros']\n        \n        fig = px.line(\n            daily_data, \n            x='data', \n            y='registros',\n            title='Registros por Data',\n            labels={'data': 'Data', 'registros': 'N√∫mero de Registros'}\n        )\n        fig.update_layout(height=400)\n        st.plotly_chart(fig, use_container_width=True)\n        \n        # Top ve√≠culos por atividade\n        st.markdown(\"### üöó **Ve√≠culos Mais Ativos**\")\n        \n        vehicle_activity = df.groupby('placa').size().sort_values(ascending=False).head(10)\n        \n        fig_bar = px.bar(\n            x=vehicle_activity.values,\n            y=vehicle_activity.index,\n            orientation='h',\n            title='Top 10 Ve√≠culos por N√∫mero de Registros',\n            labels={'x': 'N√∫mero de Registros', 'y': 'Placa'}\n        )\n        fig_bar.update_layout(height=400)\n        st.plotly_chart(fig_bar, use_container_width=True)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":5578},"debug_csv.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nDebug script to analyze the CSV file and column mapping issues\n\"\"\"\nimport pandas as pd\nimport sys\nsys.path.append('.')\n\ndef main():\n    print(\"üîç Debugging CSV column mapping and data issues...\")\n    \n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    # Read the CSV with different separators\n    print(\"\\nüìÑ Testing CSV reading with different separators...\")\n    \n    separators = [',', ';']\n    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n    \n    df = None\n    used_sep = None\n    used_enc = None\n    \n    for sep in separators:\n        for enc in encodings:\n            try:\n                df = pd.read_csv(csv_file, sep=sep, encoding=enc)\n                used_sep = sep\n                used_enc = enc\n                print(f\"‚úÖ Successfully read with separator='{sep}' and encoding='{enc}'\")\n                break\n            except Exception as e:\n                print(f\"‚ùå Failed with separator='{sep}' and encoding='{enc}': {str(e)[:100]}\")\n                continue\n        if df is not None:\n            break\n    \n    if df is None:\n        print(\"‚ùå Could not read CSV file with any combination\")\n        return\n    \n    print(f\"\\nüìä File info:\")\n    print(f\"Shape: {df.shape}\")\n    print(f\"Used separator: '{used_sep}'\")\n    print(f\"Used encoding: '{used_enc}'\")\n    \n    print(f\"\\nüìã Original columns:\")\n    for i, col in enumerate(df.columns):\n        print(f\"{i:2d}: '{col}' (len={len(col)})\")\n    \n    # Test normalization\n    print(f\"\\nüîß Testing column normalization:\")\n    normalized_cols = [' '.join(col.strip().split()) for col in df.columns]\n    for i, (orig, norm) in enumerate(zip(df.columns, normalized_cols)):\n        if orig != norm:\n            print(f\"{i:2d}: '{orig}' -> '{norm}'\")\n    \n    # Test first few rows of key columns\n    print(f\"\\nüìù Sample data (first 3 rows):\")\n    key_cols = ['Cliente', 'Placa', 'Data']\n    for col in key_cols:\n        if col in df.columns:\n            print(f\"{col}: {df[col].head(3).tolist()}\")\n        else:\n            print(f\"‚ùå Column '{col}' not found\")\n    \n    # Check unique values in key columns\n    print(f\"\\nüî¢ Unique value counts:\")\n    if 'Cliente' in df.columns:\n        print(f\"Unique clients: {df['Cliente'].nunique()}\")\n        print(f\"Client values: {df['Cliente'].unique()[:5]}\")\n    \n    if 'Placa' in df.columns:\n        print(f\"Unique plates: {df['Placa'].nunique()}\")\n        print(f\"Plate values: {df['Placa'].unique()[:5]}\")\n    \n    # Test date parsing\n    print(f\"\\nüìÖ Testing date parsing:\")\n    if 'Data' in df.columns:\n        date_sample = df['Data'].head(3)\n        print(f\"Original dates: {date_sample.tolist()}\")\n        \n        # Test without dayfirst\n        parsed_normal = pd.to_datetime(date_sample, errors='coerce')\n        print(f\"Parsed (normal): {parsed_normal.tolist()}\")\n        \n        # Test with dayfirst\n        parsed_dayfirst = pd.to_datetime(date_sample, errors='coerce', dayfirst=True)\n        print(f\"Parsed (dayfirst): {parsed_dayfirst.tolist()}\")\n        \n        # Check for NaT values\n        nat_count = parsed_dayfirst.isna().sum()\n        print(f\"NaT values with dayfirst=True: {nat_count}\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":3303},"pyproject.toml":{"content":"[project]\nname = \"repl-nix-workspace\"\nversion = \"0.1.0\"\ndescription = \"Add your description here\"\nrequires-python = \">=3.11\"\ndependencies = [\n    \"fpdf2>=2.8.4\",\n    \"numpy>=2.3.3\",\n    \"pandas>=2.3.2\",\n    \"plotly>=6.3.0\",\n    \"psycopg2-binary>=2.9.10\",\n    \"reportlab>=4.4.3\",\n    \"scikit-learn>=1.7.2\",\n    \"sqlalchemy>=2.0.43\",\n    \"streamlit-folium>=0.25.1\",\n    \"streamlit>=1.49.1\",\n    \"folium>=0.20.0\",\n]\n","size_bytes":413},"replit.md":{"content":"# Insight Hub - Fleet Monitoring System\n\n## Overview\n\nInsight Hub is a comprehensive fleet monitoring and analysis platform for municipal vehicles that processes real-time telematics data to provide intelligent insights about vehicle performance, compliance, and operational efficiency. The system processes CSV data uploads containing vehicle telemetry information and generates interactive dashboards, automated insights, and comparative analytics for fleet management.\n\n## User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n## System Architecture\n\n### Frontend Architecture\n- **Framework**: Streamlit-based multi-page web application\n- **Structure**: Page-based navigation with modular components\n- **Pages**: Dashboard, CSV Upload, Detailed Analysis, Vehicle Comparison, Automatic Insights\n- **Visualization**: Plotly for interactive charts and graphs\n- **Styling**: Custom CSS with responsive design\n\n### Data Processing Pipeline\n- **CSV Processor**: Handles upload and validation of telematics data files\n- **Field Validation**: 25 required fields including vehicle data, GPS coordinates, driver info, and operational metrics\n- **Data Storage**: Local CSV file storage in `/data/processed_data.csv`\n- **Analysis Engine**: Real-time data analysis with filtering capabilities\n\n### Core Components\n- **DataAnalyzer**: Central class for data filtering, KPI calculations, and statistical analysis\n- **InsightsGenerator**: Automated insight generation for performance, compliance, efficiency, and predictive analytics\n- **FleetVisualizations**: Chart and graph generation using Plotly\n- **CSVProcessor**: File upload validation and data structure verification\n\n### Data Model\n- **Vehicle Data**: License plate, asset ID, client information\n- **Telemetry**: GPS coordinates, speed, ignition status, odometer readings\n- **Operational**: Driver information, event types, battery levels, system status\n- **Temporal**: Date/time stamps for GPS and GPRS communications\n\n### Analytics Features\n- **KPI Dashboard**: Real-time metrics for fleet performance\n- **Filtering System**: Multi-dimensional filtering by client, vehicle, date range\n- **Comparative Analysis**: Vehicle-to-vehicle performance comparison\n- **Automated Insights**: Pattern recognition and recommendation generation\n- **Compliance Monitoring**: Speed limits, operational hours, GPS coverage analysis\n\n## External Dependencies\n\n### Core Libraries\n- **streamlit**: Web application framework\n- **pandas**: Data manipulation and analysis\n- **plotly**: Interactive visualization library\n- **numpy**: Numerical computing support\n\n### Data Processing\n- **datetime**: Date and time handling for temporal analysis\n- **os**: File system operations for data storage\n- **re**: Regular expressions for data validation\n\n### File Storage\n- **Local CSV Storage**: Processed data stored in local filesystem\n- **Upload Directory**: Temporary file handling for CSV uploads\n- **Data Persistence**: File-based data storage without external database\n\nNote: The system currently uses local file storage but could be extended to integrate with PostgreSQL or other database systems for enhanced data persistence and multi-user support.","size_bytes":3197},"step_by_step_migration.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nStep-by-step migration test to verify each component works\n\"\"\"\nimport pandas as pd\nimport sys\nsys.path.append('.')\n\nfrom database.db_manager import DatabaseManager\n\ndef test_step_by_step():\n    print(\"üîß Step-by-step migration test...\")\n    \n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    # Step 1: Test CSV reading\n    print(\"\\nüìÅ Step 1: Testing CSV reading...\")\n    try:\n        df = None\n        separators = [',', ';']\n        encodings = ['latin-1', 'utf-8', 'iso-8859-1', 'windows-1252', 'cp1252']\n        \n        for sep in separators:\n            for enc in encodings:\n                try:\n                    test_df = pd.read_csv(csv_file, sep=sep, encoding=enc)\n                    if test_df.shape[1] > 1:  # Multiple columns\n                        df = test_df\n                        print(f\"‚úÖ CSV read successfully: sep='{sep}', enc='{enc}', shape={df.shape}\")\n                        break\n                except:\n                    continue\n            if df is not None:\n                break\n        \n        if df is None:\n            print(\"‚ùå Could not read CSV\")\n            return\n            \n    except Exception as e:\n        print(f\"‚ùå CSV reading failed: {str(e)}\")\n        return\n    \n    # Step 2: Test column normalization\n    print(f\"\\nüîß Step 2: Testing column normalization...\")\n    try:\n        print(f\"Original columns: {list(df.columns[:5])}\")\n        df.columns = [' '.join(col.strip().split()) for col in df.columns]\n        print(f\"Normalized columns: {list(df.columns[:5])}\")\n        print(f\"‚úÖ Column normalization successful\")\n    except Exception as e:\n        print(f\"‚ùå Column normalization failed: {str(e)}\")\n        return\n    \n    # Step 3: Test small sample migration\n    print(f\"\\nüß™ Step 3: Testing small sample migration (first 5 rows)...\")\n    try:\n        small_df = df.head(5).copy()\n        result = DatabaseManager.migrate_csv_to_database_from_df(small_df, \"test_sample.csv\")\n        print(f\"Sample migration result: {result}\")\n        \n        if result.get('success'):\n            print(f\"‚úÖ Sample migration successful: {result.get('records_processed', 0)} records processed\")\n        else:\n            print(f\"‚ùå Sample migration failed: {result.get('error', 'Unknown error')}\")\n            return\n    except Exception as e:\n        print(f\"‚ùå Sample migration failed: {str(e)}\")\n        return\n    \n    # Step 4: Test full migration if sample worked\n    print(f\"\\nüöÄ Step 4: Testing full migration ({df.shape[0]} rows)...\")\n    try:\n        result = DatabaseManager.migrate_csv_to_database_from_df(df, \"full_migration.csv\")\n        print(f\"Full migration result: {result}\")\n        \n        if result.get('success'):\n            print(f\"‚úÖ Full migration successful!\")\n            print(f\"   Records processed: {result.get('records_processed', 0):,}\")\n            print(f\"   Unique vehicles: {result.get('unique_vehicles', 0):,}\")\n            print(f\"   Unique clients: {result.get('unique_clients', 0):,}\")\n        else:\n            print(f\"‚ùå Full migration failed: {result.get('error', 'Unknown error')}\")\n    except Exception as e:\n        print(f\"‚ùå Full migration failed: {str(e)}\")\n\nif __name__ == \"__main__\":\n    test_step_by_step()","size_bytes":3328},"test_csv_reading.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nTest CSV reading with proper separator detection\n\"\"\"\nimport pandas as pd\n\ndef test_csv_reading():\n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    print(\"üß™ Testing CSV reading with proper separator detection...\")\n    \n    separators = [',', ';']\n    encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n    \n    for sep in separators:\n        for enc in encodings:\n            try:\n                df = pd.read_csv(csv_file, sep=sep, encoding=enc)\n                print(f\"\\n‚úÖ Separator='{sep}', Encoding='{enc}':\")\n                print(f\"   Shape: {df.shape}\")\n                print(f\"   Columns: {len(df.columns)}\")\n                if df.shape[1] > 1:\n                    print(f\"   First 3 column names: {list(df.columns[:3])}\")\n                    print(f\"   Sample row 0: {dict(list(df.iloc[0].items())[:3])}\")\n                    return df, sep, enc\n                else:\n                    print(f\"   ‚ö†Ô∏è  Only {df.shape[1]} column - wrong separator\")\n            except Exception as e:\n                print(f\"‚ùå Separator='{sep}', Encoding='{enc}': {str(e)[:80]}\")\n    \n    return None, None, None\n\nif __name__ == \"__main__\":\n    df, sep, enc = test_csv_reading()\n    if df is not None:\n        print(f\"\\nüéâ Successfully found correct separator: '{sep}' with encoding: '{enc}'\")\n    else:\n        print(f\"\\n‚ùå No valid combination found\")","size_bytes":1476},"test_migration.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nTest script to verify the database migration fixes\n\"\"\"\nimport os\nimport sys\nfrom datetime import datetime\n\n# Add current directory to path\nsys.path.append('.')\n\nfrom database.db_manager import DatabaseManager\nfrom database.services import FleetDatabaseService\n\ndef main():\n    print(\"üß™ Testing database migration with user's CSV file...\")\n    print(f\"Timestamp: {datetime.now()}\")\n    \n    # Test file path\n    csv_file = 'attached_assets/relatorio_historico_de_posicoes-tfe-6d41_05-09-2025_06_47_1757817200636.csv'\n    \n    if not os.path.exists(csv_file):\n        print(f\"‚ùå CSV file not found: {csv_file}\")\n        return\n    \n    print(f\"üìÅ Found CSV file: {csv_file}\")\n    \n    # Check file size\n    file_size = os.path.getsize(csv_file)\n    print(f\"üìè File size: {file_size:,} bytes\")\n    \n    # Run migration\n    print(\"\\nüöÄ Starting migration...\")\n    result = DatabaseManager.migrate_csv_to_database(csv_file)\n    \n    print(f\"\\nüìä Migration Result:\")\n    print(f\"Success: {result.get('success', False)}\")\n    if result.get('success'):\n        print(f\"Records processed: {result.get('records_processed', 0):,}\")\n        print(f\"Unique vehicles: {result.get('unique_vehicles', 0):,}\")\n        print(f\"Unique clients: {result.get('unique_clients', 0):,}\")\n    else:\n        print(f\"Error: {result.get('error', 'Unknown error')}\")\n    \n    # Verify database state\n    print(\"\\nüîç Verifying database state...\")\n    try:\n        with FleetDatabaseService() as db:\n            summary = db.get_fleet_summary()\n            print(f\"Total clients: {summary['total_clients']:,}\")\n            print(f\"Total vehicles: {summary['total_vehicles']:,}\")\n            print(f\"Total telematics records: {summary['total_records']:,}\")\n            print(f\"Date range: {summary['start_date']} to {summary['end_date']}\")\n            print(f\"Average speed: {summary['avg_speed']} km/h\")\n            print(f\"GPS coverage: {summary['gps_coverage']}%\")\n    except Exception as e:\n        print(f\"‚ùå Error getting database summary: {str(e)}\")\n    \n    print(\"\\n‚úÖ Migration test complete!\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":2153},"verify_dashboard_data.py":{"content":"#!/usr/bin/env python3\n\"\"\"\nVerify dashboard components work with migrated data\n\"\"\"\nimport sys\nsys.path.append('.')\n\nfrom database.db_manager import DatabaseManager\nfrom database.services import FleetDatabaseService\nfrom utils.data_analyzer import DataAnalyzer\n\ndef verify_dashboard_data():\n    print(\"üîç Verifying dashboard data components...\")\n    \n    # Test 1: Check DatabaseManager.get_dashboard_data()\n    print(\"\\nüìä Test 1: DatabaseManager.get_dashboard_data()\")\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        print(f\"‚úÖ Dashboard data loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n        \n        if df.shape[0] > 0:\n            print(f\"   Date range: {df['data'].min()} to {df['data'].max()}\")\n            print(f\"   Unique clients: {df['cliente'].nunique()}\")\n            print(f\"   Unique vehicles: {df['placa'].nunique()}\")\n            print(f\"   Sample data: {df[['cliente', 'placa', 'velocidade_km']].head(2).to_dict('records')}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Dashboard data loading failed: {str(e)}\")\n        return False\n    \n    # Test 2: Check if DataAnalyzer can load from database\n    print(\"\\nüßÆ Test 2: DataAnalyzer.from_database()\")\n    try:\n        analyzer = DataAnalyzer.from_database()\n        if analyzer.data is not None and not analyzer.data.empty:\n            print(f\"‚úÖ DataAnalyzer loaded: {analyzer.data.shape[0]:,} rows\")\n            \n            # Test KPIs calculation\n            kpis = analyzer.get_kpis()\n            print(f\"   KPIs: {kpis['total_veiculos']} vehicles, {kpis['velocidade_media']:.1f} avg speed\")\n        else:\n            print(f\"‚ùå DataAnalyzer data is empty\")\n            return False\n            \n    except Exception as e:\n        print(f\"‚ùå DataAnalyzer loading failed: {str(e)}\")\n        return False\n    \n    # Test 3: Check FleetDatabaseService summary\n    print(\"\\nüìà Test 3: FleetDatabaseService.get_fleet_summary()\")\n    try:\n        with FleetDatabaseService() as db:\n            summary = db.get_fleet_summary()\n            print(f\"‚úÖ Fleet summary: {summary['total_records']:,} records\")\n            print(f\"   Vehicles: {summary['total_vehicles']}, Clients: {summary['total_clients']}\")\n            print(f\"   Speed stats: {summary['avg_speed']} avg, {summary['max_speed']} max\")\n            print(f\"   GPS coverage: {summary['gps_coverage']}%\")\n            \n    except Exception as e:\n        print(f\"‚ùå Fleet summary failed: {str(e)}\")\n        return False\n    \n    # Test 4: Test data filtering\n    print(\"\\nüîç Test 4: Data filtering functionality\")\n    try:\n        # Test with client filter\n        filtered_df = DatabaseManager.get_dashboard_data(client_filter=\"JANDAIA\")\n        print(f\"‚úÖ Client filter works: {filtered_df.shape[0]:,} records for JANDAIA\")\n        \n        # Test with vehicle filter\n        filtered_df = DatabaseManager.get_dashboard_data(vehicle_filter=\"TFE-6D41\")\n        print(f\"‚úÖ Vehicle filter works: {filtered_df.shape[0]:,} records for TFE-6D41\")\n        \n    except Exception as e:\n        print(f\"‚ùå Data filtering failed: {str(e)}\")\n        return False\n    \n    print(\"\\nüéâ Dashboard verification complete - All components working correctly!\")\n    return True\n\nif __name__ == \"__main__\":\n    success = verify_dashboard_data()\n    if success:\n        print(\"\\n‚úÖ DASHBOARD READY: All migrated data is accessible and dashboard components work correctly\")\n    else:\n        print(\"\\n‚ùå DASHBOARD ISSUES: Some components need attention\")","size_bytes":3522},"database/__init__.py":{"content":"# Database package initialization","size_bytes":33},"database/connection.py":{"content":"\"\"\"\nDatabase connection and session management\n\"\"\"\nimport os\nimport time\nfrom sqlalchemy import create_engine, text\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy.exc import DisconnectionError, OperationalError\n\n# Database connection URL from environment\nDATABASE_URL = os.getenv('DATABASE_URL')\n\n# Global variables for lazy initialization\nengine = None\nSessionLocal = None\nBase = declarative_base()\n\ndef initialize_database():\n    \"\"\"Initialize database connection lazily with robust SSL handling\"\"\"\n    global engine, SessionLocal\n    \n    if engine is not None:\n        try:\n            # Test existing connection\n            with engine.connect() as conn:\n                conn.execute(text(\"SELECT 1\"))\n            return True\n        except (DisconnectionError, OperationalError):\n            # Force recreation of engine\n            engine = None\n    \n    if not DATABASE_URL:\n        return False\n    \n    try:\n        # Create engine with robust SSL and pool settings\n        engine = create_engine(\n            DATABASE_URL,\n            pool_size=10,\n            max_overflow=20,\n            pool_pre_ping=True,  # Validates connections before use\n            pool_recycle=300,    # Recycle connections every 5 minutes\n            connect_args={\n                \"sslmode\": \"require\",\n                \"connect_timeout\": 10,\n                \"application_name\": \"insight_hub_fleet_monitor\"\n            }\n        )\n        \n        SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n        \n        # Create tables if they don't exist\n        Base.metadata.create_all(bind=engine)\n        return True\n    except Exception:\n        return False\n\ndef get_db_session():\n    \"\"\"Get database session with retry logic\"\"\"\n    max_retries = 3\n    retry_delay = 1\n    \n    for attempt in range(max_retries):\n        session = None\n        try:\n            # Ensure database is initialized\n            if not initialize_database():\n                raise Exception(\"Failed to initialize database\")\n            \n            session = SessionLocal()\n            \n            # Test the connection\n            session.execute(text(\"SELECT 1\"))\n            return session\n            \n        except (DisconnectionError, OperationalError) as e:\n            if session:\n                session.close()\n            \n            if attempt < max_retries - 1:\n                time.sleep(retry_delay)\n                retry_delay *= 2  # Exponential backoff\n                \n                # Force engine recreation on connection errors\n                global engine\n                engine = None\n                continue\n            else:\n                raise e\n        except Exception as e:\n            if session:\n                session.close()\n            raise e\n\ndef close_db_session(session):\n    \"\"\"Close database session safely\"\"\"\n    if session:\n        try:\n            session.close()\n        except Exception:\n            pass  # Ignore errors when closing\n\ndef test_connection():\n    \"\"\"Test database connection health\"\"\"\n    try:\n        with get_db_session() as session:\n            session.execute(text(\"SELECT 1\"))\n        return True\n    except Exception:\n        return False","size_bytes":3274},"database/db_manager.py":{"content":"\"\"\"\nDatabase manager for integrating with existing CSV processing workflow\n\"\"\"\nimport os\nimport pandas as pd\nfrom typing import Optional, Dict, Any, List\nfrom datetime import datetime\nfrom database.services import FleetDatabaseService\nfrom database.connection import initialize_database\n\nclass DatabaseManager:\n    \"\"\"Manager to integrate database operations with existing workflow\"\"\"\n    \n    @staticmethod\n    def _safe_int_convert(value):\n        \"\"\"Safely convert value to int, handling special cases\"\"\"\n        if pd.isna(value) or value is None or str(value).strip() == '':\n            return 0\n        try:\n            # Handle special values like 'X, X, X, X'\n            val_str = str(value).strip()\n            if 'x' in val_str.lower() or ',' in val_str:\n                return 0\n            return int(float(val_str))  # float first to handle '1.0'\n        except (ValueError, TypeError):\n            return 0\n    \n    @staticmethod\n    def _safe_float_convert(value):\n        \"\"\"Safely convert value to float, handling special cases\"\"\"\n        if pd.isna(value) or value is None or str(value).strip() == '':\n            return 0.0\n        try:\n            # Handle special values like 'X, X, X, X'\n            val_str = str(value).strip()\n            if 'x' in val_str.lower() or val_str == '-':\n                return 0.0\n            # Remove any non-numeric characters except . and -\n            val_str = ''.join(c for c in val_str if c.isdigit() or c in '.-')\n            if not val_str or val_str == '.' or val_str == '-':\n                return 0.0\n            return float(val_str)\n        except (ValueError, TypeError):\n            return 0.0\n    \n    @staticmethod\n    def migrate_csv_to_database_with_progress(csv_file_path: str, progress_callback=None) -> Dict[str, Any]:\n        \"\"\"Migrate existing CSV data to database with progress callback\"\"\"\n        if not os.path.exists(csv_file_path):\n            return {'success': False, 'error': f'File not found: {csv_file_path}'}\n        \n        try:\n            # Try different separators and encodings with proper validation\n            df = None\n            separators = [',', ';']\n            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for sep in separators:\n                for enc in encodings:\n                    try:\n                        test_df = pd.read_csv(csv_file_path, sep=sep, encoding=enc)\n                        # Validate that we have multiple columns (not just one big column)\n                        if test_df.shape[1] > 1:\n                            df = test_df\n                            print(f\"Successfully read CSV with separator='{sep}' and encoding='{enc}' - Shape: {df.shape}\")\n                            break\n                        else:\n                            print(f\"Separator '{sep}' with encoding '{enc}' resulted in only {test_df.shape[1]} column(s) - trying next combination\")\n                    except Exception as e:\n                        print(f\"Failed with separator='{sep}' and encoding='{enc}': {str(e)[:100]}\")\n                        continue\n                if df is not None:\n                    break\n            \n            if df is None:\n                return {'success': False, 'error': 'Could not read CSV file with any encoding/separator combination that produces multiple columns'}\n            \n            return DatabaseManager.migrate_csv_to_database_from_df_with_progress(df, os.path.basename(csv_file_path), progress_callback)\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def migrate_csv_to_database(csv_file_path: str) -> Dict[str, Any]:\n        \"\"\"Migrate existing CSV data to database\"\"\"\n        if not os.path.exists(csv_file_path):\n            return {'success': False, 'error': f'File not found: {csv_file_path}'}\n        \n        try:\n            # Try different separators and encodings with proper validation\n            df = None\n            separators = [',', ';']\n            encodings = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for sep in separators:\n                for enc in encodings:\n                    try:\n                        test_df = pd.read_csv(csv_file_path, sep=sep, encoding=enc)\n                        # Validate that we have multiple columns (not just one big column)\n                        if test_df.shape[1] > 1:\n                            df = test_df\n                            print(f\"Successfully read CSV with separator='{sep}' and encoding='{enc}' - Shape: {df.shape}\")\n                            break\n                        else:\n                            print(f\"Separator '{sep}' with encoding '{enc}' resulted in only {test_df.shape[1]} column(s) - trying next combination\")\n                    except Exception as e:\n                        print(f\"Failed with separator='{sep}' and encoding='{enc}': {str(e)[:100]}\")\n                        continue\n                if df is not None:\n                    break\n            \n            if df is None:\n                return {'success': False, 'error': 'Could not read CSV file with any encoding/separator combination that produces multiple columns'}\n            \n            return DatabaseManager.migrate_csv_to_database_from_df(df, os.path.basename(csv_file_path))\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def migrate_csv_to_database_from_df_with_progress(df: pd.DataFrame, filename: str = \"uploaded_data.csv\", progress_callback=None) -> Dict[str, Any]:\n        \"\"\"Migrate DataFrame to database with progress callback\"\"\"\n        try:\n            # Normalize column names first - handle double spaces and irregular spacing\n            df.columns = [' '.join(col.strip().split()) for col in df.columns]\n            \n            total_rows = len(df)\n            processed_rows = 0\n            \n            # Convert to database format\n            records = []\n            for idx, row in df.iterrows():\n                # Get client name with fallback\n                cliente_name = row.get('cliente') or row.get('Cliente')\n                if pd.isna(cliente_name) or cliente_name is None or str(cliente_name).strip() == '':\n                    cliente_name = 'Cliente Desconhecido'  # Fallback\n                \n                # Get plate with fallback\n                placa_value = row.get('placa') or row.get('Placa')\n                if pd.isna(placa_value) or placa_value is None or str(placa_value).strip() == '':\n                    continue  # Skip records without plate\n                \n                record = {\n                    'cliente': str(cliente_name).strip(),\n                    'placa': str(placa_value).strip(),\n                    'ativo': row.get('ativo') or row.get('Ativo'),\n                    'data': pd.to_datetime(row.get('data') or row.get('Data'), errors='coerce', dayfirst=True),\n                    'data_gprs': pd.to_datetime(row.get('data_gprs') or row.get('Data (GPRS)'), errors='coerce', dayfirst=True),\n                    'velocidade_km': DatabaseManager._safe_float_convert(row.get('velocidade_km') or row.get('Velocidade (Km)')),\n                    'ignicao': row.get('ignicao') or row.get('Igni√ß√£o'),\n                    'motorista': row.get('motorista') or row.get('Motorista'),\n                    'gps': DatabaseManager._safe_int_convert(row.get('gps') or row.get('GPS')),\n                    'gprs': DatabaseManager._safe_int_convert(row.get('gprs') or row.get('Gprs')),\n                    'localizacao': row.get('localizacao') or row.get('Localiza√ß√£o'),\n                    'endereco': row.get('endereco') or row.get('Endere√ßo'),\n                    'tipo_evento': row.get('tipo_evento') or row.get('Tipo do Evento'),\n                    'cerca': row.get('cerca') or row.get('Cerca'),\n                    'saida': DatabaseManager._safe_int_convert(row.get('saida') or row.get('Saida')),\n                    'entrada': DatabaseManager._safe_int_convert(row.get('entrada') or row.get('Entrada')),\n                    'pacote': row.get('pacote') or row.get('Pacote'),\n                    'odometro_periodo_km': DatabaseManager._safe_float_convert(row.get('odometro_periodo_km') or row.get('Od√¥metro Per√≠odo (Km)')),\n                    'horimetro_periodo': row.get('horimetro_periodo') or row.get('Hor√≠metro Per√≠odo'),\n                    'horimetro_embarcado': row.get('horimetro_embarcado') or row.get('Hor√≠metro Embarcado'),\n                    'odometro_embarcado_km': DatabaseManager._safe_float_convert(row.get('odometro_embarcado_km') or row.get('Od√¥metro Embarcado (Km)')),\n                    'bateria': DatabaseManager._safe_float_convert(row.get('bateria') or row.get('Bateria')),\n                    'imagem': row.get('imagem') or row.get('Imagem'),\n                    'tensao': DatabaseManager._safe_float_convert(row.get('tensao') or row.get('Tens√£o')),\n                    'bloqueado': DatabaseManager._safe_int_convert(row.get('bloqueado') or row.get('Bloqueado')),\n                    'latitude': DatabaseManager._safe_float_convert(row.get('latitude') or row.get('Latitude')),\n                    'longitude': DatabaseManager._safe_float_convert(row.get('longitude') or row.get('Longitude'))\n                }\n                \n                records.append(record)\n                processed_rows += 1\n                \n                # Report progress during data preparation\n                if progress_callback and processed_rows % 100 == 0:\n                    progress_callback(processed_rows, total_rows, \"preparando\")\n            \n            # Save to database with progress\n            if initialize_database():\n                with FleetDatabaseService() as db:\n                    records_saved = db.save_telematics_data_with_progress(records, progress_callback)\n                    \n                    # Save processing record\n                    unique_vehicles = len(set(r['placa'] for r in records))\n                    unique_clients = len(set(r['cliente'] for r in records))\n                    db.save_processing_history(\n                        filename=filename,\n                        records_processed=records_saved,\n                        unique_vehicles=unique_vehicles,\n                        unique_clients=unique_clients,\n                        file_size_bytes=0  # Will be updated if available\n                    )\n                    \n                    return {\n                        'success': True,\n                        'records_processed': records_saved,\n                        'unique_vehicles': unique_vehicles,\n                        'unique_clients': unique_clients\n                    }\n            else:\n                return {'success': False, 'error': 'Database initialization failed'}\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def migrate_csv_to_database_from_df(df: pd.DataFrame, filename: str = \"uploaded_data.csv\") -> Dict[str, Any]:\n        \"\"\"Migrate DataFrame to database\"\"\"\n        try:\n            # Normalize column names first - handle double spaces and irregular spacing\n            df.columns = [' '.join(col.strip().split()) for col in df.columns]\n            \n            # Convert to database format\n            records = []\n            for _, row in df.iterrows():\n                # Get client name with fallback\n                cliente_name = row.get('cliente') or row.get('Cliente')\n                if pd.isna(cliente_name) or cliente_name is None or str(cliente_name).strip() == '':\n                    cliente_name = 'Cliente Desconhecido'  # Fallback\n                \n                # Get plate with fallback\n                placa_value = row.get('placa') or row.get('Placa')\n                if pd.isna(placa_value) or placa_value is None or str(placa_value).strip() == '':\n                    continue  # Skip records without plate\n                \n                record = {\n                    'cliente': str(cliente_name).strip(),\n                    'placa': str(placa_value).strip(),\n                    'ativo': row.get('ativo') or row.get('Ativo'),\n                    'data': pd.to_datetime(row.get('data') or row.get('Data'), errors='coerce', dayfirst=True),\n                    'data_gprs': pd.to_datetime(row.get('data_gprs') or row.get('Data (GPRS)'), errors='coerce', dayfirst=True),\n                    'velocidade_km': DatabaseManager._safe_float_convert(row.get('velocidade_km') or row.get('Velocidade (Km)')),\n                    'ignicao': row.get('ignicao') or row.get('Igni√ß√£o'),\n                    'motorista': row.get('motorista') or row.get('Motorista'),\n                    'gps': DatabaseManager._safe_int_convert(row.get('gps') or row.get('GPS')),\n                    'gprs': DatabaseManager._safe_int_convert(row.get('gprs') or row.get('Gprs')),\n                    'localizacao': row.get('localizacao') or row.get('Localiza√ß√£o'),\n                    'endereco': row.get('endereco') or row.get('Endere√ßo'),\n                    'tipo_evento': row.get('tipo_evento') or row.get('Tipo do Evento'),\n                    'cerca': row.get('cerca') or row.get('Cerca'),\n                    'saida': DatabaseManager._safe_int_convert(row.get('saida') or row.get('Saida')),\n                    'entrada': DatabaseManager._safe_int_convert(row.get('entrada') or row.get('Entrada')),\n                    'pacote': row.get('pacote') or row.get('Pacote'),\n                    'odometro_periodo_km': DatabaseManager._safe_float_convert(row.get('odometro_periodo_km') or row.get('Od√¥metro do per√≠odo (Km)') or row.get('Od√¥metro do per√≠odo  (Km)')),\n                    'horimetro_periodo': row.get('horimetro_periodo') or row.get('Hor√≠metro do per√≠odo'),\n                    'horimetro_embarcado': row.get('horimetro_embarcado') or row.get('Hor√≠metro embarcado'),\n                    'odometro_embarcado_km': DatabaseManager._safe_float_convert(row.get('odometro_embarcado_km') or row.get('Od√¥metro embarcado (Km)')),\n                    'bateria': row.get('bateria') or row.get('Bateria'),\n                    'imagem': row.get('imagem') or row.get('Imagem'),\n                    'tensao': DatabaseManager._safe_float_convert(row.get('tensao') or row.get('Tens√£o')),\n                    'bloqueado': DatabaseManager._safe_int_convert(row.get('bloqueado') or row.get('Bloqueado'))\n                }\n                \n                # Add derived location data if available\n                location_field = row.get('localizacao') or row.get('Localiza√ß√£o')\n                if pd.notna(location_field):\n                    try:\n                        location = str(location_field)\n                        if ',' in location:\n                            lat_str, lon_str = location.split(',')\n                            record['latitude'] = float(lat_str.strip())\n                            record['longitude'] = float(lon_str.strip())\n                    except:\n                        pass\n                \n                records.append(record)\n            \n            # Save to database\n            with FleetDatabaseService() as db:\n                records_saved = db.save_telematics_data(records)\n                \n                # Save processing history\n                unique_vehicles = df[df.columns[df.columns.str.contains('placa|Placa', case=False)].tolist()[0]].nunique()\n                unique_clients = df[df.columns[df.columns.str.contains('cliente|Cliente', case=False)].tolist()[0]].nunique()\n                \n                # Find date column\n                date_col = None\n                for col in df.columns:\n                    if 'data' in col.lower() and 'gprs' not in col.lower():\n                        date_col = col\n                        break\n                \n                date_range = (None, None)\n                if date_col and not df[date_col].isna().all():\n                    # Parse dates with Brazilian format\n                    date_series = pd.to_datetime(df[date_col], errors='coerce', dayfirst=True)\n                    date_range = (date_series.min(), date_series.max())\n                \n                db.save_processing_history(\n                    filename=filename,\n                    records_processed=records_saved,\n                    unique_vehicles=unique_vehicles,\n                    unique_clients=unique_clients,\n                    date_range_start=pd.to_datetime(date_range[0]) if date_range[0] else None,\n                    date_range_end=pd.to_datetime(date_range[1]) if date_range[1] else None,\n                    file_size_bytes=len(str(df)) if df is not None else 0\n                )\n            \n            return {\n                'success': True,\n                'records_processed': records_saved,\n                'unique_vehicles': unique_vehicles,\n                'unique_clients': unique_clients\n            }\n            \n        except Exception as e:\n            return {'success': False, 'error': str(e)}\n    \n    @staticmethod\n    def get_dashboard_data(client_filter: Optional[str] = None,\n                          vehicle_filter: Optional[str] = None,\n                          start_date: Optional[datetime] = None,\n                          end_date: Optional[datetime] = None) -> pd.DataFrame:\n        \"\"\"Get dashboard data with filters\"\"\"\n        with FleetDatabaseService() as db:\n            # Convert filter values to IDs if needed\n            client_id = None\n            vehicle_id = None\n            \n            if client_filter:\n                clients = db.get_all_clients()\n                for client in clients:\n                    if client.name == client_filter:\n                        client_id = client.id\n                        break\n            \n            if vehicle_filter:\n                vehicles = db.get_all_vehicles()\n                for vehicle in vehicles:\n                    if vehicle.plate == vehicle_filter:\n                        vehicle_id = vehicle.id\n                        break\n            \n            return db.get_telematics_dataframe(\n                client_id=client_id,\n                vehicle_id=vehicle_id,\n                start_date=start_date,\n                end_date=end_date\n            )\n    \n    @staticmethod\n    def get_fleet_summary() -> Dict[str, Any]:\n        \"\"\"Get fleet summary statistics\"\"\"\n        with FleetDatabaseService() as db:\n            return db.get_fleet_summary()\n    \n    @staticmethod\n    def get_processing_history() -> List[Dict[str, Any]]:\n        \"\"\"Get processing history for display\"\"\"\n        with FleetDatabaseService() as db:\n            history = db.get_processing_history()\n            return [\n                {\n                    'filename': h.filename,\n                    'upload_timestamp': h.upload_timestamp,\n                    'records_processed': h.records_processed,\n                    'unique_vehicles': h.unique_vehicles,\n                    'unique_clients': h.unique_clients,\n                    'processing_status': h.processing_status,\n                    'file_size_bytes': h.file_size_bytes\n                }\n                for h in history\n            ]\n    \n    @staticmethod\n    def clear_all_data() -> Dict[str, int]:\n        \"\"\"Clear all data from database\"\"\"\n        with FleetDatabaseService() as db:\n            return db.clear_all_data()\n    \n    @staticmethod\n    def has_data() -> bool:\n        \"\"\"Check if database has any telematics data\"\"\"\n        try:\n            if not initialize_database():\n                return False\n            with FleetDatabaseService() as db:\n                summary = db.get_fleet_summary()\n                return summary['total_records'] > 0\n        except:\n            return False\n    \n    @staticmethod\n    def get_client_list() -> List[str]:\n        \"\"\"Get list of all client names\"\"\"\n        with FleetDatabaseService() as db:\n            clients = db.get_all_clients()\n            return [client.name for client in clients]\n    \n    @staticmethod\n    def get_vehicle_list() -> List[str]:\n        \"\"\"Get list of all vehicle plates\"\"\"\n        with FleetDatabaseService() as db:\n            vehicles = db.get_all_vehicles()\n            return [vehicle.plate for vehicle in vehicles]","size_bytes":20434},"database/init_db.py":{"content":"\"\"\"\nInitialize database schema and create all tables\n\"\"\"\nfrom database.connection import engine, Base\nfrom database.models import (\n    Client, Vehicle, TelematicsData, ProcessingHistory, \n    InsightData, AlertConfiguration\n)\n\ndef create_all_tables():\n    \"\"\"Create all database tables\"\"\"\n    try:\n        Base.metadata.create_all(bind=engine)\n        print(\"‚úÖ Database tables created successfully!\")\n        return True\n    except Exception as e:\n        print(f\"‚ùå Error creating database tables: {e}\")\n        return False\n\ndef drop_all_tables():\n    \"\"\"Drop all database tables (use with caution!)\"\"\"\n    try:\n        Base.metadata.drop_all(bind=engine)\n        print(\"‚úÖ Database tables dropped successfully!\")\n        return True\n    except Exception as e:\n        print(f\"‚ùå Error dropping database tables: {e}\")\n        return False\n\nif __name__ == \"__main__\":\n    create_all_tables()","size_bytes":898},"database/models.py":{"content":"\"\"\"\nDatabase models for fleet monitoring system\n\"\"\"\nfrom sqlalchemy import Column, Integer, String, Float, DateTime, Boolean, Text, ForeignKey\nfrom sqlalchemy.orm import relationship\nfrom sqlalchemy.sql import func\nfrom database.connection import Base\n\nclass Client(Base):\n    \"\"\"Client/Customer table\"\"\"\n    __tablename__ = 'clients'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(255), unique=True, nullable=False)\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Relationships\n    vehicles = relationship(\"Vehicle\", back_populates=\"client\")\n    telematics_data = relationship(\"TelematicsData\", back_populates=\"client\")\n\nclass Vehicle(Base):\n    \"\"\"Vehicle information table\"\"\"\n    __tablename__ = 'vehicles'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    plate = Column(String(20), unique=True, nullable=False, index=True)\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=False)\n    asset_id = Column(String(50))  # ID do ativo\n    driver_name = Column(String(255))  # Nome do motorista padr√£o\n    vehicle_type = Column(String(100))  # Tipo de ve√≠culo\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Relationships\n    client = relationship(\"Client\", back_populates=\"vehicles\")\n    telematics_data = relationship(\"TelematicsData\", back_populates=\"vehicle\")\n\nclass TelematicsData(Base):\n    \"\"\"Main telematics data table - stores all GPS and sensor data\"\"\"\n    __tablename__ = 'telematics_data'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    \n    # Basic identification\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=False, index=True)\n    vehicle_id = Column(Integer, ForeignKey('vehicles.id'), nullable=False, index=True)\n    plate = Column(String(20), nullable=False, index=True)\n    asset_id = Column(String(50))\n    \n    # Timestamps\n    timestamp = Column(DateTime(timezone=True), nullable=False, index=True)\n    gprs_timestamp = Column(DateTime(timezone=True))\n    \n    # Location and GPS data\n    latitude = Column(Float)\n    longitude = Column(Float)\n    location = Column(String(255))  # Localiza√ß√£o formatted\n    address = Column(Text)  # Endere√ßo completo\n    gps_quality = Column(Boolean, default=False)  # GPS signal quality\n    gprs_quality = Column(Boolean, default=False)  # GPRS signal quality\n    \n    # Vehicle status\n    speed_kmh = Column(Float, default=0.0)  # Velocidade em km/h\n    ignition = Column(String(10))  # D=Dirigindo, L=Ligado, etc.\n    driver_name = Column(String(255))  # Motorista\n    blocked = Column(Boolean, default=False)  # Bloqueado\n    \n    # Event information\n    event_type = Column(String(100))  # Tipo do Evento\n    geofence = Column(String(255))  # Cerca eletr√¥nica\n    entry = Column(Boolean, default=False)  # Entrada\n    exit = Column(Boolean, default=False)  # Sa√≠da\n    \n    # Technical data\n    packet_id = Column(String(50))  # ID do pacote\n    odometer_period_km = Column(Float, default=0.0)  # Od√¥metro do per√≠odo\n    engine_hours_period = Column(String(20))  # Hor√≠metro do per√≠odo\n    engine_hours_total = Column(String(20))  # Hor√≠metro embarcado\n    odometer_total_km = Column(Float, default=0.0)  # Od√¥metro embarcado\n    battery_level = Column(String(10))  # N√≠vel da bateria\n    voltage = Column(Float)  # Tens√£o\n    image_url = Column(String(500))  # URL da imagem se dispon√≠vel\n    \n    # Metadata\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    \n    # Relationships\n    client = relationship(\"Client\", back_populates=\"telematics_data\")\n    vehicle = relationship(\"Vehicle\", back_populates=\"telematics_data\")\n\nclass ProcessingHistory(Base):\n    \"\"\"Track CSV file processing history\"\"\"\n    __tablename__ = 'processing_history'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    filename = Column(String(255), nullable=False)\n    upload_timestamp = Column(DateTime(timezone=True), server_default=func.now())\n    records_processed = Column(Integer, default=0)\n    records_failed = Column(Integer, default=0)\n    unique_vehicles = Column(Integer, default=0)\n    unique_clients = Column(Integer, default=0)\n    date_range_start = Column(DateTime(timezone=True))\n    date_range_end = Column(DateTime(timezone=True))\n    processing_status = Column(String(50), default='completed')  # completed, failed, processing\n    error_message = Column(Text)\n    file_size_bytes = Column(Integer)\n\nclass InsightData(Base):\n    \"\"\"Store generated insights and analysis results\"\"\"\n    __tablename__ = 'insights'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=True)\n    vehicle_id = Column(Integer, ForeignKey('vehicles.id'), nullable=True)\n    \n    # Insight content\n    title = Column(String(255), nullable=False)\n    description = Column(Text, nullable=False)\n    recommendation = Column(Text)\n    insight_type = Column(String(50), nullable=False)  # error, warning, info, success\n    priority = Column(Integer, default=3)  # 1=alta, 2=m√©dia, 3=baixa, 4=info\n    category = Column(String(100))  # compliance, efficiency, maintenance, etc.\n    \n    # Analysis metadata\n    analysis_period_start = Column(DateTime(timezone=True))\n    analysis_period_end = Column(DateTime(timezone=True))\n    confidence_score = Column(Float)  # For ML-generated insights\n    data_source = Column(String(100))  # manual, automated, ml_model\n    \n    # Timestamps\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    updated_at = Column(DateTime(timezone=True), onupdate=func.now())\n    \n    # Status tracking\n    is_active = Column(Boolean, default=True)\n    is_resolved = Column(Boolean, default=False)\n    resolved_at = Column(DateTime(timezone=True))\n    resolved_by = Column(String(255))\n\nclass AlertConfiguration(Base):\n    \"\"\"Configuration for real-time alerts\"\"\"\n    __tablename__ = 'alert_configurations'\n    \n    id = Column(Integer, primary_key=True, index=True)\n    client_id = Column(Integer, ForeignKey('clients.id'), nullable=True)\n    vehicle_id = Column(Integer, ForeignKey('vehicles.id'), nullable=True)\n    \n    # Alert configuration\n    alert_type = Column(String(100), nullable=False)  # speed_limit, geofence, maintenance\n    threshold_value = Column(Float)\n    threshold_operator = Column(String(10))  # >, <, =, >=, <=\n    is_active = Column(Boolean, default=True)\n    \n    # Notification settings\n    notification_channels = Column(Text)  # JSON array of notification methods\n    cooldown_minutes = Column(Integer, default=15)  # Prevent spam\n    \n    # Metadata\n    created_at = Column(DateTime(timezone=True), server_default=func.now())\n    created_by = Column(String(255))","size_bytes":6925},"database/services.py":{"content":"\"\"\"\nDatabase service layer for fleet monitoring operations\n\"\"\"\nfrom typing import List, Optional, Dict, Any, Tuple\nfrom datetime import datetime, timedelta\nimport pandas as pd\nfrom sqlalchemy.orm import Session\nfrom sqlalchemy import func, and_, or_, Integer\nfrom database.connection import get_db_session, close_db_session, initialize_database\nfrom database.models import (\n    Client, Vehicle, TelematicsData, ProcessingHistory, \n    InsightData, AlertConfiguration\n)\n\nclass FleetDatabaseService:\n    \"\"\"Service class for all fleet monitoring database operations\"\"\"\n    \n    def __init__(self):\n        self.session = None\n    \n    def __enter__(self):\n        if not initialize_database():\n            raise Exception(\"Falha ao conectar com a base de dados\")\n        self.session = get_db_session()\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        if self.session:\n            if exc_type:\n                self.session.rollback()\n            else:\n                self.session.commit()\n            close_db_session(self.session)\n    \n    # Client operations\n    def get_or_create_client(self, client_name: str) -> Client:\n        \"\"\"Get existing client or create new one\"\"\"\n        client = self.session.query(Client).filter(Client.name == client_name).first()\n        if not client:\n            client = Client(name=client_name)\n            self.session.add(client)\n            self.session.flush()  # Get the ID\n        return client\n    \n    def get_all_clients(self) -> List[Client]:\n        \"\"\"Get all clients\"\"\"\n        return self.session.query(Client).all()\n    \n    # Vehicle operations\n    def get_or_create_vehicle(self, plate: str, client_id: int, asset_id: str = None) -> Vehicle:\n        \"\"\"Get existing vehicle or create new one\"\"\"\n        vehicle = self.session.query(Vehicle).filter(Vehicle.plate == plate).first()\n        if not vehicle:\n            vehicle = Vehicle(\n                plate=plate,\n                client_id=client_id,\n                asset_id=asset_id\n            )\n            self.session.add(vehicle)\n            self.session.flush()\n        return vehicle\n    \n    def get_vehicles_by_client(self, client_id: int) -> List[Vehicle]:\n        \"\"\"Get all vehicles for a client\"\"\"\n        return self.session.query(Vehicle).filter(Vehicle.client_id == client_id).all()\n    \n    def get_all_vehicles(self) -> List[Vehicle]:\n        \"\"\"Get all vehicles\"\"\"\n        return self.session.query(Vehicle).all()\n    \n    # Telematics data operations\n    def save_telematics_data_with_progress(self, data_records: List[Dict[str, Any]], progress_callback=None) -> int:\n        \"\"\"Save multiple telematics data records with progress callback\"\"\"\n        records_saved = 0\n        records_failed = 0\n        total_records = len(data_records)\n        \n        # Process in batches to avoid memory issues\n        batch_size = 50  # Smaller batch for more frequent progress updates\n        for i in range(0, total_records, batch_size):\n            batch = data_records[i:i + batch_size]\n            batch_saved = 0\n            \n            try:\n                for record in batch:\n                    try:\n                        # Validate timestamp before processing\n                        timestamp = record.get('data')\n                        if timestamp is None or pd.isna(timestamp):\n                            records_failed += 1\n                            continue\n                        \n                        # Get or create client and vehicle\n                        client = self.get_or_create_client(record['cliente'])\n                        vehicle = self.get_or_create_vehicle(\n                            record['placa'], \n                            client.id, \n                            record.get('ativo')\n                        )\n                        \n                        # Create telematics data record\n                        telematics = TelematicsData(\n                            client_id=client.id,\n                            vehicle_id=vehicle.id,\n                            plate=record['placa'],\n                            asset_id=record.get('ativo'),\n                            timestamp=timestamp,\n                            gprs_timestamp=record.get('data_gprs'),\n                            latitude=record.get('latitude'),\n                            longitude=record.get('longitude'),\n                            location=record.get('localizacao'),\n                            address=record.get('endereco'),\n                            gps_quality=record.get('gps', 0) == 1,\n                            gprs_quality=record.get('gprs', 0) == 1,\n                            speed_kmh=record.get('velocidade_km', 0.0),\n                            ignition=record.get('ignicao'),\n                            driver_name=record.get('motorista'),\n                            blocked=record.get('bloqueado', 0) == 1,\n                            event_type=record.get('tipo_evento'),\n                            geofence=record.get('cerca'),\n                            entry=record.get('entrada', 0) == 1,\n                            exit=record.get('saida', 0) == 1,\n                            packet_id=record.get('pacote'),\n                            odometer_period_km=record.get('odometro_periodo_km', 0.0),\n                            odometer_total_km=record.get('odometro_embarcado_km', 0.0),\n                            engine_hours_period=record.get('horimetro_periodo'),\n                            engine_hours_total=record.get('horimetro_embarcado'),\n                            battery_level=record.get('bateria'),\n                            voltage=record.get('tensao'),\n                            image_url=record.get('imagem')\n                        )\n                        \n                        self.session.add(telematics)\n                        batch_saved += 1\n                        records_saved += 1\n                        \n                        # Report progress every 10 records within batch\n                        if progress_callback and records_saved % 10 == 0:\n                            progress_callback(records_saved, total_records, \"inserindo\")\n                        \n                    except Exception as record_error:\n                        records_failed += 1\n                        print(f\"Failed to save record: {str(record_error)[:200]}\")\n                        continue\n                \n                # Commit the batch\n                self.session.commit()\n                \n                # Report progress after each batch\n                if progress_callback:\n                    progress_callback(records_saved, total_records, \"inserindo\")\n                \n            except Exception as batch_error:\n                self.session.rollback()\n                records_failed += batch_size\n                print(f\"Batch failed: {str(batch_error)[:200]}\")\n                continue\n        \n        print(f\"Batch insertion completed: {records_saved} saved, {records_failed} failed\")\n        return records_saved\n    \n    def save_telematics_data(self, data_records: List[Dict[str, Any]]) -> int:\n        \"\"\"Save multiple telematics data records with proper error handling\"\"\"\n        records_saved = 0\n        records_failed = 0\n        \n        # Process in batches to avoid memory issues\n        batch_size = 100\n        for i in range(0, len(data_records), batch_size):\n            batch = data_records[i:i + batch_size]\n            batch_saved = 0\n            \n            try:\n                for record in batch:\n                    try:\n                        # Validate timestamp before processing\n                        timestamp = record.get('data')\n                        if timestamp is None or pd.isna(timestamp):\n                            records_failed += 1\n                            continue\n                        \n                        # Get or create client and vehicle\n                        client = self.get_or_create_client(record['cliente'])\n                        vehicle = self.get_or_create_vehicle(\n                            record['placa'], \n                            client.id, \n                            record.get('ativo')\n                        )\n                        \n                        # Create telematics data record\n                        telematics = TelematicsData(\n                            client_id=client.id,\n                            vehicle_id=vehicle.id,\n                            plate=record['placa'],\n                            asset_id=record.get('ativo'),\n                            timestamp=timestamp,\n                            gprs_timestamp=record.get('data_gprs'),\n                            latitude=record.get('latitude'),\n                            longitude=record.get('longitude'),\n                            location=record.get('localizacao'),\n                            address=record.get('endereco'),\n                            gps_quality=record.get('gps', 0) == 1,\n                            gprs_quality=record.get('gprs', 0) == 1,\n                            speed_kmh=record.get('velocidade_km', 0.0),\n                            ignition=record.get('ignicao'),\n                            driver_name=record.get('motorista'),\n                            blocked=record.get('bloqueado', 0) == 1,\n                            event_type=record.get('tipo_evento'),\n                            geofence=record.get('cerca'),\n                            entry=record.get('entrada', 0) == 1,\n                            exit=record.get('saida', 0) == 1,\n                            packet_id=record.get('pacote'),\n                            odometer_period_km=record.get('odometro_periodo_km', 0.0),\n                            engine_hours_period=record.get('horimetro_periodo'),\n                            engine_hours_total=record.get('horimetro_embarcado'),\n                            odometer_total_km=record.get('odometro_embarcado_km', 0.0),\n                            battery_level=record.get('bateria'),\n                            voltage=record.get('tensao'),\n                            image_url=record.get('imagem')\n                        )\n                        \n                        self.session.add(telematics)\n                        batch_saved += 1\n                        \n                    except Exception as e:\n                        # Log individual record error but continue processing\n                        print(f\"Error processing record {record.get('placa', 'unknown')}: {str(e)}\")\n                        records_failed += 1\n                        continue\n                \n                # Commit the batch\n                self.session.commit()\n                records_saved += batch_saved\n                \n            except Exception as e:\n                # If batch commit fails, rollback and mark all batch records as failed\n                print(f\"Error committing batch {i//batch_size + 1}: {str(e)}\")\n                self.session.rollback()\n                records_failed += len(batch)\n        \n        print(f\"Migration complete: {records_saved} saved, {records_failed} failed\")\n        return records_saved\n    \n    def get_telematics_data(self, \n                           client_id: Optional[int] = None,\n                           vehicle_id: Optional[int] = None,\n                           plate: Optional[str] = None,\n                           start_date: Optional[datetime] = None,\n                           end_date: Optional[datetime] = None,\n                           limit: Optional[int] = None) -> List[TelematicsData]:\n        \"\"\"Get telematics data with filters\"\"\"\n        query = self.session.query(TelematicsData)\n        \n        if client_id:\n            query = query.filter(TelematicsData.client_id == client_id)\n        if vehicle_id:\n            query = query.filter(TelematicsData.vehicle_id == vehicle_id)\n        if plate:\n            query = query.filter(TelematicsData.plate == plate)\n        if start_date:\n            query = query.filter(TelematicsData.timestamp >= start_date)\n        if end_date:\n            query = query.filter(TelematicsData.timestamp <= end_date)\n        \n        query = query.order_by(TelematicsData.timestamp.desc())\n        \n        if limit:\n            query = query.limit(limit)\n        \n        return query.all()\n    \n    def get_telematics_dataframe(self, **filters) -> pd.DataFrame:\n        \"\"\"Get telematics data as pandas DataFrame\"\"\"\n        data = self.get_telematics_data(**filters)\n        \n        if not data:\n            return pd.DataFrame()\n        \n        # Convert to DataFrame with original column names for compatibility\n        records = []\n        for record in data:\n            records.append({\n                'cliente': record.client.name,\n                'placa': record.plate,\n                'ativo': record.asset_id,\n                'data': record.timestamp,\n                'data_gprs': record.gprs_timestamp,\n                'velocidade_km': record.speed_kmh,\n                'ignicao': record.ignition,\n                'motorista': record.driver_name,\n                'gps': 1 if record.gps_quality else 0,\n                'gprs': 1 if record.gprs_quality else 0,\n                'localizacao': record.location,\n                'endereco': record.address,\n                'tipo_evento': record.event_type,\n                'cerca': record.geofence,\n                'saida': 1 if record.exit else 0,\n                'entrada': 1 if record.entry else 0,\n                'pacote': record.packet_id,\n                'odometro_periodo_km': record.odometer_period_km,\n                'engine_hours_period': record.engine_hours_period,\n                'engine_hours_total': record.engine_hours_total,\n                'odometer_total_km': record.odometer_total_km,\n                'battery_level': record.battery_level,\n                'imagem': record.image_url,\n                'tensao': record.voltage,\n                'bloqueado': 1 if record.blocked else 0,\n                'latitude': record.latitude,\n                'longitude': record.longitude\n            })\n        \n        return pd.DataFrame(records)\n    \n    # Analytics and KPI methods\n    def get_fleet_summary(self) -> Dict[str, Any]:\n        \"\"\"Get overall fleet summary statistics\"\"\"\n        total_vehicles = self.session.query(Vehicle).count()\n        total_clients = self.session.query(Client).count()\n        \n        # Get latest data period\n        latest_data = self.session.query(\n            func.min(TelematicsData.timestamp).label('start_date'),\n            func.max(TelematicsData.timestamp).label('end_date'),\n            func.count(TelematicsData.id).label('total_records')\n        ).first()\n        \n        # Calculate average speed and total distance\n        speed_stats = self.session.query(\n            func.avg(TelematicsData.speed_kmh).label('avg_speed'),\n            func.max(TelematicsData.speed_kmh).label('max_speed'),\n            func.sum(TelematicsData.odometer_period_km).label('total_distance')\n        ).first()\n        \n        # GPS coverage - simplified approach\n        gps_coverage = self.session.query(\n            func.avg(TelematicsData.gps_quality.cast(Integer)).label('gps_coverage')\n        ).first()\n        \n        return {\n            'total_vehicles': total_vehicles,\n            'total_clients': total_clients,\n            'total_records': latest_data.total_records if latest_data.total_records else 0,\n            'start_date': latest_data.start_date,\n            'end_date': latest_data.end_date,\n            'avg_speed': round(speed_stats.avg_speed, 1) if speed_stats.avg_speed else 0,\n            'max_speed': speed_stats.max_speed if speed_stats.max_speed else 0,\n            'total_distance': round(speed_stats.total_distance, 1) if speed_stats.total_distance else 0,\n            'gps_coverage': round(gps_coverage.gps_coverage * 100, 1) if gps_coverage.gps_coverage else 0\n        }\n    \n    # Processing history operations\n    def save_processing_history(self, \n                               filename: str,\n                               records_processed: int,\n                               records_failed: int = 0,\n                               unique_vehicles: int = 0,\n                               unique_clients: int = 0,\n                               date_range_start: Optional[datetime] = None,\n                               date_range_end: Optional[datetime] = None,\n                               processing_status: str = 'completed',\n                               error_message: Optional[str] = None,\n                               file_size_bytes: Optional[int] = None) -> ProcessingHistory:\n        \"\"\"Save processing history record\"\"\"\n        history = ProcessingHistory(\n            filename=filename,\n            records_processed=records_processed,\n            records_failed=records_failed,\n            unique_vehicles=unique_vehicles,\n            unique_clients=unique_clients,\n            date_range_start=date_range_start,\n            date_range_end=date_range_end,\n            processing_status=processing_status,\n            error_message=error_message,\n            file_size_bytes=file_size_bytes\n        )\n        \n        self.session.add(history)\n        self.session.flush()\n        return history\n    \n    def get_processing_history(self, limit: int = 10) -> List[ProcessingHistory]:\n        \"\"\"Get recent processing history\"\"\"\n        return (self.session.query(ProcessingHistory)\n                .order_by(ProcessingHistory.upload_timestamp.desc())\n                .limit(limit)\n                .all())\n    \n    def clear_processing_history(self) -> int:\n        \"\"\"Clear all processing history\"\"\"\n        count = self.session.query(ProcessingHistory).count()\n        self.session.query(ProcessingHistory).delete()\n        return count\n    \n    def clear_all_data(self) -> Dict[str, int]:\n        \"\"\"Clear all telematics data and processing history\"\"\"\n        telematics_count = self.session.query(TelematicsData).count()\n        history_count = self.session.query(ProcessingHistory).count()\n        insights_count = self.session.query(InsightData).count()\n        vehicles_count = self.session.query(Vehicle).count()\n        clients_count = self.session.query(Client).count()\n        \n        # Clear all data\n        self.session.query(TelematicsData).delete()\n        self.session.query(ProcessingHistory).delete() \n        self.session.query(InsightData).delete()\n        self.session.query(Vehicle).delete()\n        self.session.query(Client).delete()\n        \n        return {\n            'telematics_data': telematics_count,\n            'processing_history': history_count, \n            'insights': insights_count,\n            'vehicles': vehicles_count,\n            'clients': clients_count\n        }\n    \n    # Insights operations\n    def save_insight(self, \n                    title: str,\n                    description: str,\n                    insight_type: str,\n                    priority: int = 3,\n                    recommendation: Optional[str] = None,\n                    category: Optional[str] = None,\n                    client_id: Optional[int] = None,\n                    vehicle_id: Optional[int] = None,\n                    confidence_score: Optional[float] = None,\n                    data_source: str = 'automated') -> InsightData:\n        \"\"\"Save a new insight\"\"\"\n        insight = InsightData(\n            title=title,\n            description=description,\n            recommendation=recommendation,\n            insight_type=insight_type,\n            priority=priority,\n            category=category,\n            client_id=client_id,\n            vehicle_id=vehicle_id,\n            confidence_score=confidence_score,\n            data_source=data_source\n        )\n        \n        self.session.add(insight)\n        self.session.flush()\n        return insight\n    \n    def get_insights(self, \n                    category: Optional[str] = None,\n                    client_id: Optional[int] = None,\n                    vehicle_id: Optional[int] = None,\n                    is_active: bool = True,\n                    limit: int = 50) -> List[InsightData]:\n        \"\"\"Get insights with filters\"\"\"\n        query = self.session.query(InsightData)\n        \n        if category:\n            query = query.filter(InsightData.category == category)\n        if client_id:\n            query = query.filter(InsightData.client_id == client_id)\n        if vehicle_id:\n            query = query.filter(InsightData.vehicle_id == vehicle_id)\n        if is_active is not None:\n            query = query.filter(InsightData.is_active == is_active)\n        \n        return query.order_by(InsightData.created_at.desc()).limit(limit).all()","size_bytes":20833},"pages/1_üìä_Dashboard.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport sys\n\n# Adicionar o diret√≥rio raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.visualizations import FleetVisualizations\nfrom database.db_manager import DatabaseManager\n\nst.set_page_config(\n    page_title=\"Dashboard - Insight Hub\",\n    page_icon=\"üìä\",\n    layout=\"wide\"\n)\n\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais)\"\"\"\n    try:\n        # Carregar dados diretamente da base de dados\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty:\n            st.success(f\"‚úÖ Dados reais carregados: {len(df):,} registros da base de dados\")\n            return df\n        \n        # Se n√£o h√° dados, mostrar mensagem clara\n        st.warning(\"‚ö†Ô∏è Nenhum dado encontrado na base de dados. Fa√ßa upload dos seus arquivos CSV.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados reais: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"üìä An√°lise Profissional\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"üìÅ Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados da base de dados\n    analyzer = DataAnalyzer.from_database()\n    visualizer = FleetVisualizations(analyzer)\n    \n    # Sidebar com filtros\n    st.sidebar.header(\"üîç Filtros\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por per√≠odo\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    col_data1, col_data2 = st.sidebar.columns(2)\n    with col_data1:\n        data_inicio = st.date_input(\"Data In√≠cio:\", min_date, min_value=min_date, max_value=max_date)\n    with col_data2:\n        data_fim = st.date_input(\"Data Fim:\", max_date, min_value=min_date, max_value=max_date)\n    \n    # Filtro por ve√≠culo\n    veiculos_disponiveis = ['Todos']\n    if cliente_selecionado != \"Todos\":\n        veiculos_disponiveis.extend(sorted(df[df['cliente'] == cliente_selecionado]['placa'].unique().tolist()))\n    else:\n        veiculos_disponiveis.extend(sorted(df['placa'].unique().tolist()))\n    \n    veiculo_selecionado = st.sidebar.selectbox(\"Ve√≠culo:\", veiculos_disponiveis)\n    \n    # Aplicar filtros\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        placa=veiculo_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    if filtered_df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Obter KPIs\n    kpis = analyzer.get_kpis()\n    \n    # Verificar se h√° KPIs v√°lidos\n    if not kpis:\n        st.warning(\"‚ö†Ô∏è N√£o foi poss√≠vel calcular m√©tricas com os filtros aplicados. Tente ajustar os filtros.\")\n        st.stop()\n    \n    # Mostrar m√©tricas principais\n    st.header(\"üìà M√©tricas Principais\")\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            label=\"üöó Total de Ve√≠culos\",\n            value=f\"{kpis['total_veiculos']:,}\",\n            delta=f\"{len(filtered_df):,} registros\"\n        )\n    \n    with col2:\n        st.metric(\n            label=\"‚ö° Velocidade M√©dia\",\n            value=f\"{kpis['velocidade_media']:.1f} km/h\",\n            delta=f\"Max: {kpis['velocidade_maxima']:.0f} km/h\"\n        )\n    \n    with col3:\n        st.metric(\n            label=\"üõ£Ô∏è Dist√¢ncia Total\",\n            value=f\"{kpis['distancia_total']:.0f} km\",\n            delta=f\"{kpis['tempo_ativo_horas']:.1f} horas ativas\"\n        )\n    \n    with col4:\n        st.metric(\n            label=\"üì° Cobertura GPS\",\n            value=f\"{kpis['cobertura_gps']:.1f}%\",\n            delta=f\"{kpis['veiculos_bloqueados']} bloqueados\"\n        )\n    \n    st.markdown(\"---\")\n    \n    # Gr√°ficos principais\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"üìä Distribui√ß√£o de Velocidade\")\n        \n        speed_dist_fig = px.histogram(\n            filtered_df,\n            x='velocidade_km',\n            nbins=30,\n            title='Distribui√ß√£o de Velocidade',\n            labels={'velocidade_km': 'Velocidade (km/h)', 'count': 'Frequ√™ncia'}\n        )\n        speed_dist_fig.update_layout(height=400)\n        st.plotly_chart(speed_dist_fig, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"üöó Top 10 Ve√≠culos Mais Ativos\")\n        \n        vehicle_activity = filtered_df.groupby('placa').size().sort_values(ascending=False).head(10)\n        \n        activity_fig = px.bar(\n            x=vehicle_activity.values,\n            y=vehicle_activity.index,\n            orientation='h',\n            title='Registros por Ve√≠culo',\n            labels={'x': 'N√∫mero de Registros', 'y': 'Placa'}\n        )\n        activity_fig.update_layout(height=400)\n        st.plotly_chart(activity_fig, use_container_width=True)\n    \n    # An√°lise temporal\n    st.subheader(\"‚è∞ An√°lise Temporal\")\n    \n    # Atividade por hora\n    hourly_activity = filtered_df.groupby(filtered_df['data'].dt.hour).agg({\n        'placa': 'nunique',\n        'velocidade_km': 'mean'\n    }).reset_index()\n    \n    col_temp1, col_temp2 = st.columns(2)\n    \n    with col_temp1:\n        hourly_vehicles_fig = px.line(\n            hourly_activity,\n            x='data',\n            y='placa',\n            title='Ve√≠culos Ativos por Hora',\n            labels={'data': 'Hora do Dia', 'placa': 'N√∫mero de Ve√≠culos Ativos'}\n        )\n        hourly_vehicles_fig.update_layout(height=350)\n        st.plotly_chart(hourly_vehicles_fig, use_container_width=True)\n    \n    with col_temp2:\n        hourly_speed_fig = px.line(\n            hourly_activity,\n            x='data',\n            y='velocidade_km',\n            title='Velocidade M√©dia por Hora',\n            labels={'data': 'Hora do Dia', 'velocidade_km': 'Velocidade M√©dia (km/h)'}\n        )\n        hourly_speed_fig.update_layout(height=350)\n        st.plotly_chart(hourly_speed_fig, use_container_width=True)\n    \n    # Atividade di√°ria (se mais de um dia)\n    if kpis['periodo_dias'] > 1:\n        daily_activity = filtered_df.groupby(filtered_df['data'].dt.date).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean',\n            'odometro_periodo_km': 'sum'\n        }).reset_index()\n        \n        st.subheader(\"üìÖ Tend√™ncia Di√°ria\")\n        \n        daily_fig = px.line(\n            daily_activity,\n            x='data',\n            y='placa',\n            title='Ve√≠culos Ativos por Dia',\n            labels={'data': 'Data', 'placa': 'N√∫mero de Ve√≠culos Ativos'}\n        )\n        daily_fig.update_layout(height=400)\n        st.plotly_chart(daily_fig, use_container_width=True)\n    \n    # Tabela de resumo por ve√≠culo\n    st.subheader(\"üìã Resumo por Ve√≠culo\")\n    \n    vehicle_summary = filtered_df.groupby('placa').agg({\n        'velocidade_km': ['count', 'mean', 'max'],\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.sum() / len(x)) * 100,\n        'bloqueado': 'any'\n    }).round(2)\n    \n    # Achatar MultiIndex\n    vehicle_summary.columns = [\n        'Registros', 'Vel. M√©dia', 'Vel. M√°xima', \n        'KM Total', 'GPS (%)', 'Bloqueado'\n    ]\n    \n    vehicle_summary = vehicle_summary.sort_values('Registros', ascending=False)\n    \n    st.dataframe(\n        vehicle_summary,\n        use_container_width=True,\n        height=300\n    )\n    \n    # Estat√≠sticas do per√≠odo filtrado\n    st.markdown(\"---\")\n    st.subheader(\"üìä Estat√≠sticas do Per√≠odo\")\n    \n    info_col1, info_col2, info_col3 = st.columns(3)\n    \n    with info_col1:\n        st.info(f\"\"\"\n        **üìÖ Per√≠odo Analisado:**\n        - In√≠cio: {data_inicio.strftime('%d/%m/%Y')}\n        - Fim: {data_fim.strftime('%d/%m/%Y')}\n        - Dura√ß√£o: {kpis['periodo_dias']} dias\n        \"\"\")\n    \n    with info_col2:\n        st.info(f\"\"\"\n        **üöó Frota:**\n        - Total de Ve√≠culos: {kpis['total_veiculos']:,}\n        - Total de Registros: {kpis['total_registros']:,}\n        - M√©dia por Ve√≠culo: {kpis['total_registros']/kpis['total_veiculos']:.0f}\n        \"\"\")\n    \n    with info_col3:\n        st.info(f\"\"\"\n        **‚ö° Performance:**\n        - Velocidade M√©dia: {kpis['velocidade_media']:.1f} km/h\n        - Dist√¢ncia Total: {kpis['distancia_total']:.0f} km\n        - Tempo Ativo: {kpis['tempo_ativo_horas']:.1f} horas\n        \"\"\")\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":8815},"pages/2_üìÅ_Upload_CSV.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime\nimport sys\n\n# Adicionar o diret√≥rio raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.csv_processor import CSVProcessor\nfrom database.db_manager import DatabaseManager\n\nst.set_page_config(\n    page_title=\"Upload CSV - Insight Hub\",\n    page_icon=\"üìÅ\",\n    layout=\"wide\"\n)\n\ndef main():\n    st.title(\"üìÅ Upload e Processamento de Dados CSV\")\n    st.markdown(\"---\")\n    \n    # Informa√ß√µes sobre o formato esperado\n    with st.expander(\"üìã Formato do Arquivo CSV\", expanded=False):\n        st.markdown(\"\"\"\n        ### Campos Obrigat√≥rios (25 campos):\n        \n        1. **Cliente** - Nome do cliente/empresa\n        2. **Placa** - Placa do ve√≠culo\n        3. **Ativo** - Identificador do ativo\n        4. **Data** - Data/hora do evento\n        5. **Data (GPRS)** - Data/hora de comunica√ß√£o GPRS\n        6. **Velocidade (Km)** - Velocidade em km/h\n        7. **Igni√ß√£o** - Status da igni√ß√£o (D/L)\n        8. **Motorista** - Identifica√ß√£o do motorista\n        9. **GPS** - Status do GPS (0/1)\n        10. **Gprs** - Status do GPRS (0/1)\n        11. **Localiza√ß√£o** - Coordenadas do ve√≠culo\n        12. **Endere√ßo** - Endere√ßo convertido\n        13. **Tipo do Evento** - Tipo do evento registrado\n        14. **Cerca** - Informa√ß√µes de cerca eletr√¥nica\n        15. **Saida** - Status das sa√≠das digitais\n        16. **Entrada** - Status das entradas digitais\n        17. **Pacote** - Informa√ß√µes de pacotes de dados\n        18. **Od√¥metro do per√≠odo (Km)** - Km percorridos\n        19. **Hor√≠metro do per√≠odo** - Tempo de funcionamento\n        20. **Hor√≠metro embarcado** - Contador de horas\n        21. **Od√¥metro embarcado (Km)** - Od√¥metro embarcado\n        22. **Bateria** - N√≠vel de bateria\n        23. **Imagem** - Link para imagem (opcional)\n        24. **Tens√£o** - Tens√£o el√©trica\n        25. **Bloqueado** - Status de bloqueio (0/1)\n        \n        ### Observa√ß√µes:\n        - O arquivo deve estar em formato CSV com separador v√≠rgula (,)\n        - Todos os 25 campos s√£o obrigat√≥rios\n        - Tamanho m√°ximo: 50MB\n        - At√© 100.000 registros por arquivo\n        \"\"\")\n    \n    # √Årea de upload\n    st.subheader(\"üì§ Fazer Upload do Arquivo CSV\")\n    \n    uploaded_files = st.file_uploader(\n        \"Selecione os arquivos CSV:\",\n        type=['csv'],\n        help=\"M√∫ltiplos arquivos CSV com dados telem√°ticos da frota\",\n        accept_multiple_files=True\n    )\n    \n    if uploaded_files:\n        # Mostrar informa√ß√µes dos arquivos\n        st.info(f\"üìÅ **{len(uploaded_files)} arquivo(s) selecionado(s)**\")\n        \n        total_size = sum(f.size for f in uploaded_files)\n        if total_size > 200 * 1024 * 1024:  # 200MB total\n            st.error(\"‚ùå Total de arquivos muito grande! Tamanho m√°ximo: 200MB\")\n            st.stop()\n        \n        # Mostrar lista de arquivos\n        for i, file in enumerate(uploaded_files):\n            size_mb = file.size / 1024 / 1024\n            st.write(f\"{i+1}. **{file.name}** - {size_mb:.2f} MB\")\n        \n        # Preview do primeiro arquivo\n        try:\n            first_file = uploaded_files[0]\n            preview_df = None\n            separators = [';', ',']\n            encodings = ['latin-1', 'utf-8', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for sep in separators:\n                for enc in encodings:\n                    try:\n                        first_file.seek(0)\n                        preview_df = pd.read_csv(first_file, sep=sep, encoding=enc, nrows=5)\n                        st.success(f\"üìÑ Formato detectado: separador '{sep}', encoding '{enc}'\")\n                        break\n                    except:\n                        continue\n                if preview_df is not None:\n                    break\n            \n            if preview_df is not None:\n                st.subheader(\"üëÄ Preview dos Dados\")\n                st.dataframe(preview_df, width='stretch')\n                \n                col1, col2 = st.columns(2)\n                with col1:\n                    st.metric(\"Colunas encontradas\", len(preview_df.columns))\n                with col2:\n                    st.metric(\"Total de arquivos\", len(uploaded_files))\n            \n            first_file.seek(0)\n            \n            # Bot√£o para processar todos os arquivos\n            if st.button(\"üöÄ Processar Todos os Arquivos\", type=\"primary\"):\n                # Ordenar arquivos por tamanho (menor para maior) para processamento mais eficiente\n                sorted_files = sorted(uploaded_files, key=lambda f: f.size)\n                st.info(f\"üìä Arquivos ordenados por tamanho: menor ‚Üí maior para otimizar processamento\")\n                process_multiple_csv_files(sorted_files)\n                \n        except Exception as e:\n            st.error(f\"‚ùå Erro ao ler os arquivos: {str(e)}\")\n    \n    # Mostrar hist√≥rico de arquivos processados\n    show_processing_history()\n\ndef process_multiple_csv_files(uploaded_files):\n    \"\"\"Processa m√∫ltiplos arquivos CSV com progresso detalhado\"\"\"\n    \n    # Container principal para progresso\n    progress_container = st.container()\n    \n    with progress_container:\n        st.markdown(\"### üìä Progresso de Processamento\")\n        \n        # Progresso geral\n        overall_progress = st.progress(0)\n        overall_status = st.empty()\n        \n        # Progresso do arquivo atual\n        current_file_info = st.empty()\n        file_progress_bar = st.progress(0)\n        file_status = st.empty()\n        \n        # M√©tricas em tempo real\n        metrics_cols = st.columns(3)\n        with metrics_cols[0]:\n            files_metric = st.empty()\n        with metrics_cols[1]:\n            records_metric = st.empty()\n        with metrics_cols[2]:\n            speed_metric = st.empty()\n    \n    total_files = len(uploaded_files)\n    processed_files = 0\n    total_records = 0\n    failed_files = 0\n    \n    import time\n    start_time = time.time()\n    \n    for i, uploaded_file in enumerate(uploaded_files):\n        file_start_time = time.time()\n        \n        # Atualizar progresso geral\n        overall_progress.progress(i / total_files)\n        overall_status.markdown(f\"üìÇ **Processando arquivo {i+1} de {total_files}**\")\n        \n        # Informa√ß√µes do arquivo atual\n        file_size_mb = uploaded_file.size / (1024 * 1024)\n        current_file_info.markdown(f\"\"\"\n        üìÑ **{uploaded_file.name}**  \n        üìè Tamanho: {file_size_mb:.1f} MB\n        \"\"\")\n        \n        # Resetar progresso do arquivo\n        file_progress_bar.progress(0)\n        file_status.markdown(\"üîÑ Iniciando processamento...\")\n        \n        # Atualizar m√©tricas\n        files_metric.metric(\n            label=\"üìÅ Arquivos\",\n            value=f\"{processed_files + failed_files}/{total_files}\",\n            delta=f\"{i}/{total_files}\"\n        )\n        records_metric.metric(\n            label=\"üìä Registros\",\n            value=f\"{total_records:,}\",\n            delta=\"Processando...\"\n        )\n        \n        elapsed_time = time.time() - start_time\n        if elapsed_time > 0:\n            files_per_min = (i / elapsed_time) * 60\n            speed_metric.metric(\n                label=\"‚ö° Velocidade\",\n                value=f\"{files_per_min:.1f}\",\n                delta=\"arquivos/min\"\n            )\n        \n        # Processar arquivo com callback de progresso\n        result = process_single_csv_file_with_progress(\n            uploaded_file, \n            file_progress_bar, \n            file_status\n        )\n        \n        # Finalizar progresso do arquivo\n        file_progress_bar.progress(1.0)\n        file_processing_time = time.time() - file_start_time\n        \n        if result['success']:\n            processed_files += 1\n            records_processed = result.get('records_processed', 0)\n            total_records += records_processed\n            \n            file_status.markdown(f\"‚úÖ **Conclu√≠do!** {records_processed:,} registros em {file_processing_time:.1f}s\")\n            \n            # Log de sucesso\n            with st.expander(f\"‚úÖ {uploaded_file.name}\", expanded=False):\n                st.success(f\"Processado com sucesso: {records_processed:,} registros\")\n                st.info(f\"Tempo: {file_processing_time:.1f}s | Velocidade: {records_processed/file_processing_time:.0f} reg/s\")\n        else:\n            failed_files += 1\n            file_status.markdown(\"‚ùå **Erro no processamento**\")\n            \n            # Log de erro\n            with st.expander(f\"‚ùå {uploaded_file.name}\", expanded=True):\n                st.error(f\"Erro: {result.get('error', 'Erro desconhecido')}\")\n        \n        # Pequena pausa para visualiza√ß√£o\n        time.sleep(0.1)\n    \n    # Finalizar progresso geral\n    overall_progress.progress(1.0)\n    total_time = time.time() - start_time\n    \n    overall_status.markdown(\"üéâ **Processamento Finalizado!**\")\n    current_file_info.markdown(\"üìÅ **Todos os arquivos processados**\")\n    file_status.markdown(f\"‚è±Ô∏è Tempo total: {total_time:.1f}s\")\n    \n    # M√©tricas finais\n    files_metric.metric(\n        label=\"üìÅ Arquivos\",\n        value=f\"{processed_files}/{total_files}\",\n        delta=f\"{failed_files} erros\" if failed_files > 0 else \"Todos OK\"\n    )\n    records_metric.metric(\n        label=\"üìä Registros\",\n        value=f\"{total_records:,}\",\n        delta=\"Processados\"\n    )\n    speed_metric.metric(\n        label=\"‚ö° Velocidade m√©dia\",\n        value=f\"{total_records/total_time:.0f}\" if total_time > 0 else \"0\",\n        delta=\"registros/s\"\n    )\n    \n    # Resumo final\n    if processed_files == total_files:\n        st.success(f\"\"\"\n        üéâ **Processamento 100% conclu√≠do!**\n        - ‚úÖ {processed_files} arquivos processados com sucesso\n        - üìä {total_records:,} registros inseridos na base de dados\n        - ‚è±Ô∏è Tempo total: {total_time:.1f} segundos\n        - ‚ö° Velocidade m√©dia: {total_records/total_time:.0f} registros/segundo\n        \"\"\")\n    else:\n        st.warning(f\"\"\"\n        ‚ö†Ô∏è **Processamento conclu√≠do com erros**\n        - ‚úÖ {processed_files} arquivos processados com sucesso\n        - ‚ùå {failed_files} arquivos com erro\n        - üìä {total_records:,} registros inseridos na base de dados\n        - ‚è±Ô∏è Tempo total: {total_time:.1f} segundos\n        \"\"\")\n\ndef process_single_csv_file_with_progress(uploaded_file, progress_bar, status_display):\n    \"\"\"Processa um √∫nico arquivo CSV com progresso em tempo real\"\"\"\n    \n    try:\n        import tempfile\n        import time\n        \n        # Fase 1: Salvando arquivo tempor√°rio\n        status_display.markdown(\"üì• Salvando arquivo tempor√°rio...\")\n        progress_bar.progress(0.1)\n        \n        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:\n            tmp_file.write(uploaded_file.getvalue())\n            tmp_path = tmp_file.name\n        \n        # Fase 2: Analisando estrutura\n        status_display.markdown(\"üîç Analisando estrutura do arquivo...\")\n        progress_bar.progress(0.2)\n        time.sleep(0.1)\n        \n        # Contagem estimada de linhas para melhor progresso\n        try:\n            with open(tmp_path, 'r', encoding='latin-1') as f:\n                estimated_rows = sum(1 for _ in f) - 1  # -1 para header\n        except:\n            estimated_rows = 1000  # Fallback\n        \n        status_display.markdown(f\"üìä Arquivo com ~{estimated_rows:,} registros detectados\")\n        progress_bar.progress(0.3)\n        \n        # Fase 3: Processando dados\n        status_display.markdown(\"‚öôÔ∏è Processando e validando dados...\")\n        progress_bar.progress(0.5)\n        time.sleep(0.2)\n        \n        # Fase 4: Inserindo na base de dados\n        status_display.markdown(\"üíæ Inserindo registros na base de dados...\")\n        progress_bar.progress(0.7)\n        \n        # Usar DatabaseManager para migrar com progresso\n        def update_progress(current, total, phase):\n            progress = 0.7 + (current / total) * 0.2  # 70% at√© 90%\n            progress_bar.progress(progress)\n            if phase == \"preparando\":\n                status_display.markdown(f\"‚öôÔ∏è Preparando dados: {current:,}/{total:,}\")\n            elif phase == \"inserindo\":\n                status_display.markdown(f\"üíæ Inserindo registros: {current:,}/{total:,}\")\n        \n        result = DatabaseManager.migrate_csv_to_database_with_progress(tmp_path, update_progress)\n        \n        # Fase 5: Finalizando\n        status_display.markdown(\"üîÑ Finalizando processamento...\")\n        progress_bar.progress(0.9)\n        time.sleep(0.1)\n        \n        # Limpar arquivo tempor√°rio\n        import os\n        os.unlink(tmp_path)\n        \n        return result\n        \n    except Exception as e:\n        status_display.markdown(f\"‚ùå Erro: {str(e)}\")\n        return {\n            'success': False,\n            'error': f'Erro no processamento: {str(e)}',\n            'records_processed': 0\n        }\n\ndef process_single_csv_file(uploaded_file):\n    \"\"\"Processa um √∫nico arquivo CSV (vers√£o simplificada para compatibilidade)\"\"\"\n    \n    try:\n        import tempfile\n        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as tmp_file:\n            tmp_file.write(uploaded_file.getvalue())\n            tmp_path = tmp_file.name\n        \n        result = DatabaseManager.migrate_csv_to_database(tmp_path)\n        \n        import os\n        os.unlink(tmp_path)\n        \n        return result\n        \n    except Exception as e:\n        return {\n            'success': False,\n            'error': f'Erro no processamento: {str(e)}',\n            'records_processed': 0\n        }\n\ndef show_processing_summary(summary, filename):\n    \"\"\"Mostra resumo do processamento\"\"\"\n    \n    st.subheader(\"üìä Resumo do Processamento\")\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\n            label=\"üìÅ Total de Registros\",\n            value=f\"{summary.get('total_registros', 0):,}\"\n        )\n    \n    with col2:\n        st.metric(\n            label=\"üöó Total de Ve√≠culos\",\n            value=f\"{summary.get('total_veiculos', 0):,}\"\n        )\n    \n    with col3:\n        st.metric(\n            label=\"üè¢ Total de Clientes\",\n            value=f\"{summary.get('total_clientes', 0):,}\"\n        )\n    \n    with col4:\n        st.metric(\n            label=\"üìÖ Per√≠odo\",\n            value=f\"{(summary.get('periodo_fim', datetime.now()) - summary.get('periodo_inicio', datetime.now())).days + 1} dias\"\n        )\n    \n    # M√©tricas adicionais\n    st.markdown(\"### üìà M√©tricas dos Dados\")\n    \n    col5, col6, col7, col8 = st.columns(4)\n    \n    with col5:\n        st.metric(\n            label=\"‚ö° Velocidade M√©dia\",\n            value=f\"{summary.get('velocidade_media', 0):.1f} km/h\"\n        )\n    \n    with col6:\n        st.metric(\n            label=\"üèéÔ∏è Velocidade M√°xima\",\n            value=f\"{summary.get('velocidade_maxima', 0):.0f} km/h\"\n        )\n    \n    with col7:\n        st.metric(\n            label=\"üõ£Ô∏è Total KM\",\n            value=f\"{summary.get('total_km_periodo', 0):.0f} km\"\n        )\n    \n    with col8:\n        st.metric(\n            label=\"üì° Com GPS\",\n            value=f\"{summary.get('registros_com_gps', 0):,}\"\n        )\n\ndef save_processing_record(filename, summary):\n    \"\"\"Salva registro do processamento\"\"\"\n    try:\n        os.makedirs('data', exist_ok=True)\n        \n        record = {\n            'timestamp': datetime.now().isoformat(),\n            'filename': filename,\n            'summary': summary\n        }\n        \n        # Carregar hist√≥rico existente\n        history_file = 'data/processing_history.csv'\n        \n        if os.path.exists(history_file):\n            history_df = pd.read_csv(history_file)\n        else:\n            history_df = pd.DataFrame()\n        \n        # Adicionar novo registro\n        new_record = pd.DataFrame([{\n            'timestamp': record['timestamp'],\n            'filename': filename,\n            'total_registros': summary.get('total_registros', 0),\n            'total_veiculos': summary.get('total_veiculos', 0),\n            'total_clientes': summary.get('total_clientes', 0),\n            'periodo_inicio': summary.get('periodo_inicio', ''),\n            'periodo_fim': summary.get('periodo_fim', ''),\n            'velocidade_media': summary.get('velocidade_media', 0),\n            'total_km': summary.get('total_km_periodo', 0)\n        }])\n        \n        history_df = pd.concat([history_df, new_record], ignore_index=True)\n        history_df.to_csv(history_file, index=False)\n        \n    except Exception as e:\n        st.warning(f\"‚ö†Ô∏è N√£o foi poss√≠vel salvar o registro: {str(e)}\")\n\ndef show_processing_history():\n    \"\"\"Mostra hist√≥rico de processamentos da base de dados\"\"\"\n    try:\n        # Tentar buscar hist√≥rico da base de dados com retry\n        history_records = None\n        retry_count = 0\n        max_retries = 3\n        \n        while retry_count < max_retries:\n            try:\n                history_records = DatabaseManager.get_processing_history()\n                break  # Success - exit retry loop\n            except Exception as retry_e:\n                retry_count += 1\n                if retry_count >= max_retries:\n                    # Se todas as tentativas falharam, mostrar mensagem amig√°vel\n                    st.warning(\"‚ö†Ô∏è Problema tempor√°rio de conex√£o com a base de dados. O hist√≥rico n√£o pode ser carregado no momento.\")\n                    if st.button(\"üîÑ Tentar Novamente\"):\n                        st.rerun()\n                    return\n                else:\n                    # Aguardar um pouco antes da pr√≥xima tentativa\n                    import time\n                    time.sleep(0.5)\n        \n        if not history_records:\n            st.info(\"üìã Nenhum hist√≥rico de processamento encontrado.\")\n            return\n        \n        st.markdown(\"### üìã Hist√≥rico de Processamento\")\n        \n        # Converter para DataFrame para exibi√ß√£o\n        history_df = pd.DataFrame(history_records)\n        \n        # Formata√ß√£o dos dados\n        if not history_df.empty:\n            # Renomear colunas para portugu√™s\n            history_df = history_df.rename(columns={\n                'filename': 'Nome do Arquivo',\n                'upload_timestamp': 'Data/Hora',\n                'records_processed': 'Registros',\n                'unique_vehicles': 'Ve√≠culos',\n                'unique_clients': 'Clientes',\n                'processing_status': 'Status',\n                'file_size_bytes': 'Tamanho (bytes)'\n            })\n            \n            # Formatar a coluna de data/hora\n            if 'Data/Hora' in history_df.columns:\n                history_df['Data/Hora'] = pd.to_datetime(history_df['Data/Hora']).dt.strftime('%d/%m/%Y %H:%M:%S')\n            \n            # Formatar status\n            if 'Status' in history_df.columns:\n                history_df['Status'] = history_df['Status'].map({\n                    'completed': '‚úÖ Conclu√≠do',\n                    'failed': '‚ùå Erro',\n                    'processing': 'üîÑ Processando'\n                }).fillna('‚ùì Desconhecido')\n            \n            # Formatar n√∫meros\n            for col in ['Registros', 'Ve√≠culos', 'Clientes']:\n                if col in history_df.columns:\n                    history_df[col] = history_df[col].apply(lambda x: f\"{x:,}\" if pd.notnull(x) else \"0\")\n            \n            # Ordenar por data mais recente\n            if 'Data/Hora' in history_df.columns:\n                history_df = history_df.sort_values('Data/Hora', ascending=False)\n            \n            # Mostrar tabela\n            st.dataframe(\n                history_df[['Nome do Arquivo', 'Data/Hora', 'Registros', 'Ve√≠culos', 'Clientes', 'Status']], \n                use_container_width=True,\n                hide_index=True\n            )\n            \n            # Estat√≠sticas gerais do hist√≥rico\n            total_records = sum([int(r.get('records_processed', 0)) for r in history_records])\n            total_files = len(history_records)\n            \n            col1, col2, col3 = st.columns(3)\n            with col1:\n                st.metric(\"üìÅ Arquivos Processados\", total_files)\n            with col2:\n                st.metric(\"üìä Total de Registros\", f\"{total_records:,}\")\n            with col3:\n                successful_files = len([r for r in history_records if r.get('processing_status') == 'completed'])\n                st.metric(\"‚úÖ Taxa de Sucesso\", f\"{(successful_files/total_files*100):.1f}%\" if total_files > 0 else \"0%\")\n        \n    except Exception as e:\n        st.error(f\"‚ùå Erro ao carregar hist√≥rico: {str(e)}\")\n        \n    # Bot√µes de a√ß√£o  \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        if st.button(\"üóëÔ∏è Limpar Hist√≥rico\", type=\"secondary\", help=\"Remove apenas os registros do hist√≥rico (mant√©m os dados)\"):\n            # Implementar limpeza do hist√≥rico se necess√°rio\n            st.info(\"Funcionalidade de limpeza de hist√≥rico ser√° implementada.\")\n    \n    with col2:\n        # Inicializar session state se n√£o existir\n        if \"confirm_clear_data\" not in st.session_state:\n            st.session_state.confirm_clear_data = False\n        \n        if st.button(\"üóÇÔ∏è Limpar Todos os Dados\", type=\"secondary\", help=\"Remove TODOS os dados da base (hist√≥rico + registros)\"):\n            st.session_state.confirm_clear_data = True\n        \n        # Mostrar confirma√ß√£o se solicitada\n        if st.session_state.confirm_clear_data:\n            st.warning(\"‚ö†Ô∏è **ATEN√á√ÉO**: Esta a√ß√£o ir√° remover TODOS os dados da base de dados!\")\n            \n            col_conf1, col_conf2 = st.columns(2)\n            \n            with col_conf1:\n                if st.button(\"‚úÖ SIM, Limpar Tudo\", type=\"primary\"):\n                    try:\n                        with st.spinner(\"üîÑ Limpando todos os dados...\"):\n                            # Clear all data from database\n                            result = DatabaseManager.clear_all_data()\n                        \n                        st.success(f\"\"\"\n                        üéâ **Limpeza completa realizada com sucesso!**\n                        \n                        **Dados removidos:**\n                        - üóÇÔ∏è {result.get('telematics_data', 0):,} registros telem√©tricos\n                        - üìã {result.get('processing_history', 0)} registros de hist√≥rico  \n                        - üöó {result.get('vehicles', 0)} ve√≠culos\n                        - üè¢ {result.get('clients', 0)} clientes\n                        \n                        **Sistema resetado!** Agora voc√™ pode fazer novos uploads.\n                        \"\"\")\n                        \n                        # Reset session state\n                        st.session_state.confirm_clear_data = False\n                        \n                        # Recarregar a p√°gina para refletir mudan√ßas\n                        st.rerun()\n                        \n                    except Exception as e:\n                        st.error(f\"‚ùå Erro ao limpar dados: {str(e)}\")\n                        st.session_state.confirm_clear_data = False\n            \n            with col_conf2:\n                if st.button(\"‚ùå Cancelar\", type=\"secondary\"):\n                    st.session_state.confirm_clear_data = False\n                    st.rerun()\n\ndef show_processing_history_old():\n    \"\"\"Mostra hist√≥rico de processamentos (vers√£o antiga usando arquivo)\"\"\"\n    history_file = 'data/processing_history.csv'\n    \n    if not os.path.exists(history_file):\n        st.info(\"üìã Nenhum hist√≥rico de processamento encontrado.\")\n        return\n    \n    try:\n        history_df = pd.read_csv(history_file)\n        \n        if history_df.empty:\n            st.info(\"üìã Nenhum arquivo processado ainda.\")\n            return\n        \n        st.subheader(\"üìã Hist√≥rico de Processamentos\")\n        \n        # Ordenar por timestamp mais recente\n        history_df['timestamp'] = pd.to_datetime(history_df['timestamp'])\n        history_df = history_df.sort_values('timestamp', ascending=False)\n        \n        # Formatar para exibi√ß√£o\n        display_df = history_df.copy()\n        display_df['timestamp'] = display_df['timestamp'].dt.strftime('%d/%m/%Y %H:%M')\n        display_df = display_df.rename(columns={\n            'timestamp': 'Data/Hora',\n            'filename': 'Arquivo',\n            'total_registros': 'Registros',\n            'total_veiculos': 'Ve√≠culos',\n            'total_clientes': 'Clientes',\n            'velocidade_media': 'Vel. M√©dia',\n            'total_km': 'Total KM'\n        })\n        \n        # Selecionar colunas para exibi√ß√£o\n        display_columns = ['Data/Hora', 'Arquivo', 'Registros', 'Ve√≠culos', 'Clientes', 'Vel. M√©dia', 'Total KM']\n        \n        st.dataframe(\n            display_df[display_columns].head(10),\n            use_container_width=True,\n            hide_index=True\n        )\n        \n        # Bot√µes de a√ß√£o\n        col_btn1, col_btn2 = st.columns(2)\n        \n        with col_btn1:\n            if st.button(\"üóëÔ∏è Limpar Hist√≥rico\", type=\"secondary\"):\n                if os.path.exists(history_file):\n                    os.remove(history_file)\n                    st.success(\"‚úÖ Hist√≥rico limpo com sucesso!\")\n                    st.rerun()\n        \n        with col_btn2:\n            # Bot√£o para limpar todos os dados da base PostgreSQL\n            if st.button(\"üóÇÔ∏è Limpar Todos os Dados\", type=\"secondary\"):\n                if 'confirm_clear_all' not in st.session_state:\n                    st.session_state.confirm_clear_all = False\n                \n                if not st.session_state.confirm_clear_all:\n                    st.warning(\"‚ö†Ô∏è Isso remover√° TODOS os dados da base PostgreSQL!\")\n                    if st.button(\"‚ö†Ô∏è CONFIRMAR LIMPEZA TOTAL\", type=\"primary\"):\n                        st.session_state.confirm_clear_all = True\n                        st.rerun()\n                else:\n                    # Executar limpeza da base PostgreSQL\n                    result = DatabaseManager.clear_all_data()\n                    if result:\n                        st.success(\"‚úÖ Todos os dados foram removidos da base PostgreSQL!\")\n                        st.session_state.confirm_clear_all = False\n                        st.rerun()\n                    else:\n                        st.error(\"‚ùå Erro ao limpar dados da base PostgreSQL\")\n        \n    except Exception as e:\n        error_message = str(e)\n        if \"SSL connection has been closed unexpectedly\" in error_message:\n            st.warning(\"‚ö†Ô∏è Problema tempor√°rio de conex√£o SSL com a base de dados. Tente recarregar a p√°gina.\")\n            if st.button(\"üîÑ Recarregar P√°gina\"):\n                st.rerun()\n        elif \"psycopg2.OperationalError\" in error_message:\n            st.warning(\"‚ö†Ô∏è Problema de conex√£o com a base de dados PostgreSQL. Verifique se o sistema est√° funcionando corretamente.\")\n            if st.button(\"üîÑ Tentar Novamente\"):\n                st.rerun()\n        else:\n            st.error(f\"‚ùå Erro ao carregar hist√≥rico: {error_message}\")\n            if st.button(\"üîÑ Tentar Novamente\"):\n                st.rerun()\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":27406},"pages/3_üîç_An√°lise_Detalhada.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport sys\n\n# Adicionar o diret√≥rio raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.visualizations import FleetVisualizations\n\nst.set_page_config(\n    page_title=\"An√°lise Detalhada - Insight Hub\",\n    page_icon=\"üîç\",\n    layout=\"wide\"\n)\n\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais)\"\"\"\n    try:\n        # Importar DatabaseManager\n        from database.db_manager import DatabaseManager\n        \n        # Carregar APENAS da base de dados - sem fallbacks fict√≠cios\n        if DatabaseManager.has_data():\n            df = DatabaseManager.get_dashboard_data()\n            if not df.empty:\n                st.success(f\"‚úÖ Dados reais carregados: {len(df):,} registros da base de dados\")\n                return df\n        \n        # Se n√£o h√° dados reais, mostrar mensagem clara\n        st.warning(\"‚ö†Ô∏è Nenhum dado real encontrado na base de dados. Fa√ßa upload dos seus arquivos CSV.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados reais: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"üîç An√°lise Detalhada da Frota\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"üìÅ Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados da base de dados\n    analyzer = DataAnalyzer.from_database()\n    visualizer = FleetVisualizations(analyzer)\n    \n    # Sidebar com filtros avan√ßados\n    st.sidebar.header(\"üîç Filtros Avan√ßados\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por per√≠odo\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    data_range = st.sidebar.date_input(\n        \"Per√≠odo:\",\n        value=[min_date, max_date],\n        min_value=min_date,\n        max_value=max_date\n    )\n    \n    if len(data_range) == 2:\n        data_inicio, data_fim = data_range\n    else:\n        data_inicio = data_range[0]\n        data_fim = max_date\n    \n    # Filtro por m√∫ltiplos ve√≠culos\n    veiculos_disponiveis = []\n    if cliente_selecionado != \"Todos\":\n        veiculos_disponiveis = sorted(df[df['cliente'] == cliente_selecionado]['placa'].unique().tolist())\n    else:\n        veiculos_disponiveis = sorted(df['placa'].unique().tolist())\n    \n    veiculos_selecionados = st.sidebar.multiselect(\n        \"Ve√≠culos:\",\n        veiculos_disponiveis,\n        default=veiculos_disponiveis[:5] if len(veiculos_disponiveis) > 5 else veiculos_disponiveis\n    )\n    \n    # Filtros de velocidade\n    st.sidebar.subheader(\"‚ö° Filtros de Velocidade\")\n    velocidade_min = st.sidebar.number_input(\"Velocidade M√≠nima (km/h):\", min_value=0, max_value=200, value=0)\n    velocidade_max = st.sidebar.number_input(\"Velocidade M√°xima (km/h):\", min_value=0, max_value=200, value=200)\n    \n    # Aplicar filtros\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    # Filtrar por ve√≠culos selecionados\n    if veiculos_selecionados:\n        filtered_df = filtered_df[filtered_df['placa'].isin(veiculos_selecionados)]\n    \n    # Filtrar por velocidade\n    filtered_df = filtered_df[\n        (filtered_df['velocidade_km'] >= velocidade_min) & \n        (filtered_df['velocidade_km'] <= velocidade_max)\n    ]\n    \n    if filtered_df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Atualizar analyzer com dados filtrados\n    analyzer.filtered_df = filtered_df\n    \n    # Tabs para diferentes an√°lises\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n        \"üìä Vis√£o Geral\",\n        \"‚ö° An√°lise de Velocidade\", \n        \"üõ£Ô∏è An√°lise Operacional\",\n        \"üì° Compliance\",\n        \"‚è∞ Padr√µes Temporais\"\n    ])\n    \n    with tab1:\n        show_overview_analysis(analyzer)\n    \n    with tab2:\n        show_speed_analysis(analyzer)\n    \n    with tab3:\n        show_operational_analysis(analyzer)\n    \n    with tab4:\n        show_compliance_analysis(analyzer)\n    \n    with tab5:\n        show_temporal_patterns(analyzer)\n\ndef show_overview_analysis(analyzer):\n    \"\"\"Mostra an√°lise geral\"\"\"\n    st.header(\"üìä Vis√£o Geral dos Dados Filtrados\")\n    \n    kpis = analyzer.get_kpis()\n    \n    # Verificar se h√° KPIs v√°lidos\n    if not kpis:\n        st.warning(\"‚ö†Ô∏è N√£o foi poss√≠vel calcular m√©tricas. Verifique se h√° dados para os filtros aplicados.\")\n        return\n    \n    # M√©tricas principais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üöó Ve√≠culos\", f\"{kpis['total_veiculos']:,}\")\n    \n    with col2:\n        st.metric(\"üìä Registros\", f\"{kpis['total_registros']:,}\")\n    \n    with col3:\n        st.metric(\"‚ö° Vel. M√©dia\", f\"{kpis['velocidade_media']:.1f} km/h\")\n    \n    with col4:\n        st.metric(\"üõ£Ô∏è Dist√¢ncia\", f\"{kpis['distancia_total']:.0f} km\")\n    \n    # Gr√°ficos de distribui√ß√£o\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"üìà Atividade por Ve√≠culo\")\n        \n        vehicle_activity = analyzer.filtered_df.groupby('placa').size().sort_values(ascending=False).head(15)\n        \n        fig_activity = px.bar(\n            x=vehicle_activity.values,\n            y=vehicle_activity.index,\n            orientation='h',\n            title='Top 15 Ve√≠culos por Atividade',\n            labels={'x': 'N√∫mero de Registros', 'y': 'Placa'}\n        )\n        fig_activity.update_layout(height=500)\n        st.plotly_chart(fig_activity, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"üéØ Distribui√ß√£o de Clientes\")\n        \n        client_dist = analyzer.filtered_df['cliente'].value_counts()\n        \n        fig_clients = px.pie(\n            values=client_dist.values,\n            names=client_dist.index,\n            title='Distribui√ß√£o por Cliente'\n        )\n        fig_clients.update_layout(height=500)\n        st.plotly_chart(fig_clients, use_container_width=True)\n    \n    # Estat√≠sticas detalhadas por ve√≠culo\n    st.subheader(\"üìã Estat√≠sticas Detalhadas por Ve√≠culo\")\n    \n    vehicle_stats = analyzer.filtered_df.groupby('placa').agg({\n        'velocidade_km': ['count', 'mean', 'max', 'std'],\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.sum() / len(x)) * 100,\n        'bloqueado': lambda x: x.sum()\n    }).round(2)\n    \n    # Achatar MultiIndex\n    vehicle_stats.columns = [\n        'Registros', 'Vel. M√©dia', 'Vel. M√°xima', 'Vel. Desvio',\n        'KM Total', 'GPS (%)', 'Bloqueios'\n    ]\n    \n    vehicle_stats = vehicle_stats.sort_values('Registros', ascending=False)\n    \n    # Adicionar classifica√ß√£o de performance\n    vehicle_stats['Classifica√ß√£o'] = vehicle_stats.apply(\n        lambda row: 'üèÜ Excelente' if row['GPS (%)'] > 95 and row['Vel. M√©dia'] < 60 and row['Bloqueios'] == 0\n        else '‚úÖ Bom' if row['GPS (%)'] > 90 and row['Vel. M√©dia'] < 70\n        else '‚ö†Ô∏è Aten√ß√£o' if row['GPS (%)'] > 80\n        else '‚ùå Cr√≠tico', axis=1\n    )\n    \n    st.dataframe(\n        vehicle_stats,\n        use_container_width=True,\n        height=400\n    )\n    \n    # Download dos dados\n    csv_data = vehicle_stats.to_csv().encode('utf-8')\n    st.download_button(\n        label=\"üì• Download Estat√≠sticas CSV\",\n        data=csv_data,\n        file_name=f\"estatisticas_veiculos_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\",\n        mime=\"text/csv\"\n    )\n\ndef show_speed_analysis(analyzer):\n    \"\"\"An√°lise detalhada de velocidade\"\"\"\n    st.header(\"‚ö° An√°lise Detalhada de Velocidade\")\n    \n    speed_analysis = analyzer.get_speed_analysis()\n    \n    if not speed_analysis:\n        st.warning(\"Dados insuficientes para an√°lise de velocidade.\")\n        return\n    \n    # M√©tricas de velocidade\n    col1, col2, col3, col4 = st.columns(4)\n    \n    df = analyzer.filtered_df\n    \n    with col1:\n        st.metric(\"‚ö° Velocidade M√©dia\", f\"{df['velocidade_km'].mean():.1f} km/h\")\n    \n    with col2:\n        st.metric(\"üèéÔ∏è Velocidade M√°xima\", f\"{df['velocidade_km'].max():.0f} km/h\")\n    \n    with col3:\n        st.metric(\"üêå Velocidade M√≠nima\", f\"{df['velocidade_km'].min():.0f} km/h\")\n    \n    with col4:\n        st.metric(\"üìä Desvio Padr√£o\", f\"{df['velocidade_km'].std():.1f} km/h\")\n    \n    # Gr√°ficos de velocidade\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"üìä Distribui√ß√£o por Faixas\")\n        \n        if 'distribuicao' in speed_analysis:\n            dist_data = speed_analysis['distribuicao']\n            \n            fig_ranges = px.pie(\n                values=dist_data.values,\n                names=dist_data.index,\n                title='Distribui√ß√£o por Faixas de Velocidade'\n            )\n            st.plotly_chart(fig_ranges, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"‚è∞ Velocidade por Hora\")\n        \n        if 'velocidade_por_hora' in speed_analysis:\n            hourly_speed = speed_analysis['velocidade_por_hora']\n            \n            fig_hourly = px.line(\n                x=hourly_speed.index,\n                y=hourly_speed.values,\n                title='Velocidade M√©dia por Hora do Dia',\n                labels={'x': 'Hora', 'y': 'Velocidade M√©dia (km/h)'}\n            )\n            st.plotly_chart(fig_hourly, use_container_width=True)\n    \n    # Top ve√≠culos por velocidade\n    st.subheader(\"üèéÔ∏è Ranking de Velocidade por Ve√≠culo\")\n    \n    col_speed1, col_speed2 = st.columns(2)\n    \n    with col_speed1:\n        st.write(\"**Maiores Velocidades M√©dias:**\")\n        \n        if 'velocidade_media_por_veiculo' in speed_analysis:\n            top_speed_avg = speed_analysis['velocidade_media_por_veiculo'].head(10)\n            \n            fig_avg = px.bar(\n                x=top_speed_avg.values,\n                y=top_speed_avg.index,\n                orientation='h',\n                title='Top 10 - Velocidade M√©dia',\n                labels={'x': 'Velocidade M√©dia (km/h)', 'y': 'Placa'}\n            )\n            st.plotly_chart(fig_avg, use_container_width=True)\n    \n    with col_speed2:\n        st.write(\"**Maiores Velocidades M√°ximas:**\")\n        \n        if 'velocidade_maxima_por_veiculo' in speed_analysis:\n            top_speed_max = speed_analysis['velocidade_maxima_por_veiculo'].head(10)\n            \n            fig_max = px.bar(\n                x=top_speed_max.values,\n                y=top_speed_max.index,\n                orientation='h',\n                title='Top 10 - Velocidade M√°xima',\n                labels={'x': 'Velocidade M√°xima (km/h)', 'y': 'Placa'},\n                color=top_speed_max.values,\n                color_continuous_scale='Reds'\n            )\n            st.plotly_chart(fig_max, use_container_width=True)\n    \n    # An√°lise de excesso de velocidade\n    st.subheader(\"üö® An√°lise de Excesso de Velocidade\")\n    \n    speed_limit = st.slider(\"Definir Limite de Velocidade (km/h):\", 40, 120, 80)\n    \n    violations = df[df['velocidade_km'] > speed_limit]\n    \n    col_viol1, col_viol2, col_viol3 = st.columns(3)\n    \n    with col_viol1:\n        st.metric(\"üö® Total de Viola√ß√µes\", f\"{len(violations):,}\")\n    \n    with col_viol2:\n        st.metric(\"üìä % do Total\", f\"{(len(violations)/len(df)*100):.1f}%\")\n    \n    with col_viol3:\n        violating_vehicles = violations['placa'].nunique()\n        st.metric(\"üöó Ve√≠culos Envolvidos\", f\"{violating_vehicles:,}\")\n    \n    if not violations.empty:\n        # Viola√ß√µes por ve√≠culo\n        violations_by_vehicle = violations.groupby('placa').size().sort_values(ascending=False).head(10)\n        \n        fig_violations = px.bar(\n            x=violations_by_vehicle.values,\n            y=violations_by_vehicle.index,\n            orientation='h',\n            title=f'Top 10 - Viola√ß√µes acima de {speed_limit} km/h',\n            labels={'x': 'N√∫mero de Viola√ß√µes', 'y': 'Placa'},\n            color=violations_by_vehicle.values,\n            color_continuous_scale='Reds'\n        )\n        st.plotly_chart(fig_violations, use_container_width=True)\n\ndef show_operational_analysis(analyzer):\n    \"\"\"An√°lise operacional detalhada\"\"\"\n    st.header(\"üõ£Ô∏è An√°lise Operacional\")\n    \n    operational = analyzer.get_operational_analysis()\n    \n    if not operational:\n        st.warning(\"Dados insuficientes para an√°lise operacional.\")\n        return\n    \n    # Estat√≠sticas operacionais\n    df = analyzer.filtered_df\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        total_km = df['odometro_periodo_km'].sum()\n        st.metric(\"üõ£Ô∏è Total KM\", f\"{total_km:.0f} km\")\n    \n    with col2:\n        if 'horimetro_periodo_horas' in df.columns:\n            total_hours = df['horimetro_periodo_horas'].sum()\n        else:\n            total_hours = 0\n        st.metric(\"‚è∞ Horas Ativas\", f\"{total_hours:.1f} h\")\n    \n    with col3:\n        avg_km_per_vehicle = total_km / df['placa'].nunique() if df['placa'].nunique() > 0 else 0\n        st.metric(\"üìä KM por Ve√≠culo\", f\"{avg_km_per_vehicle:.1f} km\")\n    \n    with col4:\n        gps_coverage = (df['gps'].sum() / len(df)) * 100\n        st.metric(\"üì° Cobertura GPS\", f\"{gps_coverage:.1f}%\")\n    \n    # Gr√°ficos operacionais\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"üìà Quilometragem por Ve√≠culo\")\n        \n        km_by_vehicle = df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False).head(15)\n        \n        fig_km = px.bar(\n            x=km_by_vehicle.values,\n            y=km_by_vehicle.index,\n            orientation='h',\n            title='Top 15 - Quilometragem Total',\n            labels={'x': 'Quilometragem (km)', 'y': 'Placa'}\n        )\n        st.plotly_chart(fig_km, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"‚è∞ Utiliza√ß√£o por Hora\")\n        \n        hourly_usage = df.groupby(df['data'].dt.hour).agg({\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        }).reset_index()\n        \n        fig_hourly = px.bar(\n            hourly_usage,\n            x='data',\n            y='placa',\n            title='Ve√≠culos Ativos por Hora',\n            labels={'data': 'Hora', 'placa': 'N√∫mero de Ve√≠culos'}\n        )\n        st.plotly_chart(fig_hourly, use_container_width=True)\n    \n    # An√°lise de efici√™ncia\n    st.subheader(\"üìä An√°lise de Efici√™ncia\")\n    \n    efficiency_data = []\n    for placa in df['placa'].unique():\n        vehicle_data = df[df['placa'] == placa]\n        \n        total_records = len(vehicle_data)\n        total_km = vehicle_data['odometro_periodo_km'].sum()\n        avg_speed = vehicle_data['velocidade_km'].mean()\n        gps_coverage = (vehicle_data['gps'].mean()) * 100\n        \n        # Tempo parado\n        stopped_time = len(vehicle_data[vehicle_data['velocidade_km'] == 0]) / total_records * 100\n        \n        # Score de efici√™ncia\n        efficiency_score = (\n            (avg_speed / 80 * 30) +  # Velocidade adequada (30%)\n            (gps_coverage) * 0.3 +    # Cobertura GPS (30%)\n            ((100 - stopped_time) * 0.2) + # Tempo ativo (20%)\n            (min(total_km / 1000, 1) * 20)  # Produtividade KM (20%)\n        )\n        \n        efficiency_data.append({\n            'Placa': placa,\n            'Registros': total_records,\n            'Total KM': total_km,\n            'Vel. M√©dia': avg_speed,\n            'GPS (%)': gps_coverage,\n            'Tempo Parado (%)': stopped_time,\n            'Score Efici√™ncia': min(efficiency_score, 100)\n        })\n    \n    efficiency_df = pd.DataFrame(efficiency_data)\n    efficiency_df = efficiency_df.sort_values('Score Efici√™ncia', ascending=False)\n    \n    # Colorir por score de efici√™ncia\n    def color_efficiency(val):\n        if val >= 80:\n            return 'background-color: #d4edda'  # Verde claro\n        elif val >= 60:\n            return 'background-color: #fff3cd'  # Amarelo claro\n        else:\n            return 'background-color: #f8d7da'  # Vermelho claro\n    \n    styled_df = efficiency_df.style.applymap(color_efficiency, subset=['Score Efici√™ncia'])\n    \n    st.dataframe(styled_df, use_container_width=True, height=400)\n    \n    # Top performers\n    col_top1, col_top2 = st.columns(2)\n    \n    with col_top1:\n        st.subheader(\"üèÜ Mais Eficientes\")\n        top_efficient = efficiency_df.head(5)\n        for _, row in top_efficient.iterrows():\n            st.success(f\"**{row['Placa']}** - Score: {row['Score Efici√™ncia']:.1f}%\")\n    \n    with col_top2:\n        st.subheader(\"‚ö†Ô∏è Necessitam Aten√ß√£o\")\n        low_efficient = efficiency_df.tail(5)\n        for _, row in low_efficient.iterrows():\n            st.warning(f\"**{row['Placa']}** - Score: {row['Score Efici√™ncia']:.1f}%\")\n\ndef show_compliance_analysis(analyzer):\n    \"\"\"An√°lise de compliance\"\"\"\n    st.header(\"üì° An√°lise de Compliance\")\n    \n    compliance = analyzer.get_compliance_analysis()\n    \n    if not compliance:\n        st.warning(\"Dados insuficientes para an√°lise de compliance.\")\n        return\n    \n    # M√©tricas de compliance\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üö® Viola√ß√µes Velocidade\", f\"{compliance.get('violacoes_velocidade', 0):,}\")\n    \n    with col2:\n        st.metric(\"üì° Problemas GPS\", f\"{compliance.get('veiculos_baixo_gps', 0):,}\")\n    \n    with col3:\n        st.metric(\"üîí Ve√≠culos Bloqueados\", f\"{compliance.get('veiculos_bloqueados', 0):,}\")\n    \n    with col4:\n        if compliance.get('score_compliance'):\n            avg_score = sum(compliance['score_compliance'].values()) / len(compliance['score_compliance'])\n            st.metric(\"üìä Score M√©dio\", f\"{avg_score:.1f}%\")\n    \n    # Gr√°ficos de compliance\n    col_left, col_right = st.columns(2)\n    \n    with col_left:\n        st.subheader(\"üéØ Score de Compliance por Ve√≠culo\")\n        \n        if compliance.get('score_compliance'):\n            scores_df = pd.DataFrame(\n                list(compliance['score_compliance'].items()),\n                columns=['Placa', 'Score']\n            ).sort_values('Score', ascending=True).tail(15)\n            \n            # Definir cores baseadas no score\n            colors = ['#d62728' if score < 70 else '#ff7f0e' if score < 90 else '#2ca02c' \n                     for score in scores_df['Score']]\n            \n            fig_scores = px.bar(\n                scores_df,\n                x='Score',\n                y='Placa',\n                orientation='h',\n                title='Score de Compliance (Bottom 15)',\n                color=scores_df['Score'],\n                color_continuous_scale='RdYlGn'\n            )\n            st.plotly_chart(fig_scores, use_container_width=True)\n    \n    with col_right:\n        st.subheader(\"üì° Cobertura GPS por Ve√≠culo\")\n        \n        if 'cobertura_gps_por_veiculo' in compliance:\n            gps_coverage = compliance['cobertura_gps_por_veiculo'].head(15)\n            \n            fig_gps = px.bar(\n                x=gps_coverage.values,\n                y=gps_coverage.index,\n                orientation='h',\n                title='Cobertura GPS (Bottom 15)',\n                color=gps_coverage.values,\n                color_continuous_scale='RdYlGn',\n                labels={'x': 'Cobertura GPS (%)', 'y': 'Placa'}\n            )\n            st.plotly_chart(fig_gps, use_container_width=True)\n    \n    # Detalhes de viola√ß√µes\n    if 'detalhes_violacoes' in compliance and not compliance['detalhes_violacoes'].empty:\n        st.subheader(\"üö® Detalhes das Viola√ß√µes de Velocidade\")\n        \n        violations_detail = compliance['detalhes_violacoes'].head(15).reset_index()\n        violations_detail.columns = ['Placa', 'N√∫mero de Viola√ß√µes']\n        \n        fig_violations = px.bar(\n            violations_detail,\n            x='N√∫mero de Viola√ß√µes',\n            y='Placa',\n            orientation='h',\n            title='Top 15 - Viola√ß√µes de Velocidade',\n            color='N√∫mero de Viola√ß√µes',\n            color_continuous_scale='Reds'\n        )\n        st.plotly_chart(fig_violations, use_container_width=True)\n    \n    # Recomenda√ß√µes de compliance\n    st.subheader(\"üí° Recomenda√ß√µes de Melhoria\")\n    \n    recommendations = []\n    \n    if compliance.get('violacoes_velocidade', 0) > 0:\n        recommendations.append(\"üö® **Controle de Velocidade**: Implementar treinamento de condutores sobre limites de velocidade\")\n    \n    if compliance.get('veiculos_baixo_gps', 0) > 0:\n        recommendations.append(\"üì° **Melhoria GPS**: Verificar equipamentos e cobertura de sinal GPS\")\n    \n    if compliance.get('veiculos_bloqueados', 0) > 0:\n        recommendations.append(\"üîí **Revis√£o de Bloqueios**: Analisar motivos de bloqueio e normalizar situa√ß√£o\")\n    \n    if compliance.get('score_compliance'):\n        low_score_vehicles = [k for k, v in compliance['score_compliance'].items() if v < 70]\n        if low_score_vehicles:\n            recommendations.append(f\"‚ö†Ô∏è **Aten√ß√£o Especial**: {len(low_score_vehicles)} ve√≠culos com score baixo precisam de a√ß√£o imediata\")\n    \n    if recommendations:\n        for rec in recommendations:\n            st.info(rec)\n    else:\n        st.success(\"‚úÖ **Excelente!** A frota est√° em conformidade com os padr√µes estabelecidos.\")\n\ndef show_temporal_patterns(analyzer):\n    \"\"\"An√°lise de padr√µes temporais\"\"\"\n    st.header(\"‚è∞ Padr√µes Temporais de Uso\")\n    \n    patterns = analyzer.get_temporal_patterns()\n    df = analyzer.filtered_df\n    \n    if not patterns or df.empty:\n        st.warning(\"Dados insuficientes para an√°lise temporal.\")\n        return\n    \n    # Padr√µes por hora do dia\n    st.subheader(\"üïê Padr√µes por Hora do Dia\")\n    \n    hourly_data = df.groupby(df['data'].dt.hour).agg({\n        'placa': 'nunique',\n        'velocidade_km': 'mean',\n        'odometro_periodo_km': 'sum'\n    }).reset_index()\n    \n    col_hour1, col_hour2 = st.columns(2)\n    \n    with col_hour1:\n        fig_hourly_vehicles = px.line(\n            hourly_data,\n            x='data',\n            y='placa',\n            title='Ve√≠culos Ativos por Hora',\n            labels={'data': 'Hora', 'placa': 'N√∫mero de Ve√≠culos'}\n        )\n        st.plotly_chart(fig_hourly_vehicles, use_container_width=True)\n    \n    with col_hour2:\n        fig_hourly_speed = px.line(\n            hourly_data,\n            x='data',\n            y='velocidade_km',\n            title='Velocidade M√©dia por Hora',\n            labels={'data': 'Hora', 'velocidade_km': 'Velocidade (km/h)'}\n        )\n        st.plotly_chart(fig_hourly_speed, use_container_width=True)\n    \n    # Padr√µes por dia da semana\n    # Definir mapeamento de dias para portugu√™s (usado em m√∫ltiplos lugares)\n    dias_ordem = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    dias_pt = ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo']\n    \n    if len(df['data'].dt.date.unique()) > 7:  # Mais de uma semana de dados\n        st.subheader(\"üìÖ Padr√µes por Dia da Semana\")\n        \n        df['dia_semana'] = df['data'].dt.day_name()\n        \n        weekly_data = df.groupby('dia_semana').agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean',\n            'odometro_periodo_km': 'sum'\n        }).reindex(dias_ordem).reset_index()\n        \n        weekly_data['dia_semana'] = dias_pt\n        \n        col_week1, col_week2 = st.columns(2)\n        \n        with col_week1:\n            fig_weekly_vehicles = px.bar(\n                weekly_data,\n                x='dia_semana',\n                y='placa',\n                title='Ve√≠culos Ativos por Dia da Semana',\n                labels={'dia_semana': 'Dia da Semana', 'placa': 'N√∫mero de Ve√≠culos'}\n            )\n            st.plotly_chart(fig_weekly_vehicles, use_container_width=True)\n        \n        with col_week2:\n            fig_weekly_km = px.bar(\n                weekly_data,\n                x='dia_semana',\n                y='odometro_periodo_km',\n                title='Quilometragem por Dia da Semana',\n                labels={'dia_semana': 'Dia da Semana', 'odometro_periodo_km': 'KM Total'}\n            )\n            st.plotly_chart(fig_weekly_km, use_container_width=True)\n    \n    # Heatmap de atividade\n    st.subheader(\"üî• Mapa de Calor - Atividade por Hora e Dia\")\n    \n    # Criar dados para heatmap\n    df['hora'] = df['data'].dt.hour\n    df['dia'] = df['data'].dt.day_name()\n    \n    heatmap_data = df.groupby(['dia', 'hora']).size().unstack(fill_value=0)\n    \n    # Reordenar dias da semana\n    dias_ordem = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    heatmap_data = heatmap_data.reindex(dias_ordem)\n    heatmap_data.index = dias_pt\n    \n    fig_heatmap = px.imshow(\n        heatmap_data,\n        title='Atividade da Frota (Registros por Hora/Dia)',\n        labels={'x': 'Hora do Dia', 'y': 'Dia da Semana', 'color': 'N√∫mero de Registros'},\n        aspect='auto'\n    )\n    \n    st.plotly_chart(fig_heatmap, use_container_width=True)\n    \n    # Insights temporais\n    st.subheader(\"üí° Insights Temporais\")\n    \n    # Pico de atividade\n    peak_hour = hourly_data.loc[hourly_data['placa'].idxmax(), 'data']\n    peak_vehicles = hourly_data['placa'].max()\n    \n    st.info(f\"üïê **Pico de Atividade**: {peak_hour}h com {peak_vehicles} ve√≠culos ativos\")\n    \n    # Per√≠odo de menor atividade\n    low_hour = hourly_data.loc[hourly_data['placa'].idxmin(), 'data']\n    low_vehicles = hourly_data['placa'].min()\n    \n    st.info(f\"üò¥ **Menor Atividade**: {low_hour}h com {low_vehicles} ve√≠culos ativos\")\n    \n    # Hor√°rio de maior velocidade m√©dia\n    fastest_hour = hourly_data.loc[hourly_data['velocidade_km'].idxmax(), 'data']\n    fastest_speed = hourly_data['velocidade_km'].max()\n    \n    st.info(f\"üèéÔ∏è **Maior Velocidade M√©dia**: {fastest_hour}h com {fastest_speed:.1f} km/h\")\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":26415},"pages/4_üìà_Compara√ß√£o_Ve√≠culos.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport sys\n\n# Adicionar o diret√≥rio raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.visualizations import FleetVisualizations\n\nst.set_page_config(\n    page_title=\"Compara√ß√£o de Ve√≠culos - Insight Hub\",\n    page_icon=\"üìà\",\n    layout=\"wide\"\n)\n\n@st.cache_data(ttl=300)  # Cache por 5 minutos\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais) com otimiza√ß√µes\"\"\"\n    try:\n        # Importar DatabaseManager\n        from database.db_manager import DatabaseManager\n        \n        # Carregar TODOS os dados por padr√£o conforme solicitado pelo usu√°rio\n        with st.spinner(\"üîÑ Carregando todos os dados para compara√ß√£o...\"):\n            df = DatabaseManager.get_dashboard_data()\n            \n            if not df.empty:\n                st.success(f\"‚úÖ Todos os dados carregados: {len(df):,} registros\")\n                return df\n        \n        st.warning(\"‚ö†Ô∏è Nenhum dado encontrado na base de dados.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"üìà Compara√ß√£o de Ve√≠culos\")\n    st.markdown(\"An√°lise comparativa detalhada entre ve√≠culos da frota\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"üìÅ Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados carregados (mais eficiente)\n    analyzer = DataAnalyzer(df)\n    visualizer = FleetVisualizations(analyzer)\n    \n    # Sidebar com filtros\n    st.sidebar.header(\"üîç Filtros para Compara√ß√£o\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por per√≠odo\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    data_range = st.sidebar.date_input(\n        \"Per√≠odo:\",\n        value=[min_date, max_date],\n        min_value=min_date,\n        max_value=max_date\n    )\n    \n    if len(data_range) == 2:\n        data_inicio, data_fim = data_range\n    else:\n        data_inicio = data_range[0]\n        data_fim = max_date\n    \n    # Aplicar filtros iniciais\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    if filtered_df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Sele√ß√£o de ve√≠culos para compara√ß√£o\n    st.subheader(\"üöó Sele√ß√£o de Ve√≠culos para Compara√ß√£o\")\n    \n    # Mostrar estat√≠sticas r√°pidas dos ve√≠culos dispon√≠veis\n    vehicle_stats = filtered_df.groupby('placa').agg({\n        'velocidade_km': ['count', 'mean'],\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.mean() * 100)\n    }).round(2)\n    \n    vehicle_stats.columns = ['Registros', 'Vel. M√©dia', 'KM Total', 'GPS (%)']\n    vehicle_stats = vehicle_stats.sort_values('Registros', ascending=False)\n    \n    col_selection1, col_selection2 = st.columns([2, 1])\n    \n    with col_selection1:\n        # Multiselect para escolher ve√≠culos\n        veiculos_disponiveis = vehicle_stats.index.tolist()\n        \n        if len(veiculos_disponiveis) < 2:\n            st.error(\"‚ùå Pelo menos 2 ve√≠culos s√£o necess√°rios para compara√ß√£o.\")\n            st.stop()\n        \n        # Sugerir ve√≠culos com mais atividade como padr√£o\n        default_vehicles = veiculos_disponiveis[:min(5, len(veiculos_disponiveis))]\n        \n        veiculos_selecionados = st.multiselect(\n            \"Selecione os ve√≠culos para comparar:\",\n            veiculos_disponiveis,\n            default=default_vehicles,\n            help=\"Selecione entre 2 e 10 ve√≠culos para compara√ß√£o\"\n        )\n        \n        if len(veiculos_selecionados) < 2:\n            st.warning(\"‚ö†Ô∏è Selecione pelo menos 2 ve√≠culos para compara√ß√£o.\")\n            st.stop()\n        \n        if len(veiculos_selecionados) > 10:\n            st.warning(\"‚ö†Ô∏è M√°ximo de 10 ve√≠culos permitidos para melhor visualiza√ß√£o.\")\n            veiculos_selecionados = veiculos_selecionados[:10]\n    \n    with col_selection2:\n        st.subheader(\"üìä Ve√≠culos Dispon√≠veis\")\n        st.dataframe(\n            vehicle_stats.head(10),\n            use_container_width=True,\n            height=300\n        )\n    \n    # Filtrar dados para ve√≠culos selecionados\n    comparison_df = filtered_df[filtered_df['placa'].isin(veiculos_selecionados)]\n    analyzer.filtered_df = comparison_df\n    \n    # Executar compara√ß√£o\n    comparison_data = analyzer.compare_vehicles(veiculos_selecionados)\n    \n    if not comparison_data:\n        st.error(\"‚ùå N√£o foi poss√≠vel gerar dados de compara√ß√£o.\")\n        st.stop()\n    \n    # Tabs para diferentes tipos de compara√ß√£o\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n        \"üìä Vis√£o Geral\",\n        \"‚ö° Performance\",\n        \"üõ£Ô∏è Operacional\",\n        \"üì° Qualidade\",\n        \"üìà Gr√°ficos Detalhados\"\n    ])\n    \n    with tab1:\n        show_overview_comparison(comparison_data, comparison_df)\n    \n    with tab2:\n        show_performance_comparison(comparison_data, comparison_df)\n    \n    with tab3:\n        show_operational_comparison(comparison_data, comparison_df)\n    \n    with tab4:\n        show_quality_comparison(comparison_data, comparison_df)\n    \n    with tab5:\n        show_detailed_charts(comparison_data, comparison_df, analyzer)\n\ndef show_overview_comparison(comparison_data, df):\n    \"\"\"Mostra compara√ß√£o geral entre ve√≠culos\"\"\"\n    st.header(\"üìä Vis√£o Geral Comparativa\")\n    \n    # Converter dados para DataFrame\n    comparison_df = pd.DataFrame(comparison_data).T\n    comparison_df = comparison_df.round(2)\n    \n    # M√©tricas principais em colunas\n    st.subheader(\"üìà M√©tricas Principais\")\n    \n    metrics_to_show = [\n        ('total_registros', 'Total de Registros'),\n        ('velocidade_media', 'Velocidade M√©dia (km/h)'),\n        ('distancia_total', 'Dist√¢ncia Total (km)'),\n        ('tempo_ativo', 'Tempo Ativo (h)'),\n        ('cobertura_gps', 'Cobertura GPS (%)'),\n        ('violacoes_velocidade', 'Viola√ß√µes de Velocidade'),\n        ('bloqueios', 'Bloqueios')\n    ]\n    \n    # Criar DataFrame para exibi√ß√£o\n    display_data = []\n    for placa, data in comparison_data.items():\n        row = {'Placa': placa}\n        for metric_key, metric_name in metrics_to_show:\n            if metric_key in data:\n                value = data[metric_key]\n                if metric_key in ['velocidade_media', 'cobertura_gps']:\n                    row[metric_name] = f\"{value:.1f}\"\n                elif metric_key in ['distancia_total', 'tempo_ativo']:\n                    row[metric_name] = f\"{value:.1f}\"\n                else:\n                    row[metric_name] = f\"{int(value):,}\"\n            else:\n                row[metric_name] = \"N/A\"\n        display_data.append(row)\n    \n    display_df = pd.DataFrame(display_data)\n    \n    # Estilizar tabela com cores baseadas na performance\n    def highlight_best_worst(s):\n        \"\"\"Destaca melhores e piores valores\"\"\"\n        if s.name == 'Placa':\n            return [''] * len(s)\n        \n        # Converter para num√©rico, ignorando strings\n        numeric_values = []\n        for val in s:\n            try:\n                numeric_values.append(float(val.replace(',', '')) if isinstance(val, str) and val != 'N/A' else float(val))\n            except:\n                numeric_values.append(0)\n        \n        if not numeric_values or all(v == 0 for v in numeric_values):\n            return [''] * len(s)\n        \n        # Para viola√ß√µes e bloqueios, menor √© melhor\n        if 'Viola√ß√µes' in s.name or 'Bloqueios' in s.name:\n            min_val = min(numeric_values)\n            colors = ['background-color: #d4edda' if v == min_val and v == 0 \n                     else 'background-color: #f8d7da' if v == max(numeric_values) and v > 0\n                     else '' for v in numeric_values]\n        else:\n            # Para outras m√©tricas, maior √© melhor\n            max_val = max(numeric_values)\n            min_val = min(numeric_values)\n            colors = ['background-color: #d4edda' if v == max_val \n                     else 'background-color: #fff3cd' if v == min_val\n                     else '' for v in numeric_values]\n        \n        return colors\n    \n    styled_df = display_df.style.apply(highlight_best_worst, axis=0)\n    st.dataframe(styled_df, use_container_width=True, hide_index=True)\n    \n    # Ranking geral\n    st.subheader(\"üèÜ Ranking Geral\")\n    \n    # Calcular score geral para cada ve√≠culo\n    scores = {}\n    for placa, data in comparison_data.items():\n        # Normalizar m√©tricas (0-100)\n        vel_score = max(0, 100 - abs(data.get('velocidade_media', 50) - 50))  # Ideal ~50 km/h\n        gps_score = data.get('cobertura_gps', 0)\n        dist_score = min(100, (data.get('distancia_total', 0) / 1000) * 100)  # Normalizar por 1000km\n        \n        # Penalizar viola√ß√µes e bloqueios\n        violation_penalty = min(50, data.get('violacoes_velocidade', 0) * 5)\n        block_penalty = data.get('bloqueios', 0) * 20\n        \n        # Score final\n        final_score = (vel_score * 0.2 + gps_score * 0.3 + dist_score * 0.3 + \n                      (100 - violation_penalty) * 0.1 + (100 - block_penalty) * 0.1)\n        \n        scores[placa] = max(0, min(100, final_score))\n    \n    # Ordenar por score\n    sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n    \n    col_rank1, col_rank2 = st.columns(2)\n    \n    with col_rank1:\n        st.write(\"**ü•á Top Performers:**\")\n        for i, (placa, score) in enumerate(sorted_scores[:3]):\n            medal = [\"ü•á\", \"ü•à\", \"ü•â\"][i]\n            st.success(f\"{medal} **{placa}** - Score: {score:.1f}/100\")\n    \n    with col_rank2:\n        st.write(\"**‚ö†Ô∏è Necessitam Aten√ß√£o:**\")\n        for placa, score in sorted_scores[-3:]:\n            if score < 70:\n                st.warning(f\"‚ö†Ô∏è **{placa}** - Score: {score:.1f}/100\")\n            else:\n                st.info(f\"üìä **{placa}** - Score: {score:.1f}/100\")\n    \n    # Gr√°fico de radar comparativo\n    st.subheader(\"üéØ Compara√ß√£o em Radar\")\n    \n    # Preparar dados para gr√°fico de radar\n    categories = ['Velocidade', 'Cobertura GPS', 'Produtividade', 'Conformidade']\n    \n    fig = go.Figure()\n    \n    colors = px.colors.qualitative.Set3\n    \n    for i, (placa, data) in enumerate(comparison_data.items()):\n        # Normalizar valores para 0-100\n        vel_norm = max(0, 100 - abs(data.get('velocidade_media', 50) - 50))\n        gps_norm = data.get('cobertura_gps', 0)\n        prod_norm = min(100, (data.get('distancia_total', 0) / 1000) * 100)\n        conf_norm = max(0, 100 - (data.get('violacoes_velocidade', 0) * 10))\n        \n        values = [vel_norm, gps_norm, prod_norm, conf_norm]\n        \n        fig.add_trace(go.Scatterpolar(\n            r=values + [values[0]],  # Fechar o pol√≠gono\n            theta=categories + [categories[0]],\n            fill='toself',\n            name=placa,\n            line_color=colors[i % len(colors)]\n        ))\n    \n    fig.update_layout(\n        polar=dict(\n            radialaxis=dict(\n                visible=True,\n                range=[0, 100]\n            )),\n        showlegend=True,\n        title=\"Compara√ß√£o Multidimensional de Performance\",\n        height=500\n    )\n    \n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_performance_comparison(comparison_data, df):\n    \"\"\"Mostra compara√ß√£o de performance\"\"\"\n    st.header(\"‚ö° Compara√ß√£o de Performance\")\n    \n    # Gr√°ficos de velocidade\n    col_perf1, col_perf2 = st.columns(2)\n    \n    with col_perf1:\n        st.subheader(\"üìä Velocidade M√©dia\")\n        \n        vel_data = {placa: data['velocidade_media'] for placa, data in comparison_data.items()}\n        \n        fig_vel = px.bar(\n            x=list(vel_data.keys()),\n            y=list(vel_data.values()),\n            title='Velocidade M√©dia por Ve√≠culo',\n            labels={'x': 'Placa', 'y': 'Velocidade M√©dia (km/h)'},\n            color=list(vel_data.values()),\n            color_continuous_scale='Viridis'\n        )\n        fig_vel.update_layout(height=400)\n        st.plotly_chart(fig_vel, use_container_width=True)\n    \n    with col_perf2:\n        st.subheader(\"üõ£Ô∏è Dist√¢ncia Total\")\n        \n        dist_data = {placa: data['distancia_total'] for placa, data in comparison_data.items()}\n        \n        fig_dist = px.bar(\n            x=list(dist_data.keys()),\n            y=list(dist_data.values()),\n            title='Dist√¢ncia Total por Ve√≠culo',\n            labels={'x': 'Placa', 'y': 'Dist√¢ncia Total (km)'},\n            color=list(dist_data.values()),\n            color_continuous_scale='Blues'\n        )\n        fig_dist.update_layout(height=400)\n        st.plotly_chart(fig_dist, use_container_width=True)\n    \n    # An√°lise de distribui√ß√£o de velocidade por ve√≠culo\n    st.subheader(\"üìà Distribui√ß√£o de Velocidade por Ve√≠culo\")\n    \n    fig_box = px.box(\n        df,\n        x='placa',\n        y='velocidade_km',\n        title='Distribui√ß√£o de Velocidade (Box Plot)',\n        labels={'placa': 'Placa', 'velocidade_km': 'Velocidade (km/h)'}\n    )\n    fig_box.update_layout(height=400)\n    st.plotly_chart(fig_box, use_container_width=True)\n    \n    # M√©tricas de efici√™ncia\n    st.subheader(\"üéØ M√©tricas de Efici√™ncia\")\n    \n    efficiency_data = []\n    for placa, data in comparison_data.items():\n        if data['tempo_ativo'] > 0:\n            km_por_hora = data['distancia_total'] / data['tempo_ativo']\n        else:\n            km_por_hora = 0\n        \n        efficiency_data.append({\n            'Placa': placa,\n            'KM por Hora': round(km_por_hora, 2),\n            'Registros por Dia': round(data['total_registros'] / 30, 1),  # Assumindo ~30 dias\n            'Utiliza√ß√£o (%)': min(100, round((data['tempo_ativo'] / (30 * 24)) * 100, 1))\n        })\n    \n    efficiency_df = pd.DataFrame(efficiency_data)\n    efficiency_df = efficiency_df.sort_values('KM por Hora', ascending=False)\n    \n    st.dataframe(efficiency_df, use_container_width=True, hide_index=True)\n    \n    # Gr√°fico de efici√™ncia\n    fig_eff = px.scatter(\n        efficiency_df,\n        x='KM por Hora',\n        y='Utiliza√ß√£o (%)',\n        text='Placa',\n        title='Efici√™ncia: KM/Hora vs Utiliza√ß√£o',\n        labels={'KM por Hora': 'Quil√¥metros por Hora', 'Utiliza√ß√£o (%)': 'Utiliza√ß√£o (%)'}\n    )\n    fig_eff.update_traces(textposition=\"top center\")\n    fig_eff.update_layout(height=400)\n    st.plotly_chart(fig_eff, use_container_width=True)\n\ndef show_operational_comparison(comparison_data, df):\n    \"\"\"Mostra compara√ß√£o operacional\"\"\"\n    st.header(\"üõ£Ô∏è Compara√ß√£o Operacional\")\n    \n    # Tempo ativo vs dist√¢ncia\n    col_op1, col_op2 = st.columns(2)\n    \n    with col_op1:\n        st.subheader(\"‚è∞ Tempo Ativo\")\n        \n        tempo_data = {placa: data['tempo_ativo'] for placa, data in comparison_data.items()}\n        \n        fig_tempo = px.bar(\n            x=list(tempo_data.keys()),\n            y=list(tempo_data.values()),\n            title='Tempo Ativo por Ve√≠culo',\n            labels={'x': 'Placa', 'y': 'Tempo Ativo (horas)'},\n            color=list(tempo_data.values()),\n            color_continuous_scale='Oranges'\n        )\n        fig_tempo.update_layout(height=400)\n        st.plotly_chart(fig_tempo, use_container_width=True)\n    \n    with col_op2:\n        st.subheader(\"üìä Total de Registros\")\n        \n        registros_data = {placa: data['total_registros'] for placa, data in comparison_data.items()}\n        \n        fig_reg = px.bar(\n            x=list(registros_data.keys()),\n            y=list(registros_data.values()),\n            title='Total de Registros por Ve√≠culo',\n            labels={'x': 'Placa', 'y': 'Total de Registros'},\n            color=list(registros_data.values()),\n            color_continuous_scale='Greens'\n        )\n        fig_reg.update_layout(height=400)\n        st.plotly_chart(fig_reg, use_container_width=True)\n    \n    # An√°lise temporal de atividade\n    st.subheader(\"‚è∞ Padr√µes de Atividade por Hora\")\n    \n    # Atividade por hora para cada ve√≠culo\n    hourly_activity = df.groupby(['placa', df['data'].dt.hour]).size().unstack(fill_value=0)\n    \n    # Criar heatmap\n    fig_heatmap = px.imshow(\n        hourly_activity,\n        title='Atividade por Hora (Registros)',\n        labels={'x': 'Hora do Dia', 'y': 'Placa', 'color': 'N√∫mero de Registros'},\n        aspect='auto'\n    )\n    fig_heatmap.update_layout(height=500)\n    st.plotly_chart(fig_heatmap, use_container_width=True)\n    \n    # An√°lise de picos de atividade\n    st.subheader(\"üìà An√°lise de Picos de Atividade\")\n    \n    peak_analysis = []\n    for placa in df['placa'].unique():\n        vehicle_data = df[df['placa'] == placa]\n        hourly_counts = vehicle_data.groupby(vehicle_data['data'].dt.hour).size()\n        \n        if not hourly_counts.empty:\n            peak_hour = hourly_counts.idxmax()\n            peak_count = hourly_counts.max()\n            avg_count = hourly_counts.mean()\n            \n            peak_analysis.append({\n                'Placa': placa,\n                'Hora de Pico': f\"{peak_hour}h\",\n                'Registros no Pico': peak_count,\n                'M√©dia por Hora': round(avg_count, 1),\n                'Intensidade do Pico': round(peak_count / avg_count, 1) if avg_count > 0 else 0\n            })\n    \n    peak_df = pd.DataFrame(peak_analysis)\n    peak_df = peak_df.sort_values('Intensidade do Pico', ascending=False)\n    \n    st.dataframe(peak_df, use_container_width=True, hide_index=True)\n\ndef show_quality_comparison(comparison_data, df):\n    \"\"\"Mostra compara√ß√£o de qualidade dos dados\"\"\"\n    st.header(\"üì° Compara√ß√£o de Qualidade\")\n    \n    # M√©tricas de qualidade\n    col_qual1, col_qual2 = st.columns(2)\n    \n    with col_qual1:\n        st.subheader(\"üì° Cobertura GPS\")\n        \n        gps_data = {placa: data['cobertura_gps'] for placa, data in comparison_data.items()}\n        \n        fig_gps = px.bar(\n            x=list(gps_data.keys()),\n            y=list(gps_data.values()),\n            title='Cobertura GPS por Ve√≠culo (%)',\n            labels={'x': 'Placa', 'y': 'Cobertura GPS (%)'},\n            color=list(gps_data.values()),\n            color_continuous_scale='RdYlGn'\n        )\n        fig_gps.update_layout(height=400)\n        # Adicionar linha de refer√™ncia para 95%\n        fig_gps.add_hline(y=95, line_dash=\"dash\", line_color=\"red\", \n                         annotation_text=\"Meta: 95%\")\n        st.plotly_chart(fig_gps, use_container_width=True)\n    \n    with col_qual2:\n        st.subheader(\"üö® Viola√ß√µes de Velocidade\")\n        \n        violacoes_data = {placa: data['violacoes_velocidade'] for placa, data in comparison_data.items()}\n        \n        fig_viol = px.bar(\n            x=list(violacoes_data.keys()),\n            y=list(violacoes_data.values()),\n            title='Viola√ß√µes de Velocidade por Ve√≠culo',\n            labels={'x': 'Placa', 'y': 'N√∫mero de Viola√ß√µes'},\n            color=list(violacoes_data.values()),\n            color_continuous_scale='Reds'\n        )\n        fig_viol.update_layout(height=400)\n        st.plotly_chart(fig_viol, use_container_width=True)\n    \n    # Score de qualidade combinado\n    st.subheader(\"üéØ Score de Qualidade Geral\")\n    \n    quality_scores = []\n    for placa, data in comparison_data.items():\n        # Calcular score de qualidade (0-100)\n        gps_score = data['cobertura_gps']\n        violation_penalty = min(50, data['violacoes_velocidade'] * 5)  # M√°ximo 50 pontos de penalidade\n        block_penalty = data['bloqueios'] * 20  # 20 pontos por bloqueio\n        \n        final_score = max(0, gps_score - violation_penalty - block_penalty)\n        \n        quality_scores.append({\n            'Placa': placa,\n            'Cobertura GPS (%)': round(data['cobertura_gps'], 1),\n            'Viola√ß√µes': data['violacoes_velocidade'],\n            'Bloqueios': data['bloqueios'],\n            'Score de Qualidade': round(final_score, 1)\n        })\n    \n    quality_df = pd.DataFrame(quality_scores)\n    quality_df = quality_df.sort_values('Score de Qualidade', ascending=False)\n    \n    # Colorir por score\n    def color_quality_score(val):\n        if val >= 90:\n            return 'background-color: #d4edda'  # Verde\n        elif val >= 70:\n            return 'background-color: #fff3cd'  # Amarelo\n        else:\n            return 'background-color: #f8d7da'  # Vermelho\n    \n    styled_quality = quality_df.style.applymap(\n        color_quality_score, \n        subset=['Score de Qualidade']\n    )\n    \n    st.dataframe(styled_quality, use_container_width=True, hide_index=True)\n    \n    # Gr√°fico de score de qualidade\n    fig_quality = px.bar(\n        quality_df,\n        x='Placa',\n        y='Score de Qualidade',\n        title='Score de Qualidade por Ve√≠culo',\n        labels={'Score de Qualidade': 'Score de Qualidade (0-100)'},\n        color='Score de Qualidade',\n        color_continuous_scale='RdYlGn'\n    )\n    fig_quality.update_layout(height=400)\n    # Linha de meta para 80%\n    fig_quality.add_hline(y=80, line_dash=\"dash\", line_color=\"orange\", \n                         annotation_text=\"Meta: 80%\")\n    st.plotly_chart(fig_quality, use_container_width=True)\n\ndef show_detailed_charts(comparison_data, df, analyzer):\n    \"\"\"Mostra gr√°ficos detalhados\"\"\"\n    st.header(\"üìà Gr√°ficos Detalhados\")\n    \n    # Seletor de tipo de gr√°fico\n    chart_type = st.selectbox(\n        \"Selecione o tipo de an√°lise:\",\n        [\n            \"Evolu√ß√£o Temporal\",\n            \"Compara√ß√£o de Velocidade\",\n            \"An√°lise de Correla√ß√£o\",\n            \"Dispers√£o Multivariada\"\n        ]\n    )\n    \n    if chart_type == \"Evolu√ß√£o Temporal\":\n        show_temporal_evolution(df)\n    elif chart_type == \"Compara√ß√£o de Velocidade\":\n        show_speed_comparison_charts(df)\n    elif chart_type == \"An√°lise de Correla√ß√£o\":\n        show_correlation_analysis(df)\n    elif chart_type == \"Dispers√£o Multivariada\":\n        show_multivariate_scatter(comparison_data)\n\ndef show_temporal_evolution(df):\n    \"\"\"Mostra evolu√ß√£o temporal dos ve√≠culos\"\"\"\n    st.subheader(\"‚è∞ Evolu√ß√£o Temporal\")\n    \n    # Agregar dados por dia\n    daily_data = df.groupby(['placa', df['data'].dt.date]).agg({\n        'velocidade_km': 'mean',\n        'odometro_periodo_km': 'sum',\n        'gps': lambda x: (x.mean() * 100)\n    }).reset_index()\n    \n    # Gr√°fico de velocidade ao longo do tempo\n    fig_temporal = px.line(\n        daily_data,\n        x='data',\n        y='velocidade_km',\n        color='placa',\n        title='Evolu√ß√£o da Velocidade M√©dia por Dia',\n        labels={'data': 'Data', 'velocidade_km': 'Velocidade M√©dia (km/h)'}\n    )\n    fig_temporal.update_layout(height=500)\n    st.plotly_chart(fig_temporal, use_container_width=True)\n    \n    # Gr√°fico de quilometragem acumulada\n    daily_data['km_acumulado'] = daily_data.groupby('placa')['odometro_periodo_km'].cumsum()\n    \n    fig_cumulative = px.line(\n        daily_data,\n        x='data',\n        y='km_acumulado',\n        color='placa',\n        title='Quilometragem Acumulada por Dia',\n        labels={'data': 'Data', 'km_acumulado': 'KM Acumulado'}\n    )\n    fig_cumulative.update_layout(height=500)\n    st.plotly_chart(fig_cumulative, use_container_width=True)\n\ndef show_speed_comparison_charts(df):\n    \"\"\"Mostra gr√°ficos de compara√ß√£o de velocidade\"\"\"\n    st.subheader(\"‚ö° Compara√ß√£o Detalhada de Velocidade\")\n    \n    # Histograma comparativo\n    fig_hist = px.histogram(\n        df,\n        x='velocidade_km',\n        color='placa',\n        nbins=30,\n        title='Distribui√ß√£o de Velocidade por Ve√≠culo',\n        labels={'velocidade_km': 'Velocidade (km/h)', 'count': 'Frequ√™ncia'},\n        opacity=0.7\n    )\n    fig_hist.update_layout(height=500)\n    st.plotly_chart(fig_hist, use_container_width=True)\n    \n    # Violin plot\n    fig_violin = px.violin(\n        df,\n        x='placa',\n        y='velocidade_km',\n        title='Distribui√ß√£o de Velocidade (Violin Plot)',\n        labels={'placa': 'Placa', 'velocidade_km': 'Velocidade (km/h)'}\n    )\n    fig_violin.update_layout(height=500)\n    st.plotly_chart(fig_violin, use_container_width=True)\n\ndef show_correlation_analysis(df):\n    \"\"\"Mostra an√°lise de correla√ß√£o\"\"\"\n    st.subheader(\"üîó An√°lise de Correla√ß√£o\")\n    \n    # Calcular correla√ß√µes por ve√≠culo\n    correlation_data = []\n    \n    for placa in df['placa'].unique():\n        vehicle_data = df[df['placa'] == placa]\n        \n        if len(vehicle_data) > 10:  # M√≠nimo de dados para correla√ß√£o\n            # Adicionar vari√°veis temporais\n            vehicle_data = vehicle_data.copy()\n            vehicle_data['hora'] = vehicle_data['data'].dt.hour\n            vehicle_data['dia_semana'] = vehicle_data['data'].dt.dayofweek\n            \n            # Correla√ß√£o entre velocidade e hora\n            corr_vel_hora = vehicle_data['velocidade_km'].corr(vehicle_data['hora'])\n            \n            # Correla√ß√£o entre velocidade e GPS\n            corr_vel_gps = vehicle_data['velocidade_km'].corr(vehicle_data['gps'].astype(int))\n            \n            correlation_data.append({\n                'Placa': placa,\n                'Velocidade vs Hora': round(corr_vel_hora, 3),\n                'Velocidade vs GPS': round(corr_vel_gps, 3),\n                'Registros': len(vehicle_data)\n            })\n    \n    if correlation_data:\n        corr_df = pd.DataFrame(correlation_data)\n        st.dataframe(corr_df, use_container_width=True, hide_index=True)\n        \n        # Gr√°fico de scatter: velocidade vs hora para todos os ve√≠culos\n        fig_scatter = px.scatter(\n            df,\n            x=df['data'].dt.hour,\n            y='velocidade_km',\n            color='placa',\n            title='Velocidade vs Hora do Dia',\n            labels={'x': 'Hora do Dia', 'velocidade_km': 'Velocidade (km/h)'},\n            opacity=0.6\n        )\n        fig_scatter.update_layout(height=500)\n        st.plotly_chart(fig_scatter, use_container_width=True)\n\ndef show_multivariate_scatter(comparison_data):\n    \"\"\"Mostra an√°lise multivariada\"\"\"\n    st.subheader(\"üéØ An√°lise Multivariada\")\n    \n    # Converter para DataFrame\n    multi_df = pd.DataFrame(comparison_data).T.reset_index()\n    multi_df.columns = ['Placa'] + list(multi_df.columns[1:])\n    \n    # Scatter plot multidimensional\n    x_axis = st.selectbox(\"Eixo X:\", ['velocidade_media', 'distancia_total', 'tempo_ativo', 'cobertura_gps'])\n    y_axis = st.selectbox(\"Eixo Y:\", ['cobertura_gps', 'velocidade_media', 'distancia_total', 'tempo_ativo'])\n    size_var = st.selectbox(\"Tamanho:\", ['total_registros', 'distancia_total', 'tempo_ativo'])\n    color_var = st.selectbox(\"Cor:\", ['violacoes_velocidade', 'cobertura_gps', 'bloqueios'])\n    \n    fig_multi = px.scatter(\n        multi_df,\n        x=x_axis,\n        y=y_axis,\n        size=size_var,\n        color=color_var,\n        text='Placa',\n        title=f'An√°lise Multivariada: {x_axis} vs {y_axis}',\n        labels={\n            x_axis: x_axis.replace('_', ' ').title(),\n            y_axis: y_axis.replace('_', ' ').title()\n        }\n    )\n    fig_multi.update_traces(textposition=\"top center\")\n    fig_multi.update_layout(height=600)\n    st.plotly_chart(fig_multi, use_container_width=True)\n    \n    # Matriz de correla√ß√£o\n    st.subheader(\"üìä Matriz de Correla√ß√£o\")\n    \n    numeric_columns = ['velocidade_media', 'distancia_total', 'tempo_ativo', \n                      'cobertura_gps', 'violacoes_velocidade', 'bloqueios', 'total_registros']\n    \n    corr_matrix = multi_df[numeric_columns].corr()\n    \n    fig_corr = px.imshow(\n        corr_matrix,\n        title='Matriz de Correla√ß√£o entre M√©tricas',\n        labels={'color': 'Correla√ß√£o'},\n        color_continuous_scale='RdBu'\n    )\n    fig_corr.update_layout(height=500)\n    st.plotly_chart(fig_corr, use_container_width=True)\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":28471},"pages/4_üîÆ_Manuten√ß√£o_Preditiva.py":{"content":"\"\"\"\nP√°gina de Manuten√ß√£o Preditiva - An√°lise Avan√ßada com ML\n\"\"\"\n\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nfrom utils.ml_predictive import PredictiveMaintenanceAnalyzer\n\n# Configura√ß√£o da p√°gina\nst.set_page_config(\n    page_title=\"Manuten√ß√£o Preditiva\",\n    page_icon=\"üîÆ\",\n    layout=\"wide\"\n)\n\nst.title(\"üîÆ Manuten√ß√£o Preditiva\")\nst.markdown(\"An√°lise avan√ßada com Machine Learning para preven√ß√£o de falhas\")\n\n# Verificar se h√° dados\nif not DatabaseManager.has_data():\n    st.warning(\"‚ö†Ô∏è Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV primeiro.\")\n    st.stop()\n\n# Carregar dados\nwith st.spinner(\"Carregando dados...\"):\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if df.empty:\n            st.error(\"‚ùå Erro ao carregar dados da base de dados\")\n            st.stop()\n    except Exception as e:\n        st.error(f\"‚ùå Erro: {e}\")\n        st.stop()\n\n# Filtros laterais\nst.sidebar.header(\"üîß Filtros\")\n\n# Filtro de cliente\nclientes = sorted(df['cliente'].unique())\ncliente_selecionado = st.sidebar.selectbox(\n    \"Cliente:\",\n    options=['Todos'] + clientes,\n    index=0\n)\n\n# Filtro de ve√≠culo\nif cliente_selecionado != 'Todos':\n    veiculos = sorted(df[df['cliente'] == cliente_selecionado]['placa'].unique())\nelse:\n    veiculos = sorted(df['placa'].unique())\n\nveiculo_selecionado = st.sidebar.selectbox(\n    \"Ve√≠culo:\",\n    options=['Todos'] + list(veiculos),\n    index=0\n)\n\n# Filtro de per√≠odo\nperiodo = st.sidebar.selectbox(\n    \"Per√≠odo:\",\n    options=['√öltimos 7 dias', '√öltimos 30 dias', 'Todos'],\n    index=1\n)\n\n# Aplicar filtros\ndf_filtrado = df.copy()\n\nif cliente_selecionado != 'Todos':\n    df_filtrado = df_filtrado[df_filtrado['cliente'] == cliente_selecionado]\n\nif veiculo_selecionado != 'Todos':\n    df_filtrado = df_filtrado[df_filtrado['placa'] == veiculo_selecionado]\n\n# Filtro de per√≠odo\nif periodo != 'Todos':\n    data_limite = datetime.now()\n    if periodo == '√öltimos 7 dias':\n        data_limite -= timedelta(days=7)\n    elif periodo == '√öltimos 30 dias':\n        data_limite -= timedelta(days=30)\n    \n    df_filtrado = df_filtrado[df_filtrado['data'] >= data_limite]\n\nif df_filtrado.empty:\n    st.warning(\"‚ö†Ô∏è Nenhum dado encontrado com os filtros aplicados.\")\n    st.stop()\n\n# An√°lise de Manuten√ß√£o Preditiva\nst.header(\"ü§ñ An√°lise de Machine Learning\")\n\nwith st.spinner(\"Executando an√°lise preditiva...\"):\n    analyzer = PredictiveMaintenanceAnalyzer()\n    resultado = analyzer.analyze_vehicle_health(df_filtrado)\n\nif resultado['status'] == 'error':\n    st.error(f\"‚ùå {resultado['message']}\")\n    st.stop()\n\n# Dashboard de Health Scores\ncol1, col2, col3, col4 = st.columns(4)\n\nhealth_scores = resultado['health_scores']\n\nwith col1:\n    score_geral = health_scores.get('geral', 0)\n    cor_geral = 'green' if score_geral > 80 else 'orange' if score_geral > 60 else 'red'\n    st.metric(\n        label=\"üè• Sa√∫de Geral\",\n        value=f\"{score_geral}%\",\n        delta=None\n    )\n    st.markdown(f\"<div style='color:{cor_geral}'>‚óè</div>\", unsafe_allow_html=True)\n\nwith col2:\n    score_bateria = health_scores.get('bateria', 0)\n    cor_bateria = 'green' if score_bateria > 70 else 'orange' if score_bateria > 50 else 'red'\n    st.metric(\n        label=\"üîã Bateria\",\n        value=f\"{score_bateria}%\"\n    )\n    st.markdown(f\"<div style='color:{cor_bateria}'>‚óè</div>\", unsafe_allow_html=True)\n\nwith col3:\n    score_comportamento = health_scores.get('comportamento', 0)\n    cor_comportamento = 'green' if score_comportamento > 70 else 'orange' if score_comportamento > 50 else 'red'\n    st.metric(\n        label=\"üìä Comportamento\",\n        value=f\"{score_comportamento}%\"\n    )\n    st.markdown(f\"<div style='color:{cor_comportamento}'>‚óè</div>\", unsafe_allow_html=True)\n\nwith col4:\n    score_velocidade = health_scores.get('velocidade', 0)\n    cor_velocidade = 'green' if score_velocidade > 70 else 'orange' if score_velocidade > 50 else 'red'\n    st.metric(\n        label=\"üöó Velocidade\",\n        value=f\"{score_velocidade}%\"\n    )\n    st.markdown(f\"<div style='color:{cor_velocidade}'>‚óè</div>\", unsafe_allow_html=True)\n\n# Gr√°fico de Health Scores\nfig_health = go.Figure()\n\nscores = [health_scores.get(k, 0) for k in ['bateria', 'comportamento', 'velocidade']]\nlabels = ['Bateria', 'Comportamento', 'Velocidade']\ncolors = ['#ff7f0e', '#2ca02c', '#d62728']\n\nfig_health.add_trace(go.Bar(\n    x=labels,\n    y=scores,\n    marker_color=colors,\n    text=[f\"{s}%\" for s in scores],\n    textposition='auto',\n))\n\nfig_health.update_layout(\n    title=\"üìä Scores de Sa√∫de do Ve√≠culo\",\n    yaxis_title=\"Score (%)\",\n    yaxis=dict(range=[0, 100]),\n    showlegend=False,\n    height=400\n)\n\nst.plotly_chart(fig_health, use_container_width=True)\n\n# Alertas de Manuten√ß√£o\nst.header(\"üö® Alertas de Manuten√ß√£o\")\n\nmaintenance_alerts = resultado['maintenance_alerts']\n\nif maintenance_alerts:\n    for alert in maintenance_alerts:\n        severidade = alert['severidade']\n        cor = 'üî¥' if severidade == 'Alta' else 'üü°' if severidade == 'M√©dia' else 'üü¢'\n        \n        with st.expander(f\"{cor} {alert['tipo']} - {severidade}\"):\n            st.write(f\"**Descri√ß√£o:** {alert['descricao']}\")\n            st.write(f\"**Prazo:** {alert['prazo']}\")\nelse:\n    st.success(\"‚úÖ Nenhum alerta de manuten√ß√£o detectado!\")\n\n# Anomalias Detectadas\nst.header(\"‚ö†Ô∏è Anomalias Detectadas\")\n\nanomalies = resultado['anomalies']\nanomaly_count = anomalies.get('count', 0)\n\ncol1, col2 = st.columns([1, 2])\n\nwith col1:\n    st.metric(\n        label=\"Total de Anomalias\",\n        value=anomaly_count\n    )\n    \n    if anomaly_count > 0:\n        severity = anomalies.get('severity', 'Baixa')\n        color_map = {'Alta': 'üî¥', 'M√©dia': 'üü°', 'Baixa': 'üü¢'}\n        st.write(f\"**Severidade:** {color_map.get(severity, 'üü¢')} {severity}\")\n\nwith col2:\n    # Gr√°fico de anomalias ao longo do tempo\n    if anomaly_count > 0 and 'indices' in anomalies:\n        try:\n            anomaly_indices = anomalies['indices']\n            if anomaly_indices:\n                df_anomalies = df_filtrado.iloc[anomaly_indices].copy()\n                \n                fig_anomalies = px.scatter(\n                    df_anomalies,\n                    x='data',\n                    y='velocidade_km',\n                    title=\"Anomalias de Velocidade\",\n                    color_discrete_sequence=['red'],\n                    hover_data=['placa']\n                )\n                \n                fig_anomalies.update_layout(height=300)\n                st.plotly_chart(fig_anomalies, use_container_width=True)\n        except Exception as e:\n            st.write(\"Dados de anomalias n√£o dispon√≠veis para visualiza√ß√£o\")\n\n# Padr√µes de Uso\nst.header(\"üìà Padr√µes de Uso\")\n\npatterns = resultado.get('patterns', {})\n\nif patterns:\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.subheader(\"üöó Padr√µes de Velocidade\")\n        vel_patterns = patterns.get('velocidade', {})\n        if vel_patterns:\n            st.write(f\"**Velocidade M√©dia:** {vel_patterns.get('media', 0)} km/h\")\n            st.write(f\"**Velocidade M√°xima:** {vel_patterns.get('maxima', 0)} km/h\")\n            st.write(f\"**Excessos de Velocidade:** {vel_patterns.get('excessos', 0)}\")\n            st.write(f\"**Paradas:** {vel_patterns.get('paradas', 0)}\")\n    \n    with col2:\n        st.subheader(\"‚è∞ Padr√µes Temporais\")\n        temp_patterns = patterns.get('uso_temporal', {})\n        if temp_patterns:\n            st.write(f\"**Hor√°rio de Pico:** {temp_patterns.get('horario_pico', 'N/A')}h\")\n            st.write(f\"**Hor√°rio de Menor Uso:** {temp_patterns.get('horario_baixo', 'N/A')}h\")\n            st.write(f\"**Uso Noturno:** {temp_patterns.get('uso_noturno', 0)} registros\")\n\n# Recomenda√ß√µes\nst.header(\"üí° Recomenda√ß√µes\")\n\nrecommendations = resultado.get('recommendations', [])\nif recommendations:\n    for rec in recommendations:\n        st.write(f\"‚Ä¢ {rec}\")\nelse:\n    st.info(\"üìã Nenhuma recomenda√ß√£o espec√≠fica no momento.\")\n\n# Informa√ß√µes adicionais\nst.header(\"‚ÑπÔ∏è Informa√ß√µes da An√°lise\")\n\ncol1, col2, col3 = st.columns(3)\n\nwith col1:\n    st.metric(\"üìä Registros Analisados\", len(df_filtrado))\n\nwith col2:\n    periodo_analise = df_filtrado['data'].max() - df_filtrado['data'].min()\n    st.metric(\"üìÖ Per√≠odo\", f\"{periodo_analise.days} dias\")\n\nwith col3:\n    st.metric(\"üöó Ve√≠culos\", df_filtrado['placa'].nunique())\n\nst.info(\"ü§ñ **Sobre a An√°lise:** Esta an√°lise utiliza algoritmos de Machine Learning (Isolation Forest) para detectar anomalias e padr√µes nos dados telem√°ticos, fornecendo insights preditivos para manuten√ß√£o preventiva.\")","size_bytes":8846},"pages/5_üß†_Insights_Autom√°ticos.py":{"content":"import streamlit as st\nimport pandas as pd\nimport os\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport sys\n\n# Adicionar o diret√≥rio raiz ao path\nsys.path.append(os.path.dirname(os.path.dirname(__file__)))\n\nfrom utils.data_analyzer import DataAnalyzer\nfrom utils.insights_generator import InsightsGenerator\n\nst.set_page_config(\n    page_title=\"Insights Autom√°ticos - Insight Hub\",\n    page_icon=\"üß†\",\n    layout=\"wide\"\n)\n\ndef load_data():\n    \"\"\"Carrega dados APENAS da base de dados (dados reais)\"\"\"\n    try:\n        # Importar DatabaseManager\n        from database.db_manager import DatabaseManager\n        \n        # Carregar APENAS da base de dados - sem fallbacks fict√≠cios\n        if DatabaseManager.has_data():\n            df = DatabaseManager.get_dashboard_data()\n            if not df.empty:\n                st.success(f\"‚úÖ Dados reais carregados: {len(df):,} registros da base de dados\")\n                return df\n        \n        # Se n√£o h√° dados reais, mostrar mensagem clara\n        st.warning(\"‚ö†Ô∏è Nenhum dado real encontrado na base de dados. Fa√ßa upload dos seus arquivos CSV.\")\n        return pd.DataFrame()\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar dados reais: {str(e)}\")\n        return pd.DataFrame()\n\ndef main():\n    st.title(\"üß† Insights Autom√°ticos\")\n    st.markdown(\"An√°lise inteligente e recomenda√ß√µes baseadas em dados da frota\")\n    st.markdown(\"---\")\n    \n    # Carregar dados\n    df = load_data()\n    \n    if df.empty:\n        st.warning(\"üìÅ Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV primeiro.\")\n        st.stop()\n    \n    # Inicializar analisador com dados da base de dados\n    analyzer = DataAnalyzer.from_database()\n    \n    # Sidebar com filtros\n    st.sidebar.header(\"üîç Filtros para An√°lise\")\n    \n    # Filtro por cliente\n    clientes = ['Todos'] + sorted(df['cliente'].unique().tolist())\n    cliente_selecionado = st.sidebar.selectbox(\"Cliente:\", clientes)\n    \n    # Filtro por per√≠odo\n    min_date = df['data'].min().date()\n    max_date = df['data'].max().date()\n    \n    # Per√≠odo de an√°lise\n    periodo_analise = st.sidebar.selectbox(\n        \"Per√≠odo de An√°lise:\",\n        [\n            \"√öltimos 7 dias\",\n            \"√öltimos 30 dias\",\n            \"√öltimos 90 dias\",\n            \"Todo o per√≠odo\",\n            \"Personalizado\"\n        ]\n    )\n    \n    # Calcular datas baseado no per√≠odo\n    if periodo_analise == \"√öltimos 7 dias\":\n        data_inicio = max_date - timedelta(days=7)\n        data_fim = max_date\n    elif periodo_analise == \"√öltimos 30 dias\":\n        data_inicio = max_date - timedelta(days=30)\n        data_fim = max_date\n    elif periodo_analise == \"√öltimos 90 dias\":\n        data_inicio = max_date - timedelta(days=90)\n        data_fim = max_date\n    elif periodo_analise == \"Todo o per√≠odo\":\n        data_inicio = min_date\n        data_fim = max_date\n    else:  # Personalizado\n        data_range = st.sidebar.date_input(\n            \"Per√≠odo Personalizado:\",\n            value=[min_date, max_date],\n            min_value=min_date,\n            max_value=max_date\n        )\n        if len(data_range) == 2:\n            data_inicio, data_fim = data_range\n        else:\n            data_inicio = data_range[0]\n            data_fim = max_date\n    \n    # Aplicar filtros\n    filtered_df = analyzer.apply_filters(\n        cliente=cliente_selecionado,\n        data_inicio=data_inicio,\n        data_fim=data_fim\n    )\n    \n    if filtered_df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum registro encontrado com os filtros aplicados.\")\n        st.stop()\n    \n    # Inicializar gerador de insights\n    insights_generator = InsightsGenerator(analyzer)\n    \n    # Gerar insights\n    with st.spinner(\"üß† Gerando insights inteligentes...\"):\n        insights = insights_generator.generate_all_insights()\n    \n    # Tabs para diferentes tipos de insights\n    tab1, tab2, tab3, tab4, tab5 = st.tabs([\n        \"üìä Resumo Executivo\",\n        \"‚ö†Ô∏è Alertas Cr√≠ticos\",\n        \"üìà Oportunidades\",\n        \"üîÆ Predi√ß√µes\",\n        \"üìã Relat√≥rio Completo\"\n    ])\n    \n    with tab1:\n        show_executive_summary(insights, analyzer)\n    \n    with tab2:\n        show_critical_alerts(insights, analyzer)\n    \n    with tab3:\n        show_opportunities(insights, analyzer)\n    \n    with tab4:\n        show_predictions(insights, analyzer)\n    \n    with tab5:\n        show_complete_report(insights, insights_generator, analyzer)\n\ndef show_executive_summary(insights, analyzer):\n    \"\"\"Mostra resumo executivo dos insights\"\"\"\n    st.header(\"üìä Resumo Executivo\")\n    st.markdown(\"**Vis√£o geral da performance da sua frota com recomenda√ß√µes pr√°ticas**\")\n    \n    kpis = analyzer.get_kpis()\n    \n    # Verificar se h√° KPIs v√°lidos\n    if not kpis:\n        st.warning(\"‚ö†Ô∏è N√£o foi poss√≠vel calcular m√©tricas para gerar insights. Verifique se h√° dados nos filtros aplicados.\")\n        return\n    \n    # KPIs principais com contexto\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üöó Ve√≠culos Analisados\", f\"{kpis['total_veiculos']:,}\")\n    \n    with col2:\n        st.metric(\"üìä Total de Registros\", f\"{kpis['total_registros']:,}\")\n    \n    with col3:\n        st.metric(\"üìÖ Per√≠odo\", f\"{kpis['periodo_dias']} dias\")\n    \n    with col4:\n        total_insights = len(insights)\n        critical_insights = len([i for i in insights if i['type'] == 'error'])\n        st.metric(\"üß† Insights Gerados\", f\"{total_insights}\", delta=f\"{critical_insights} cr√≠ticos\")\n    \n    # Resumo por categoria\n    st.subheader(\"üìã Resumo por Categoria\")\n    \n    insight_summary = {\n        'error': {'count': 0, 'label': 'üö® Cr√≠ticos', 'color': '#dc3545'},\n        'warning': {'count': 0, 'label': '‚ö†Ô∏è Aten√ß√£o', 'color': '#ffc107'},\n        'info': {'count': 0, 'label': '‚ÑπÔ∏è Informativos', 'color': '#17a2b8'},\n        'success': {'count': 0, 'label': '‚úÖ Positivos', 'color': '#28a745'}\n    }\n    \n    for insight in insights:\n        insight_summary[insight['type']]['count'] += 1\n    \n    col_summary = st.columns(4)\n    \n    for i, (type_key, data) in enumerate(insight_summary.items()):\n        with col_summary[i]:\n            st.metric(\n                label=data['label'],\n                value=data['count'],\n                delta=f\"{(data['count']/len(insights)*100):.0f}%\" if insights else \"0%\"\n            )\n    \n    # Gr√°fico de distribui√ß√£o de insights\n    if insights:\n        fig_insights = px.pie(\n            values=[data['count'] for data in insight_summary.values()],\n            names=[data['label'] for data in insight_summary.values()],\n            title='Distribui√ß√£o de Insights por Categoria',\n            color_discrete_sequence=[data['color'] for data in insight_summary.values()]\n        )\n        st.plotly_chart(fig_insights, use_container_width=True)\n    \n    # Top 3 insights mais importantes - vers√£o melhorada\n    st.subheader(\"üéØ A√ß√µes Priorit√°rias para Sua Frota\")\n    st.markdown(\"**As 3 a√ß√µes mais importantes que voc√™ deve tomar agora:**\")\n    \n    priority_insights = sorted(insights, key=lambda x: x['priority'])[:3]\n    \n    for i, insight in enumerate(priority_insights):\n        icon = \"üö®\" if insight['type'] == 'error' else \"‚ö†Ô∏è\" if insight['type'] == 'warning' else \"‚úÖ\" if insight['type'] == 'success' else \"‚ÑπÔ∏è\"\n        priority_label = \"URGENTE\" if insight['type'] == 'error' else \"IMPORTANTE\" if insight['type'] == 'warning' else \"POSITIVO\" if insight['type'] == 'success' else \"INFORMATIVO\"\n        \n        with st.container():\n            if insight['type'] == 'error':\n                st.error(f\"{icon} **PRIORIDADE {priority_label}:** {insight['title']}\")\n            elif insight['type'] == 'warning':\n                st.warning(f\"{icon} **PRIORIDADE {priority_label}:** {insight['title']}\")\n            else:\n                st.info(f\"{icon} **{priority_label}:** {insight['title']}\")\n            \n            col_insight1, col_insight2 = st.columns([2, 1])\n            with col_insight1:\n                st.markdown(f\"**üí° O que fazer:** {insight['recommendation']}\")\n                st.markdown(f\"**üìù Por qu√™:** {insight['description']}\")\n            \n            with col_insight2:\n                if insight['type'] == 'error':\n                    st.metric(\"‚è∞ Prazo\", \"24-48 horas\", delta=\"URGENTE\")\n                elif insight['type'] == 'warning':\n                    st.metric(\"‚è∞ Prazo\", \"1-2 semanas\", delta=\"Importante\")\n                else:\n                    st.metric(\"üìä Status\", \"Monitorar\", delta=\"Positivo\")\n            \n            st.markdown(\"---\")\n            \n            # Adicionar m√©tricas relacionadas se dispon√≠vel\n            if 'compliance' in insight['title'].lower():\n                compliance = analyzer.get_compliance_analysis()\n                if compliance:\n                    col_comp1, col_comp2 = st.columns(2)\n                    with col_comp1:\n                        st.metric(\"Viola√ß√µes\", compliance.get('violacoes_velocidade', 0))\n                    with col_comp2:\n                        st.metric(\"Score M√©dio\", f\"{sum(compliance.get('score_compliance', {}).values()) / len(compliance.get('score_compliance', {})):.1f}%\" if compliance.get('score_compliance') else \"N/A\")\n    \n    # Resumo de performance geral\n    st.subheader(\"üìà Performance Geral da Frota\")\n    \n    # Score geral baseado nos insights\n    total_score = 100\n    for insight in insights:\n        if insight['type'] == 'error':\n            total_score -= 15\n        elif insight['type'] == 'warning':\n            total_score -= 8\n        elif insight['type'] == 'success':\n            total_score += 5\n    \n    total_score = max(0, min(100, total_score))\n    \n    # Gauge para score geral\n    fig_gauge = go.Figure(go.Indicator(\n        mode = \"gauge+number+delta\",\n        value = total_score,\n        domain = {'x': [0, 1], 'y': [0, 1]},\n        title = {'text': \"Score Geral da Frota\"},\n        delta = {'reference': 80},\n        gauge = {\n            'axis': {'range': [None, 100]},\n            'bar': {'color': \"darkblue\"},\n            'steps': [\n                {'range': [0, 50], 'color': \"lightgray\"},\n                {'range': [50, 80], 'color': \"gray\"}],\n            'threshold': {\n                'line': {'color': \"red\", 'width': 4},\n                'thickness': 0.75,\n                'value': 90}}))\n    \n    fig_gauge.update_layout(height=400)\n    st.plotly_chart(fig_gauge, use_container_width=True)\n\ndef show_critical_alerts(insights, analyzer):\n    \"\"\"Mostra alertas cr√≠ticos com planos de a√ß√£o claros\"\"\"\n    st.header(\"üö® Alertas Cr√≠ticos\")\n    st.markdown(\"**Problemas que precisam de sua aten√ß√£o imediata com planos de a√ß√£o espec√≠ficos**\")\n    \n    critical_insights = [i for i in insights if i['type'] == 'error']\n    warning_insights = [i for i in insights if i['type'] == 'warning']\n    \n    if not critical_insights and not warning_insights:\n        st.success(\"‚úÖ **Excelente!** Nenhum alerta cr√≠tico identificado.\")\n        st.balloons()\n        return\n    \n    # Alertas cr√≠ticos\n    if critical_insights:\n        st.subheader(\"üö® Requer A√ß√£o Imediata\")\n        \n        for insight in critical_insights:\n            with st.container():\n                st.error(f\"**{insight['title']}**\")\n                col_alert1, col_alert2 = st.columns([2, 1])\n                \n                with col_alert1:\n                    st.write(f\"üìù **Problema:** {insight['description']}\")\n                    st.write(f\"üí° **A√ß√£o Recomendada:** {insight['recommendation']}\")\n                \n                with col_alert2:\n                    st.metric(\"Prioridade\", \"ALTA\", delta=\"Cr√≠tico\")\n                \n                st.markdown(\"---\")\n    \n    # Alertas de aten√ß√£o\n    if warning_insights:\n        st.subheader(\"‚ö†Ô∏è Requer Aten√ß√£o\")\n        \n        for insight in warning_insights:\n            with st.container():\n                st.warning(f\"**{insight['title']}**\")\n                col_warn1, col_warn2 = st.columns([2, 1])\n                \n                with col_warn1:\n                    st.write(f\"üìù **Observa√ß√£o:** {insight['description']}\")\n                    st.write(f\"üí° **Recomenda√ß√£o:** {insight['recommendation']}\")\n                \n                with col_warn2:\n                    st.metric(\"Prioridade\", \"M√âDIA\", delta=\"Aten√ß√£o\")\n                \n                st.markdown(\"---\")\n    \n    # An√°lise detalhada dos problemas\n    st.subheader(\"üîç An√°lise Detalhada dos Problemas\")\n    \n    compliance = analyzer.get_compliance_analysis()\n    \n    if compliance:\n        col_detail1, col_detail2, col_detail3 = st.columns(3)\n        \n        with col_detail1:\n            st.metric(\n                \"üö® Viola√ß√µes de Velocidade\",\n                f\"{compliance.get('violacoes_velocidade', 0):,}\",\n                help=\"Registros com velocidade acima do limite\"\n            )\n        \n        with col_detail2:\n            st.metric(\n                \"üì° Problemas de GPS\",\n                f\"{compliance.get('veiculos_baixo_gps', 0):,}\",\n                help=\"Ve√≠culos com cobertura GPS abaixo de 95%\"\n            )\n        \n        with col_detail3:\n            st.metric(\n                \"üîí Ve√≠culos Bloqueados\",\n                f\"{compliance.get('veiculos_bloqueados', 0):,}\",\n                help=\"Ve√≠culos com status de bloqueio ativo\"\n            )\n        \n        # Gr√°fico de ve√≠culos com problemas\n        if compliance.get('score_compliance'):\n            problem_vehicles = {k: v for k, v in compliance['score_compliance'].items() if v < 70}\n            \n            if problem_vehicles:\n                st.subheader(\"üöó Ve√≠culos que Necessitam Aten√ß√£o Urgente\")\n                \n                fig_problems = px.bar(\n                    x=list(problem_vehicles.keys()),\n                    y=list(problem_vehicles.values()),\n                    title='Score de Compliance - Ve√≠culos Problem√°ticos',\n                    labels={'x': 'Placa', 'y': 'Score de Compliance (%)'},\n                    color=list(problem_vehicles.values()),\n                    color_continuous_scale='Reds'\n                )\n                fig_problems.add_hline(y=70, line_dash=\"dash\", line_color=\"red\", \n                                     annotation_text=\"Limite M√≠nimo: 70%\")\n                st.plotly_chart(fig_problems, use_container_width=True)\n    \n    # Plano de a√ß√£o recomendado - vers√£o melhorada\n    st.subheader(\"üìã Plano de A√ß√£o Detalhado\")\n    st.markdown(\"**Cronograma espec√≠fico para resolver os problemas identificados:**\")\n    \n    action_plan = []\n    \n    if critical_insights:\n        for insight in critical_insights:\n            action_plan.append({\n                'Prioridade': 'üö® CR√çTICA',\n                'Prazo': '24-48 horas',\n                'A√ß√£o': f\"Resolver: {insight['title']}\",\n                'Como Fazer': insight['recommendation'],\n                'Respons√°vel': 'Gestor de Frota + Equipe T√©cnica'\n            })\n    \n    if warning_insights:\n        action_plan.append({\n            'Prioridade': 'M√âDIA',\n            'Prazo': '1 semana',\n            'A√ß√£o': 'Implementar melhorias para alertas de aten√ß√£o',\n            'Respons√°vel': 'Equipe Operacional'\n        })\n    \n    if compliance and compliance.get('violacoes_velocidade', 0) > 0:\n        action_plan.append({\n            'Prioridade': 'ALTA',\n            'Prazo': '3 dias',\n            'A√ß√£o': 'Treinamento de condutores sobre limites de velocidade',\n            'Respons√°vel': 'RH/Treinamento'\n        })\n    \n    if action_plan:\n        action_df = pd.DataFrame(action_plan)\n        st.dataframe(action_df, use_container_width=True, hide_index=True)\n\ndef show_opportunities(insights, analyzer):\n    \"\"\"Mostra oportunidades de melhoria com potencial de economia\"\"\"\n    st.header(\"üìà Oportunidades de Economia e Melhoria\")\n    st.markdown(\"**Onde voc√™ pode economizar dinheiro e melhorar a efici√™ncia da sua frota**\")\n    \n    info_insights = [i for i in insights if i['type'] == 'info']\n    success_insights = [i for i in insights if i['type'] == 'success']\n    \n    # Oportunidades identificadas\n    st.subheader(\"üí° Oportunidades Identificadas\")\n    \n    if info_insights:\n        for insight in info_insights:\n            with st.container():\n                st.info(f\"**{insight['title']}**\")\n                col_opp1, col_opp2 = st.columns([3, 1])\n                \n                with col_opp1:\n                    st.write(f\"üìä **An√°lise:** {insight['description']}\")\n                    st.write(f\"üéØ **Oportunidade:** {insight['recommendation']}\")\n                \n                with col_opp2:\n                    # Estimar impacto potencial com valores em reais\n                    kpis = analyzer.get_kpis()\n                    num_vehicles = kpis.get('total_veiculos', 1)\n                    \n                    if 'velocidade' in insight['title'].lower():\n                        economy_month = num_vehicles * 300  # R$ 300/m√™s por ve√≠culo em combust√≠vel\n                        st.metric(\"üí∞ Economia/M√™s\", f\"R$ {economy_month:,}\", delta=\"Combust√≠vel\")\n                    elif 'gps' in insight['title'].lower():\n                        st.metric(\"üìä Melhoria\", \"10-20%\", delta=\"Controle\")\n                    else:\n                        st.metric(\"‚ö° Impacto\", \"M√©dio\", delta=\"Efici√™ncia\")\n                \n                st.markdown(\"---\")\n    \n    # Pontos fortes identificados\n    if success_insights:\n        st.subheader(\"‚úÖ Pontos Fortes da Frota\")\n        \n        for insight in success_insights:\n            st.success(f\"**{insight['title']}** - {insight['description']}\")\n    \n    # An√°lise de efici√™ncia\n    st.subheader(\"üìä An√°lise de Efici√™ncia\")\n    \n    efficiency = analyzer.get_efficiency_metrics()\n    \n    if efficiency and 'eficiencia_por_veiculo' in efficiency:\n        # Identificar top performers e underperformers\n        vehicle_efficiency = efficiency['eficiencia_por_veiculo']\n        \n        if vehicle_efficiency is not None and hasattr(vehicle_efficiency, 'empty') and not vehicle_efficiency.empty:\n            # Converter series para lista de dicts\n            efficiency_data = []\n            for placa, data in vehicle_efficiency.items():\n                if isinstance(data, dict):\n                    efficiency_data.append({\n                        'Placa': placa,\n                        'KM por Dia': data.get('km_por_dia', 0),\n                        'Utiliza√ß√£o Di√°ria': data.get('utilizacao_diaria', 0),\n                        'Velocidade M√©dia': data.get('velocidade_media', 0),\n                        'Tempo Parado (%)': data.get('tempo_parado_pct', 0)\n                    })\n            \n            if efficiency_data:\n                efficiency_df = pd.DataFrame(efficiency_data)\n                \n                # Top performers\n                top_performers = efficiency_df.nlargest(5, 'KM por Dia')\n                \n                col_eff1, col_eff2 = st.columns(2)\n                \n                with col_eff1:\n                    st.write(\"**üèÜ Top Performers (KM por Dia):**\")\n                    for _, row in top_performers.iterrows():\n                        st.success(f\"**{row['Placa']}** - {row['KM por Dia']:.1f} km/dia\")\n                \n                with col_eff2:\n                    st.write(\"**‚ö†Ô∏è Baixa Utiliza√ß√£o:**\")\n                    low_performers = efficiency_df.nsmallest(5, 'Utiliza√ß√£o Di√°ria')\n                    for _, row in low_performers.iterrows():\n                        if row['Utiliza√ß√£o Di√°ria'] < 10:\n                            st.warning(f\"**{row['Placa']}** - {row['Utiliza√ß√£o Di√°ria']:.1f} reg/dia\")\n    \n    # Oportunidades de otimiza√ß√£o\n    st.subheader(\"üéØ Oportunidades de Otimiza√ß√£o\")\n    \n    optimization_opportunities = [\n        {\n            '√Årea': 'Otimiza√ß√£o de Rotas',\n            'Descri√ß√£o': 'An√°lise de padr√µes de movimenta√ß√£o para reduzir dist√¢ncias',\n            'Impacto Estimado': '10-15% redu√ß√£o de combust√≠vel',\n            'Investimento': 'Baixo',\n            'Prazo': '2-4 semanas'\n        },\n        {\n            '√Årea': 'Treinamento de Condutores',\n            'Descri√ß√£o': 'Programa de condu√ß√£o econ√¥mica e segura',\n            'Impacto Estimado': '15-20% redu√ß√£o de viola√ß√µes',\n            'Investimento': 'M√©dio',\n            'Prazo': '1-2 meses'\n        },\n        {\n            '√Årea': 'Manuten√ß√£o Preventiva',\n            'Descri√ß√£o': 'Implementa√ß√£o de manuten√ß√£o baseada em dados',\n            'Impacto Estimado': '20-30% redu√ß√£o de quebras',\n            'Investimento': 'Alto',\n            'Prazo': '3-6 meses'\n        }\n    ]\n    \n    for opp in optimization_opportunities:\n        with st.expander(f\"üéØ {opp['√Årea']}\"):\n            col_desc, col_impact = st.columns([2, 1])\n            \n            with col_desc:\n                st.write(f\"**Descri√ß√£o:** {opp['Descri√ß√£o']}\")\n                st.write(f\"**Prazo de Implementa√ß√£o:** {opp['Prazo']}\")\n            \n            with col_impact:\n                st.metric(\"Impacto Estimado\", opp['Impacto Estimado'])\n                st.metric(\"Investimento\", opp['Investimento'])\n\ndef show_predictions(insights, analyzer):\n    \"\"\"Mostra predi√ß√µes pr√°ticas para planejamento\"\"\"\n    st.header(\"üîÆ Previs√µes para Planejamento\")\n    st.markdown(\"**Prepare-se para o que est√° por vir: manuten√ß√µes, custos e necessidades da frota**\")\n    \n    df = analyzer.filtered_df\n    \n    if len(df) < 14:  # Menos de 2 semanas de dados\n        st.warning(\"‚ö†Ô∏è **Dados insuficientes para previs√µes.** Carregue pelo menos 14 dias de dados para ter previs√µes confi√°veis.\")\n        return\n    \n    # An√°lise de tend√™ncias\n    st.subheader(\"üìà An√°lise de Tend√™ncias\")\n    \n    # Dividir dados em per√≠odos\n    df_sorted = df.sort_values('data')\n    mid_point = len(df_sorted) // 2\n    \n    first_half = df_sorted.iloc[:mid_point]\n    second_half = df_sorted.iloc[mid_point:]\n    \n    # Calcular mudan√ßas\n    trends = {}\n    \n    if not first_half.empty and not second_half.empty:\n        trends['velocidade'] = second_half['velocidade_km'].mean() - first_half['velocidade_km'].mean()\n        trends['gps_coverage'] = (second_half['gps'].mean() - first_half['gps'].mean()) * 100\n        trends['activity'] = len(second_half) / len(first_half) - 1\n    \n    col_trend1, col_trend2, col_trend3 = st.columns(3)\n    \n    with col_trend1:\n        if trends.get('velocidade', 0) != 0:\n            st.metric(\n                \"Velocidade M√©dia\",\n                f\"{second_half['velocidade_km'].mean():.1f} km/h\",\n                delta=f\"{trends['velocidade']:.1f} km/h\"\n            )\n    \n    with col_trend2:\n        if trends.get('gps_coverage', 0) != 0:\n            st.metric(\n                \"Cobertura GPS\",\n                f\"{second_half['gps'].mean()*100:.1f}%\",\n                delta=f\"{trends['gps_coverage']:.1f}%\"\n            )\n    \n    with col_trend3:\n        if trends.get('activity', 0) != 0:\n            st.metric(\n                \"Atividade da Frota\",\n                f\"{len(second_half):,} registros\",\n                delta=f\"{trends['activity']*100:.1f}%\"\n            )\n    \n    # Predi√ß√µes para pr√≥ximo per√≠odo - vers√£o melhorada\n    st.subheader(\"üìÖ O que Esperar nos Pr√≥ximos 30 Dias\")\n    st.markdown(\"**Planeje-se para estas necessidades previstas da sua frota:**\")\n    \n    predictions = []\n    \n    # Predi√ß√£o de manuten√ß√£o\n    high_usage_vehicles = df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False).head(10)\n    \n    for placa, total_km in high_usage_vehicles.items():\n        if total_km > 1000:  # Ve√≠culos com alta quilometragem\n            vehicle_data = df[df['placa'] == placa]\n            daily_avg = total_km / len(vehicle_data['data'].dt.date.unique())\n            \n            predicted_km_month = daily_avg * 30\n            \n            days_to_maintenance = int(5000/daily_avg)\n            cost_estimate = 800  # Estimativa de custo de manuten√ß√£o\n            predictions.append({\n                'Tipo': 'üîß Manuten√ß√£o',\n                'Ve√≠culo': placa,\n                'Quando': f'Em {days_to_maintenance} dias',\n                'Custo Estimado': f'R$ {cost_estimate:,}',\n                'A√ß√£o Necess√°ria': 'Agendar revis√£o preventiva',\n                'Confian√ßa': '85%'\n            })\n    \n    # Predi√ß√£o de compliance\n    compliance = analyzer.get_compliance_analysis()\n    if compliance and compliance.get('score_compliance'):\n        declining_vehicles = {k: v for k, v in compliance['score_compliance'].items() if v < 80}\n        \n        for placa in declining_vehicles:\n            predictions.append({\n                'Tipo': '‚ö†Ô∏è Compliance',\n                'Ve√≠culo': placa,\n                'Quando': 'Pr√≥ximas 2 semanas',\n                'Custo Estimado': 'R$ 500-2.000 (multas)',\n                'A√ß√£o Necess√°ria': 'Treinamento urgente do motorista',\n                'Confian√ßa': '75%'\n            })\n    \n    # Exibir predi√ß√µes\n    if predictions:\n        predictions_df = pd.DataFrame(predictions)\n        \n        # Colorir por tipo\n        def color_prediction_type(val):\n            if val == 'Manuten√ß√£o Preventiva':\n                return 'background-color: #fff3cd'\n            elif val == 'Compliance':\n                return 'background-color: #f8d7da'\n            else:\n                return ''\n        \n        styled_predictions = predictions_df.style.applymap(\n            color_prediction_type, \n            subset=['Tipo']\n        )\n        \n        st.dataframe(styled_predictions, use_container_width=True, hide_index=True)\n    else:\n        st.info(\"‚ÑπÔ∏è Nenhuma predi√ß√£o cr√≠tica identificada para os pr√≥ximos 30 dias.\")\n    \n    # Gr√°fico de tend√™ncia temporal\n    st.subheader(\"üìä Visualiza√ß√£o de Tend√™ncias\")\n    \n    # Agregar dados por semana\n    df['semana'] = df['data'].dt.to_period('W')\n    weekly_trends = df.groupby('semana').agg({\n        'velocidade_km': 'mean',\n        'gps': lambda x: x.mean() * 100,\n        'placa': 'nunique'\n    }).reset_index()\n    \n    weekly_trends['semana'] = weekly_trends['semana'].dt.start_time\n    \n    fig_trends = px.line(\n        weekly_trends,\n        x='semana',\n        y=['velocidade_km', 'gps', 'placa'],\n        title='Tend√™ncias Semanais da Frota',\n        labels={'value': 'Valor', 'semana': 'Semana', 'variable': 'M√©trica'}\n    )\n    \n    st.plotly_chart(fig_trends, use_container_width=True)\n    \n    # Alertas preditivos\n    st.subheader(\"üö® Alertas Preditivos\")\n    \n    predictive_alerts = []\n    \n    # Alerta de tend√™ncia decrescente na cobertura GPS\n    if trends.get('gps_coverage', 0) < -5:\n        predictive_alerts.append({\n            'Alerta': 'üì° Degrada√ß√£o de GPS',\n            'Descri√ß√£o': 'Cobertura GPS em decl√≠nio detectada',\n            'Probabilidade': '80%',\n            'A√ß√£o': 'Verificar equipamentos GPS da frota'\n        })\n    \n    # Alerta de aumento de velocidade\n    if trends.get('velocidade', 0) > 5:\n        predictive_alerts.append({\n            'Alerta': '‚ö° Aumento de Velocidade',\n            'Descri√ß√£o': 'Velocidades m√©dias em crescimento',\n            'Probabilidade': '75%',\n            'A√ß√£o': 'Refor√ßar treinamento de condutores'\n        })\n    \n    if predictive_alerts:\n        for alert in predictive_alerts:\n            st.warning(f\"**{alert['Alerta']}** - {alert['Descri√ß√£o']} (Probabilidade: {alert['Probabilidade']})\")\n            st.write(f\"üí° **A√ß√£o Recomendada:** {alert['A√ß√£o']}\")\n    else:\n        st.success(\"‚úÖ Nenhum alerta preditivo identificado. Tend√™ncias est√°veis.\")\n\ndef show_complete_report(insights, insights_generator, analyzer):\n    \"\"\"Mostra relat√≥rio completo\"\"\"\n    st.header(\"üìã Relat√≥rio Completo de Insights\")\n    \n    # Informa√ß√µes do relat√≥rio\n    col_info1, col_info2 = st.columns(2)\n    \n    with col_info1:\n        st.info(f\"\"\"\n        **üìä Informa√ß√µes do Relat√≥rio:**\n        - Data de Gera√ß√£o: {datetime.now().strftime('%d/%m/%Y %H:%M')}\n        - Total de Insights: {len(insights)}\n        - Per√≠odo Analisado: {analyzer.get_kpis()['periodo_dias']} dias\n        \"\"\")\n    \n    with col_info2:\n        st.info(f\"\"\"\n        **üöó Dados da Frota:**\n        - Ve√≠culos Analisados: {analyzer.get_kpis()['total_veiculos']:,}\n        - Registros Processados: {analyzer.get_kpis()['total_registros']:,}\n        - Score Geral: {calculate_general_score(insights):.1f}/100\n        \"\"\")\n    \n    # Filtro por categoria\n    categoria_filtro = st.selectbox(\n        \"Filtrar por categoria:\",\n        [\"Todos\", \"Cr√≠ticos\", \"Aten√ß√£o\", \"Informativos\", \"Positivos\"]\n    )\n    \n    filtered_insights = insights\n    if categoria_filtro != \"Todos\":\n        type_map = {\n            \"Cr√≠ticos\": \"error\",\n            \"Aten√ß√£o\": \"warning\", \n            \"Informativos\": \"info\",\n            \"Positivos\": \"success\"\n        }\n        filtered_insights = [i for i in insights if i['type'] == type_map[categoria_filtro]]\n    \n    # Exibir insights filtrados\n    if filtered_insights:\n        for i, insight in enumerate(filtered_insights, 1):\n            icon_map = {\n                'error': 'üö®',\n                'warning': '‚ö†Ô∏è',\n                'info': '‚ÑπÔ∏è',\n                'success': '‚úÖ'\n            }\n            \n            icon = icon_map.get(insight['type'], '‚ÑπÔ∏è')\n            \n            with st.expander(f\"{i}. {icon} {insight['title']}\"):\n                col_insight1, col_insight2 = st.columns([3, 1])\n                \n                with col_insight1:\n                    st.write(f\"**üìù Descri√ß√£o:** {insight['description']}\")\n                    st.write(f\"**üí° Recomenda√ß√£o:** {insight['recommendation']}\")\n                    st.write(f\"**üïí Gerado em:** {insight['timestamp'].strftime('%d/%m/%Y %H:%M')}\")\n                \n                with col_insight2:\n                    priority_text = {1: \"ALTA\", 2: \"M√âDIA\", 3: \"BAIXA\", 4: \"INFO\"}\n                    st.metric(\"Prioridade\", priority_text.get(insight['priority'], \"N/A\"))\n                    \n                    type_text = {\n                        'error': \"CR√çTICO\",\n                        'warning': \"ATEN√á√ÉO\", \n                        'info': \"INFO\",\n                        'success': \"POSITIVO\"\n                    }\n                    st.metric(\"Categoria\", type_text.get(insight['type'], \"N/A\"))\n    else:\n        st.info(f\"Nenhum insight encontrado para a categoria '{categoria_filtro}'.\")\n    \n    # Bot√µes de a√ß√£o\n    st.markdown(\"---\")\n    st.subheader(\"üì§ Exportar Relat√≥rio\")\n    \n    col_export1, col_export2, col_export3 = st.columns(3)\n    \n    with col_export1:\n        # Export texto\n        if st.button(\"üìÑ Exportar como Texto\", use_container_width=True):\n            report_text = insights_generator.export_insights_to_text()\n            st.download_button(\n                label=\"üì• Download Relat√≥rio TXT\",\n                data=report_text,\n                file_name=f\"relatorio_insights_{datetime.now().strftime('%Y%m%d_%H%M')}.txt\",\n                mime=\"text/plain\"\n            )\n    \n    with col_export2:\n        # Export CSV\n        if st.button(\"üìä Exportar como CSV\", use_container_width=True):\n            insights_df = pd.DataFrame([\n                {\n                    'Timestamp': insight['timestamp'],\n                    'Titulo': insight['title'],\n                    'Descricao': insight['description'],\n                    'Recomendacao': insight['recommendation'],\n                    'Tipo': insight['type'],\n                    'Prioridade': insight['priority']\n                }\n                for insight in insights\n            ])\n            \n            csv_data = insights_df.to_csv(index=False).encode('utf-8')\n            st.download_button(\n                label=\"üì• Download Relat√≥rio CSV\",\n                data=csv_data,\n                file_name=f\"insights_{datetime.now().strftime('%Y%m%d_%H%M')}.csv\",\n                mime=\"text/csv\"\n            )\n    \n    with col_export3:\n        # Gerar novo relat√≥rio\n        if st.button(\"üîÑ Gerar Novo Relat√≥rio\", use_container_width=True):\n            st.rerun()\n    \n    # Estat√≠sticas do relat√≥rio\n    st.markdown(\"---\")\n    st.subheader(\"üìà Estat√≠sticas do Relat√≥rio\")\n    \n    col_stats1, col_stats2, col_stats3, col_stats4 = st.columns(4)\n    \n    with col_stats1:\n        critical_count = len([i for i in insights if i['type'] == 'error'])\n        st.metric(\"üö® Insights Cr√≠ticos\", critical_count)\n    \n    with col_stats2:\n        warning_count = len([i for i in insights if i['type'] == 'warning'])\n        st.metric(\"‚ö†Ô∏è Requer Aten√ß√£o\", warning_count)\n    \n    with col_stats3:\n        positive_count = len([i for i in insights if i['type'] == 'success'])\n        st.metric(\"‚úÖ Pontos Positivos\", positive_count)\n    \n    with col_stats4:\n        total_recommendations = len([i for i in insights if i['recommendation']])\n        st.metric(\"üí° Recomenda√ß√µes\", total_recommendations)\n\ndef calculate_general_score(insights):\n    \"\"\"Calcula score geral baseado nos insights\"\"\"\n    if not insights:\n        return 75  # Score neutro\n    \n    score = 100\n    \n    for insight in insights:\n        if insight['type'] == 'error':\n            score -= 15\n        elif insight['type'] == 'warning':\n            score -= 8\n        elif insight['type'] == 'success':\n            score += 5\n        # info insights n√£o afetam o score\n    \n    return max(0, min(100, score))\n\nif __name__ == \"__main__\":\n    main()\n","size_bytes":33766},"pages/6_üìÑ_Relat√≥rios.py":{"content":"\"\"\"P√°gina de Relat√≥rios Avan√ßados\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nfrom utils.pdf_reports import PDFReportGenerator\nfrom utils.data_analyzer import DataAnalyzer\nimport os\n\nst.set_page_config(page_title=\"Relat√≥rios\", page_icon=\"üìÑ\", layout=\"wide\")\nst.title(\"üìÑ Relat√≥rios Avan√ßados\")\nst.markdown(\"*Sistema completo de gera√ß√£o de relat√≥rios com dados consolidados de todos os pain√©is*\")\n\n# Carregar dados diretamente da base de dados\ndf_inicial = DatabaseManager.get_dashboard_data()\nif df_inicial.empty:\n    st.warning(\"‚ö†Ô∏è Nenhum dado encontrado. Fa√ßa upload de arquivos CSV primeiro.\")\n    st.stop()\nelse:\n    st.success(f\"‚úÖ Dados carregados: {len(df_inicial):,} registros para gera√ß√£o de relat√≥rios\")\n\n# Carregar dados com cache para performance\n@st.cache_data(ttl=300)  # Cache por 5 minutos\ndef load_report_data():\n    \"\"\"Carrega dados otimizados para relat√≥rios\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        summary = DatabaseManager.get_fleet_summary()\n        \n        if df.empty:\n            return None, None\n            \n        return df, summary\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {str(e)}\")\n        return None, None\n\n# Configura√ß√µes do relat√≥rio\nst.sidebar.header(\"‚öôÔ∏è Configura√ß√µes do Relat√≥rio\")\n\n# Filtros de per√≠odo\nperiodo_opcoes = {\n    \"√öltimos 7 dias\": 7,\n    \"√öltimos 30 dias\": 30,\n    \"√öltimos 90 dias\": 90,\n    \"Todos os dados\": None\n}\n\nperiodo_selecionado = st.sidebar.selectbox(\n    \"üìÖ Per√≠odo:\",\n    options=list(periodo_opcoes.keys()),\n    index=1  # Padr√£o: √∫ltimos 30 dias\n)\n\n# Tipos de relat√≥rio\ntipo_relatorio = st.sidebar.selectbox(\n    \"üìä Tipo de Relat√≥rio:\",\n    [\n        \"üìã Relat√≥rio Executivo Completo\",\n        \"üöó An√°lise Detalhada por Ve√≠culo\", \n        \"‚ö° Relat√≥rio de Performance\",\n        \"üö® Relat√≥rio de Conformidade\",\n        \"üìà An√°lise de Tend√™ncias\",\n        \"üîç Relat√≥rio Personalizado\"\n    ]\n)\n\n# Op√ß√µes de formato\nformato_saida = st.sidebar.selectbox(\n    \"üìÑ Formato:\",\n    [\"PDF Profissional\", \"Visualiza√ß√£o Web\", \"Dados CSV\", \"Relat√≥rio Completo (PDF + CSV)\"]\n)\n\n# Incluir gr√°ficos\nincluir_graficos = st.sidebar.checkbox(\"üìä Incluir Gr√°ficos\", value=True)\nincluir_mapas = st.sidebar.checkbox(\"üó∫Ô∏è Incluir Mapas\", value=False)\n\n# Carregar dados\ndf, summary = load_report_data()\n\nif df is None or df.empty:\n    st.error(\"‚ùå N√£o foi poss√≠vel carregar os dados para o relat√≥rio.\")\n    st.stop()\n\n# Validar colunas essenciais\nrequired_columns = ['data', 'placa', 'cliente', 'velocidade_km', 'gps']\nmissing_columns = [col for col in required_columns if col not in df.columns]\nif missing_columns:\n    st.error(f\"‚ùå Colunas essenciais ausentes nos dados: {', '.join(missing_columns)}\")\n    st.stop()\n\n# Filtrar dados por per√≠odo se especificado - com valida√ß√£o de tipo\ndf_filtered = df.copy()\n\n# Converter coluna de data para datetime de forma segura\nif 'data' in df_filtered.columns:\n    df_filtered['data'] = pd.to_datetime(df_filtered['data'], errors='coerce')\n    df_filtered = df_filtered.dropna(subset=['data'])\n\nif periodo_opcoes[periodo_selecionado] is not None:\n    dias = periodo_opcoes[periodo_selecionado]\n    cutoff_date = datetime.now() - timedelta(days=dias)\n    \n    # Filtrar apenas se a coluna de data est√° dispon√≠vel e foi convertida com sucesso\n    if 'data' in df_filtered.columns and not df_filtered.empty:\n        # Verificar se a coluna de data tem timezone e ajustar a compara√ß√£o\n        if hasattr(df_filtered['data'].dtype, 'tz') and df_filtered['data'].dtype.tz is not None:\n            # Se dados t√™m timezone, converter cutoff_date para timezone-aware\n            from datetime import timezone\n            cutoff_date = cutoff_date.replace(tzinfo=timezone.utc)\n        else:\n            # Se dados s√£o naive, garantir que cutoff_date tamb√©m seja naive\n            cutoff_date = cutoff_date.replace(tzinfo=None)\n        \n        df_filtered = df_filtered[df_filtered['data'] >= cutoff_date]\n        st.info(f\"üìÖ Dados filtrados: {len(df_filtered):,} registros dos √∫ltimos {dias} dias\")\n    else:\n        st.warning(\"‚ö†Ô∏è N√£o foi poss√≠vel filtrar por per√≠odo - dados de data indispon√≠veis\")\nelse:\n    st.info(f\"üìÖ Usando todos os dados: {len(df_filtered):,} registros\")\n\n# An√°lise preliminar para o relat√≥rio\nanalyzer = DataAnalyzer(df_filtered)\n\n# ========== DEFINI√á√ïES DAS FUN√á√ïES ==========\n\ndef show_report_preview(df, summary, analyzer, tipo_relatorio):\n    \"\"\"Mostra pr√©-visualiza√ß√£o do relat√≥rio\"\"\"\n    st.subheader(\"üëÅÔ∏è Pr√©-visualiza√ß√£o do Relat√≥rio\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para o per√≠odo selecionado.\")\n        col1, col2, col3, col4 = st.columns(4)\n        with col1:\n            st.metric(\"üìä Total de Registros\", \"0\")\n        with col2:\n            st.metric(\"üöó Ve√≠culos\", \"0\")\n        with col3:\n            st.metric(\"üè¢ Clientes\", \"0\")\n        with col4:\n            st.metric(\"üìÖ Per√≠odo\", \"0 dias\")\n        return\n    \n    # Estat√≠sticas gerais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üìä Total de Registros\", f\"{len(df):,}\")\n    with col2:\n        st.metric(\"üöó Ve√≠culos\", f\"{df['placa'].nunique()}\")\n    with col3:\n        st.metric(\"üè¢ Clientes\", f\"{df['cliente'].nunique()}\")\n    with col4:\n        # Calcular per√≠odo de forma segura\n        try:\n            periodo_dias = (df['data'].max() - df['data'].min()).days\n            if pd.isna(periodo_dias):\n                periodo_dias = 0\n        except:\n            periodo_dias = 0\n        st.metric(\"üìÖ Per√≠odo\", f\"{periodo_dias} dias\")\n    \n    # Conte√∫do baseado no tipo de relat√≥rio\n    if \"Executivo\" in tipo_relatorio:\n        show_executive_preview(df, summary, analyzer)\n    elif \"Ve√≠culo\" in tipo_relatorio:\n        show_vehicle_preview(df, analyzer)\n    elif \"Performance\" in tipo_relatorio:\n        show_performance_preview(df, analyzer)\n    elif \"Conformidade\" in tipo_relatorio:\n        show_compliance_preview(df, analyzer)\n    elif \"Tend√™ncias\" in tipo_relatorio:\n        show_trends_preview(df, analyzer)\n    else:\n        show_custom_preview(df, analyzer)\n\ndef show_executive_preview(df, summary, analyzer):\n    \"\"\"Preview do relat√≥rio executivo\"\"\"\n    st.markdown(\"### üìã Resumo Executivo\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para o resumo executivo.\")\n        return\n    \n    # KPIs principais\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        velocidade_media = df['velocidade_km'].mean()\n        st.metric(\"‚ö° Velocidade M√©dia\", f\"{velocidade_media:.1f} km/h\")\n        \n    with col2:\n        gps_coverage = (df['gps'].sum() / len(df)) * 100 if len(df) > 0 else 0\n        st.metric(\"üì° Cobertura GPS\", f\"{gps_coverage:.1f}%\")\n        \n    with col3:\n        utilizacao = len(df[df['velocidade_km'] > 0]) / len(df) * 100 if len(df) > 0 else 0\n        st.metric(\"üöó Taxa de Utiliza√ß√£o\", f\"{utilizacao:.1f}%\")\n    \n    # Gr√°fico de utiliza√ß√£o por ve√≠culo\n    vehicle_usage = df.groupby('placa').agg({\n        'velocidade_km': 'mean',\n        'data': 'count'\n    }).reset_index()\n    vehicle_usage.columns = ['Ve√≠culo', 'Velocidade M√©dia', 'Registros']\n    \n    fig = px.bar(vehicle_usage, x='Ve√≠culo', y='Registros', \n                 title=\"üìä Registros por Ve√≠culo\")\n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_vehicle_preview(df, analyzer):\n    \"\"\"Preview da an√°lise por ve√≠culo\"\"\"\n    st.markdown(\"### üöó An√°lise por Ve√≠culo\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para an√°lise por ve√≠culo.\")\n        return\n    \n    # Sele√ß√£o de ve√≠culo\n    veiculos = sorted(df['placa'].unique())\n    veiculo_selecionado = st.selectbox(\"Selecione um ve√≠culo:\", veiculos)\n    \n    df_veiculo = df[df['placa'] == veiculo_selecionado]\n    \n    # Verificar se h√° dados para o ve√≠culo selecionado\n    if df_veiculo.empty:\n        st.warning(f\"‚ö†Ô∏è Nenhum dado encontrado para o ve√≠culo {veiculo_selecionado}.\")\n        return\n    \n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üìä Registros\", f\"{len(df_veiculo):,}\")\n    with col2:\n        vel_media = df_veiculo['velocidade_km'].mean()\n        st.metric(\"‚ö° Velocidade M√©dia\", f\"{vel_media:.1f} km/h\")\n    with col3:\n        vel_max = df_veiculo['velocidade_km'].max()\n        st.metric(\"üèéÔ∏è Velocidade M√°xima\", f\"{vel_max:.1f} km/h\")\n    with col4:\n        utilizacao = len(df_veiculo[df_veiculo['velocidade_km'] > 0]) / len(df_veiculo) * 100 if len(df_veiculo) > 0 else 0\n        st.metric(\"üìà Utiliza√ß√£o\", f\"{utilizacao:.1f}%\")\n    \n    # Gr√°fico de velocidade ao longo do tempo\n    df_veiculo_sample = df_veiculo.sample(min(500, len(df_veiculo)))\n    fig = px.line(df_veiculo_sample, x='data', y='velocidade_km',\n                  title=f\"üìà Velocidade ao Longo do Tempo - {veiculo_selecionado}\")\n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_performance_preview(df, analyzer):\n    \"\"\"Preview do relat√≥rio de performance\"\"\"\n    st.markdown(\"### ‚ö° An√°lise de Performance\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para an√°lise de performance.\")\n        return\n    \n    # Top performers\n    performance = df.groupby('placa').agg({\n        'velocidade_km': 'mean',\n        'odometro_periodo_km': 'sum',\n        'data': 'count'\n    }).reset_index()\n    performance.columns = ['Ve√≠culo', 'Velocidade M√©dia', 'KM Total', 'Registros']\n    performance = performance.sort_values('KM Total', ascending=False)\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.markdown(\"**üèÜ Top 5 - Maior Quilometragem**\")\n        top_km = performance.head(5)\n        st.dataframe(top_km[['Ve√≠culo', 'KM Total']], hide_index=True)\n        \n    with col2:\n        st.markdown(\"**‚ö° Top 5 - Maior Velocidade M√©dia**\")\n        top_speed = performance.sort_values('Velocidade M√©dia', ascending=False).head(5)\n        st.dataframe(top_speed[['Ve√≠culo', 'Velocidade M√©dia']], hide_index=True)\n    \n    # Gr√°fico de efici√™ncia\n    fig = px.scatter(performance, x='Velocidade M√©dia', y='KM Total', \n                     size='Registros', text='Ve√≠culo',\n                     title=\"üìä Matriz de Performance: Velocidade vs Quilometragem\")\n    st.plotly_chart(fig, use_container_width=True)\n\ndef show_compliance_preview(df, analyzer):\n    \"\"\"Preview do relat√≥rio de conformidade\"\"\"\n    st.markdown(\"### üö® An√°lise de Conformidade\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para an√°lise de conformidade.\")\n        return\n    \n    # An√°lise de velocidade\n    excesso_velocidade = df[df['velocidade_km'] > 80]\n    violacoes_criticas = df[df['velocidade_km'] > 100]\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        st.metric(\"‚ö†Ô∏è Excessos de Velocidade\", len(excesso_velocidade))\n    with col2:\n        st.metric(\"üö® Viola√ß√µes Cr√≠ticas\", len(violacoes_criticas))\n    with col3:\n        conformidade = (1 - len(excesso_velocidade) / len(df)) * 100 if len(df) > 0 else 100\n        st.metric(\"‚úÖ Taxa de Conformidade\", f\"{conformidade:.1f}%\")\n    \n    if len(excesso_velocidade) > 0:\n        # Ve√≠culos com mais viola√ß√µes\n        violacoes_por_veiculo = excesso_velocidade.groupby('placa').size().reset_index(name='Viola√ß√µes')\n        violacoes_por_veiculo = violacoes_por_veiculo.sort_values('Viola√ß√µes', ascending=False)\n        \n        fig = px.bar(violacoes_por_veiculo.head(10), x='placa', y='Viola√ß√µes',\n                     title=\"üö® Ve√≠culos com Mais Viola√ß√µes de Velocidade\")\n        st.plotly_chart(fig, use_container_width=True)\n\ndef show_trends_preview(df, analyzer):\n    \"\"\"Preview da an√°lise de tend√™ncias\"\"\"\n    st.markdown(\"### üìà An√°lise de Tend√™ncias\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para an√°lise de tend√™ncias.\")\n        return\n    \n    # Tend√™ncias por dia\n    df['dia'] = df['data'].dt.date\n    tendencias_diarias = df.groupby('dia').agg({\n        'velocidade_km': 'mean',\n        'data': 'count'\n    }).reset_index()\n    tendencias_diarias.columns = ['Data', 'Velocidade M√©dia', 'Registros']\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        fig1 = px.line(tendencias_diarias, x='Data', y='Velocidade M√©dia',\n                       title=\"üìà Tend√™ncia de Velocidade M√©dia\")\n        st.plotly_chart(fig1, use_container_width=True)\n        \n    with col2:\n        fig2 = px.line(tendencias_diarias, x='Data', y='Registros',\n                       title=\"üìä Tend√™ncia de Atividade\")\n        st.plotly_chart(fig2, use_container_width=True)\n\ndef show_custom_preview(df, analyzer):\n    \"\"\"Preview do relat√≥rio personalizado\"\"\"\n    st.markdown(\"### üîç Relat√≥rio Personalizado\")\n    st.info(\"Configure suas m√©tricas personalizadas na aba Configura√ß√µes\")\n\ndef show_report_dashboard(df, analyzer):\n    \"\"\"Dashboard interativo do relat√≥rio\"\"\"\n    st.subheader(\"üìä Dashboard Interativo\")\n    \n    # Verificar se h√° dados\n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado dispon√≠vel para o dashboard.\")\n        return\n    \n    # Filtros interativos\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        veiculos_selecionados = st.multiselect(\n            \"üöó Ve√≠culos:\",\n            options=sorted(df['placa'].unique()),\n            default=sorted(df['placa'].unique())[:5]  # Primeiros 5 por padr√£o\n        )\n    \n    with col2:\n        # Tratamento seguro para datas\n        try:\n            min_date = df['data'].min()\n            if pd.isna(min_date):\n                min_date = datetime.now().date() - timedelta(days=30)\n            else:\n                min_date = min_date.date()\n            data_inicio = st.date_input(\"üìÖ Data In√≠cio:\", value=min_date)\n        except:\n            data_inicio = st.date_input(\"üìÖ Data In√≠cio:\", value=datetime.now().date() - timedelta(days=30))\n    \n    with col3:\n        # Tratamento seguro para datas\n        try:\n            max_date = df['data'].max()\n            if pd.isna(max_date):\n                max_date = datetime.now().date()\n            else:\n                max_date = max_date.date()\n            data_fim = st.date_input(\"üìÖ Data Fim:\", value=max_date)\n        except:\n            data_fim = st.date_input(\"üìÖ Data Fim:\", value=datetime.now().date())\n    \n    # Filtrar dados\n    df_filtered = df[\n        (df['placa'].isin(veiculos_selecionados)) &\n        (df['data'].dt.date >= data_inicio) &\n        (df['data'].dt.date <= data_fim)\n    ]\n    \n    if df_filtered.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado encontrado com os filtros aplicados\")\n        return\n    \n    # M√©tricas principais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üìä Registros Filtrados\", f\"{len(df_filtered):,}\")\n    with col2:\n        vel_media = df_filtered['velocidade_km'].mean()\n        st.metric(\"‚ö° Velocidade M√©dia\", f\"{vel_media:.1f} km/h\")\n    with col3:\n        km_total = df_filtered['odometro_periodo_km'].sum()\n        st.metric(\"üõ£Ô∏è KM Total\", f\"{km_total:.1f}\")\n    with col4:\n        gps_coverage = (df_filtered['gps'].sum() / len(df_filtered)) * 100\n        st.metric(\"üì° GPS Coverage\", f\"{gps_coverage:.1f}%\")\n    \n    # Gr√°ficos principais\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        # Distribui√ß√£o de velocidade\n        fig1 = px.histogram(df_filtered, x='velocidade_km', nbins=30,\n                           title=\"üìä Distribui√ß√£o de Velocidade\")\n        st.plotly_chart(fig1, use_container_width=True)\n    \n    with col2:\n        # Atividade por hora\n        df_filtered['hora'] = df_filtered['data'].dt.hour\n        atividade_hora = df_filtered.groupby('hora').size().reset_index(name='Registros')\n        fig2 = px.bar(atividade_hora, x='hora', y='Registros',\n                      title=\"üìà Atividade por Hora do Dia\")\n        st.plotly_chart(fig2, use_container_width=True)\n\ndef show_advanced_settings():\n    \"\"\"Configura√ß√µes avan√ßadas do relat√≥rio\"\"\"\n    st.subheader(\"‚öôÔ∏è Configura√ß√µes Avan√ßadas\")\n    \n    # Configura√ß√µes de m√©tricas\n    st.markdown(\"### üìä M√©tricas Personalizadas\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.checkbox(\"üìà Incluir an√°lise de tend√™ncias\", value=True)\n        st.checkbox(\"üö® Incluir alertas de seguran√ßa\", value=True)\n        st.checkbox(\"‚ö° Incluir m√©tricas de performance\", value=True)\n        st.checkbox(\"üó∫Ô∏è Incluir an√°lise geogr√°fica\", value=False)\n    \n    with col2:\n        st.number_input(\"üèéÔ∏è Limite de velocidade (km/h):\", min_value=50, max_value=120, value=80)\n        st.number_input(\"üîã Bateria baixa (V):\", min_value=10.0, max_value=15.0, value=12.0)\n        st.slider(\"üìä N√∫mero de ve√≠culos no top ranking:\", 5, 20, 10)\n        \n    # Configura√ß√µes de formato\n    st.markdown(\"### üìÑ Formata√ß√£o do Relat√≥rio\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        st.selectbox(\"üé® Tema do relat√≥rio:\", [\"Profissional\", \"Moderno\", \"Cl√°ssico\"])\n        st.selectbox(\"üìä Estilo dos gr√°ficos:\", [\"Plotly\", \"Matplotlib\", \"Seaborn\"])\n    \n    with col2:\n        st.selectbox(\"üìÑ Tamanho da p√°gina:\", [\"A4\", \"Letter\", \"A3\"])\n        st.selectbox(\"üî§ Idioma:\", [\"Portugu√™s\", \"English\", \"Espa√±ol\"])\n\ndef show_download_options(df, summary, analyzer, tipo_relatorio, formato_saida, incluir_graficos):\n    \"\"\"Op√ß√µes de download do relat√≥rio\"\"\"\n    st.subheader(\"üì• Download do Relat√≥rio\")\n    \n    # Informa√ß√µes do relat√≥rio a ser gerado\n    st.markdown(f\"**üìä Tipo:** {tipo_relatorio}\")\n    st.markdown(f\"**üìÑ Formato:** {formato_saida}\")\n    st.markdown(f\"**üìÖ Per√≠odo:** {len(df):,} registros\")\n    st.markdown(f\"**üöó Ve√≠culos:** {df['placa'].nunique()}\")\n    \n    # Bot√µes de gera√ß√£o\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        if st.button(\"üìÑ Gerar PDF Profissional\", type=\"primary\"):\n            with st.spinner(\"üîÑ Gerando relat√≥rio PDF...\"):\n                try:\n                    generator = PDFReportGenerator()\n                    # Usar m√©todo existente at√© implementar o avan√ßado\n                    pdf_path = generator.generate_fleet_report()\n                    \n                    if os.path.exists(pdf_path):\n                        with open(pdf_path, \"rb\") as file:\n                            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n                            filename = f\"relatorio_{tipo_relatorio.split()[1].lower()}_{timestamp}.pdf\"\n                            \n                            st.download_button(\n                                label=\"‚¨áÔ∏è Baixar PDF\",\n                                data=file.read(),\n                                file_name=filename,\n                                mime=\"application/pdf\"\n                            )\n                        st.success(\"‚úÖ Relat√≥rio PDF gerado com sucesso!\")\n                    else:\n                        st.error(\"‚ùå Erro ao gerar relat√≥rio PDF\")\n                except Exception as e:\n                    st.error(f\"‚ùå Erro: {str(e)}\")\n    \n    with col2:\n        if st.button(\"üìä Exportar Dados CSV\"):\n            csv_data = df.to_csv(index=False)\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            filename = f\"dados_frota_{timestamp}.csv\"\n            \n            st.download_button(\n                label=\"‚¨áÔ∏è Baixar CSV\",\n                data=csv_data,\n                file_name=filename,\n                mime=\"text/csv\"\n            )\n            st.success(\"‚úÖ Dados CSV prontos para download!\")\n    \n    with col3:\n        if st.button(\"üìà Relat√≥rio Completo\"):\n            st.info(\"üöß Gerando relat√≥rio completo (PDF + CSV + Gr√°ficos)...\")\n            st.markdown(\"*Funcionalidade em desenvolvimento*\")\n    \n    # Hist√≥rico de relat√≥rios\n    st.markdown(\"### üìã Hist√≥rico de Relat√≥rios\")\n    st.info(\"üìÅ √öltimos relat√≥rios gerados aparecer√£o aqui\")\n\n# ========== SE√á√ÉO PRINCIPAL - EXECUTADA AP√ìS DEFINI√á√ïES ==========\n# Se√ß√£o principal de gera√ß√£o de relat√≥rios\nst.header(\"üìä Gera√ß√£o de Relat√≥rios\")\n\n# Tabs para diferentes visualiza√ß√µes\ntab1, tab2, tab3, tab4 = st.tabs([\"üìã Pr√©-visualiza√ß√£o\", \"üìä Dashboard\", \"‚öôÔ∏è Configura√ß√µes\", \"üì• Download\"])\n\nwith tab1:\n    show_report_preview(df_filtered, summary, analyzer, tipo_relatorio)\n\nwith tab2:\n    show_report_dashboard(df_filtered, analyzer)\n\nwith tab3:\n    show_advanced_settings()\n\nwith tab4:\n    show_download_options(df_filtered, summary, analyzer, tipo_relatorio, formato_saida, incluir_graficos)","size_bytes":21086},"utils/alert_system.py":{"content":"\"\"\"Sistema de Alertas em Tempo Real\"\"\"\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any\nfrom database.db_manager import DatabaseManager\n\nclass AlertSystem:\n    def __init__(self):\n        self.alert_configs = {\n            'velocidade_maxima': 80,\n            'velocidade_critica': 100,\n            'bateria_baixa': 12,\n            'bateria_critica': 11,\n            'tempo_parado_max': 180,  # minutos\n            'uso_noturno_inicio': 22,\n            'uso_noturno_fim': 6\n        }\n    \n    def check_realtime_alerts(self) -> List[Dict[str, Any]]:\n        \"\"\"Verifica alertas em tempo real\"\"\"\n        alerts = []\n        df = DatabaseManager.get_dashboard_data()\n        \n        if df.empty:\n            return alerts\n        \n        # Validar e converter coluna de data\n        if 'data' not in df.columns:\n            return alerts\n            \n        # Converter para datetime de forma segura\n        df = df.copy()\n        df['data'] = pd.to_datetime(df['data'], errors='coerce')\n        \n        # Remover registros com data inv√°lida\n        df = df.dropna(subset=['data'])\n        \n        if df.empty:\n            return alerts\n        \n        # Filtrar √∫ltimas 24h - timezone correto\n        from datetime import timezone\n        cutoff = datetime.now(timezone.utc) - timedelta(hours=24)\n        \n        # Verificar se dados t√™m timezone\n        if df['data'].dt.tz is not None:\n            # Se dados n√£o est√£o em UTC, converter\n            if df['data'].dt.tz != timezone.utc:\n                cutoff = cutoff.astimezone(df['data'].dt.tz)\n        else:\n            # Se dados s√£o naive, usar cutoff naive\n            cutoff = cutoff.replace(tzinfo=None)\n        \n        df_recent = df[df['data'] >= cutoff]\n        \n        # Alertas de velocidade\n        speed_alerts = self._check_speed_alerts(df_recent)\n        alerts.extend(speed_alerts)\n        \n        # Alertas de bateria\n        battery_alerts = self._check_battery_alerts(df_recent)\n        alerts.extend(battery_alerts)\n        \n        # Alertas de uso noturno\n        night_alerts = self._check_night_usage(df_recent)\n        alerts.extend(night_alerts)\n        \n        return alerts\n    \n    def _check_speed_alerts(self, df: pd.DataFrame) -> List[Dict]:\n        alerts = []\n        if 'velocidade_km' not in df.columns:\n            return alerts\n            \n        # Filtrar apenas registros com velocidade > 0 para alertas\n        df_moving = df[df['velocidade_km'] > 0]\n        \n        for _, row in df_moving.iterrows():\n            vel = float(row['velocidade_km'])\n            if vel > self.alert_configs['velocidade_critica']:\n                alerts.append({\n                    'tipo': 'Velocidade Cr√≠tica',\n                    'severidade': 'Alta',\n                    'veiculo': row['placa'],\n                    'valor': f\"{vel:.1f} km/h\",\n                    'timestamp': row['data'],\n                    'localizacao': row.get('endereco', 'Localiza√ß√£o n√£o dispon√≠vel')\n                })\n            elif vel > self.alert_configs['velocidade_maxima']:\n                alerts.append({\n                    'tipo': 'Excesso de Velocidade',\n                    'severidade': 'M√©dia',\n                    'veiculo': row['placa'],\n                    'valor': f\"{vel:.1f} km/h\",\n                    'timestamp': row['data'],\n                    'localizacao': row.get('endereco', 'Localiza√ß√£o n√£o dispon√≠vel')\n                })\n        return alerts\n    \n    def _check_battery_alerts(self, df: pd.DataFrame) -> List[Dict]:\n        alerts = []\n        if 'battery_level' in df.columns:\n            # Converter battery_level para num√©rico (vem como string)\n            df_copy = df.copy()\n            df_copy['bateria_num'] = pd.to_numeric(df_copy['battery_level'], errors='coerce')\n            \n            # Filtrar apenas valores v√°lidos\n            df_copy = df_copy.dropna(subset=['bateria_num'])\n            \n            if not df_copy.empty:\n                # Alertas de bateria baixa (abaixo de 12V)\n                low_battery = df_copy[df_copy['bateria_num'] < self.alert_configs['bateria_baixa']]\n                \n                for _, row in low_battery.iterrows():\n                    bat = row['bateria_num']\n                    severity = 'Alta' if bat < self.alert_configs['bateria_critica'] else 'M√©dia'\n                    alerts.append({\n                        'tipo': 'Bateria Baixa',\n                        'severidade': severity,\n                        'veiculo': row['placa'],\n                        'valor': f\"{bat:.1f}V\",\n                        'timestamp': row['data'],\n                        'localizacao': row.get('endereco', 'Localiza√ß√£o n√£o dispon√≠vel')\n                    })\n        return alerts\n    \n    def _check_night_usage(self, df: pd.DataFrame) -> List[Dict]:\n        alerts = []\n        if 'data' not in df.columns:\n            return alerts\n            \n        # Trabalhar com c√≥pia para n√£o modificar original\n        df_copy = df.copy()\n        \n        # Converter para datetime se necess√°rio\n        if not pd.api.types.is_datetime64_any_dtype(df_copy['data']):\n            df_copy['data'] = pd.to_datetime(df_copy['data'], errors='coerce')\n            \n        # Remover registros com data inv√°lida\n        df_copy = df_copy.dropna(subset=['data'])\n        \n        if df_copy.empty:\n            return alerts\n            \n        df_copy['hora'] = df_copy['data'].dt.hour\n        night_usage = df_copy[\n            (df_copy['hora'] >= self.alert_configs['uso_noturno_inicio']) | \n            (df_copy['hora'] <= self.alert_configs['uso_noturno_fim'])\n        ]\n        \n        if len(night_usage) > 10:  # Mais de 10 registros noturnos\n            for veiculo in night_usage['placa'].unique():\n                count = len(night_usage[night_usage['placa'] == veiculo])\n                if count > 5:\n                    alerts.append({\n                        'tipo': 'Uso Noturno Frequente',\n                        'severidade': 'Baixa',\n                        'veiculo': veiculo,\n                        'valor': f\"{count} ocorr√™ncias\",\n                        'timestamp': night_usage['data'].max(),\n                        'localizacao': 'M√∫ltiplas'\n                    })\n        return alerts\n    \n    def get_alert_summary(self) -> Dict[str, Any]:\n        \"\"\"Resumo dos alertas\"\"\"\n        alerts = self.check_realtime_alerts()\n        summary = {\n            'total_alerts': len(alerts),\n            'high_severity': len([a for a in alerts if a['severidade'] == 'Alta']),\n            'medium_severity': len([a for a in alerts if a['severidade'] == 'M√©dia']),\n            'low_severity': len([a for a in alerts if a['severidade'] == 'Baixa']),\n            'by_type': {}\n        }\n        \n        for alert in alerts:\n            tipo = alert['tipo']\n            if tipo not in summary['by_type']:\n                summary['by_type'][tipo] = 0\n            summary['by_type'][tipo] += 1\n            \n        return summary","size_bytes":7053},"utils/csv_processor.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport streamlit as st\nimport os\nimport re\nimport sys\nsys.path.append('.')\nfrom database.db_manager import DatabaseManager\n\nclass CSVProcessor:\n    \"\"\"Classe para processar arquivos CSV de dados telem√°ticos\"\"\"\n    \n    # Campos obrigat√≥rios do CSV\n    REQUIRED_FIELDS = [\n        'Cliente', 'Placa', 'Ativo', 'Data', 'Data (GPRS)',\n        'Velocidade (Km)', 'Igni√ß√£o', 'Motorista', 'GPS', 'Gprs',\n        'Localiza√ß√£o', 'Endere√ßo', 'Tipo do Evento', 'Cerca',\n        'Saida', 'Entrada', 'Pacote', 'Od√¥metro do per√≠odo (Km)',\n        'Hor√≠metro do per√≠odo', 'Hor√≠metro embarcado',\n        'Od√¥metro embarcado (Km)', 'Bateria', 'Imagem', 'Tens√£o', 'Bloqueado'\n    ]\n    \n    def __init__(self):\n        \"\"\"Inicializa o processador de CSV\"\"\"\n        self.data = None\n        self.validation_errors = []\n        self.use_database = True  # Enable database integration\n        \n    def normalize_column_name(self, col_name):\n        \"\"\"Normaliza nome da coluna removendo espa√ßos extras e caracteres especiais\"\"\"\n        # Remover espa√ßos extras e normalizar\n        normalized = re.sub(r'\\s+', ' ', str(col_name).strip())\n        return normalized\n    \n    def validate_csv_structure(self, df):\n        \"\"\"Valida a estrutura do CSV\"\"\"\n        self.validation_errors = []\n        \n        # Normalizar nomes das colunas - handle double spaces and irregular spacing\n        df.columns = [' '.join(col.strip().split()) for col in df.columns]\n        \n        # Criar mapeamento de colunas normalizadas\n        normalized_columns = {self.normalize_column_name(col): col for col in df.columns}\n        \n        # Verificar se todos os campos obrigat√≥rios est√£o presentes\n        missing_fields = []\n        for field in self.REQUIRED_FIELDS:\n            field_normalized = self.normalize_column_name(field)\n            if field_normalized not in normalized_columns:\n                missing_fields.append(field)\n        \n        if missing_fields:\n            self.validation_errors.append(f\"Campos obrigat√≥rios ausentes: {', '.join(missing_fields)}\")\n        \n        # Verificar se h√° dados\n        if df.empty:\n            self.validation_errors.append(\"O arquivo CSV est√° vazio\")\n        \n        # Verificar tipos de dados b√°sicos\n        try:\n            # Tentar converter datas (formato brasileiro)\n            if 'Data' in df.columns:\n                pd.to_datetime(df['Data'], errors='coerce', dayfirst=True)\n            if 'Data (GPRS)' in df.columns:\n                pd.to_datetime(df['Data (GPRS)'], errors='coerce', dayfirst=True)\n        except Exception as e:\n            self.validation_errors.append(f\"Erro na valida√ß√£o de datas: {str(e)}\")\n        \n        return len(self.validation_errors) == 0\n    \n    def clean_and_standardize_data(self, df):\n        \"\"\"Limpa e padroniza os dados do CSV\"\"\"\n        try:\n            # Criar c√≥pia do DataFrame\n            clean_df = df.copy()\n            \n            # Padronizar nomes das colunas\n            column_mapping = {\n                'Cliente': 'cliente',\n                'Placa': 'placa',\n                'Ativo': 'ativo',\n                'Data': 'data',\n                'Data (GPRS)': 'data_gprs',\n                'Velocidade (Km)': 'velocidade_km',\n                'Igni√ß√£o': 'ignicao',\n                'Motorista': 'motorista',\n                'GPS': 'gps',\n                'Gprs': 'gprs',\n                'Localiza√ß√£o': 'localizacao',\n                'Endere√ßo': 'endereco',\n                'Tipo do Evento': 'tipo_evento',\n                'Cerca': 'cerca',\n                'Saida': 'saida',\n                'Entrada': 'entrada',\n                'Pacote': 'pacote',\n                'Od√¥metro do per√≠odo (Km)': 'odometro_periodo_km',\n                'Od√¥metro do per√≠odo  (Km)': 'odometro_periodo_km',  # Handle double space variant\n                'Hor√≠metro do per√≠odo': 'horimetro_periodo',\n                'Hor√≠metro embarcado': 'horimetro_embarcado',\n                'Od√¥metro embarcado (Km)': 'odometro_embarcado_km',\n                'Bateria': 'bateria',\n                'Imagem': 'imagem',\n                'Tens√£o': 'tensao',\n                'Bloqueado': 'bloqueado'\n            }\n            \n            clean_df = clean_df.rename(columns=column_mapping)\n            \n            # Converter datas com formato brasileiro (DD/MM/YYYY)\n            clean_df['data'] = pd.to_datetime(clean_df['data'], errors='coerce', dayfirst=True)\n            clean_df['data_gprs'] = pd.to_datetime(clean_df['data_gprs'], errors='coerce', dayfirst=True)\n            \n            # Converter velocidade para num√©rico\n            clean_df['velocidade_km'] = pd.to_numeric(clean_df['velocidade_km'], errors='coerce')\n            \n            # Converter GPS e GPRS para booleano\n            clean_df['gps'] = clean_df['gps'].astype(str).str.strip() == '1'\n            clean_df['gprs'] = clean_df['gprs'].astype(str).str.strip() == '1'\n            \n            # Converter bloqueado para booleano\n            clean_df['bloqueado'] = clean_df['bloqueado'].astype(str).str.strip() == '1'\n            \n            # Converter od√¥metros para num√©rico\n            clean_df['odometro_periodo_km'] = pd.to_numeric(clean_df['odometro_periodo_km'], errors='coerce')\n            clean_df['odometro_embarcado_km'] = pd.to_numeric(clean_df['odometro_embarcado_km'], errors='coerce')\n            \n            # Converter tens√£o para num√©rico\n            clean_df['tensao'] = pd.to_numeric(clean_df['tensao'], errors='coerce')\n            \n            # Limpar campos de texto\n            text_fields = ['cliente', 'placa', 'ativo', 'motorista', 'endereco', 'tipo_evento']\n            for field in text_fields:\n                if field in clean_df.columns:\n                    clean_df[field] = clean_df[field].astype(str).str.strip()\n            \n            # Padronizar placas (remover espa√ßos e converter para mai√∫sculas)\n            clean_df['placa'] = clean_df['placa'].str.upper().str.replace(' ', '')\n            \n            # Adicionar timestamp de processamento\n            clean_df['processed_at'] = datetime.now()\n            \n            return clean_df\n            \n        except Exception as e:\n            self.validation_errors.append(f\"Erro na limpeza dos dados: {str(e)}\")\n            return None\n    \n    def convert_time_to_hours(self, time_str):\n        \"\"\"Converte string de tempo (HH:MM:SS) para horas decimais\"\"\"\n        if pd.isna(time_str) or time_str == '':\n            return 0.0\n        \n        try:\n            time_str = str(time_str).strip()\n            if ':' in time_str:\n                parts = time_str.split(':')\n                hours = int(parts[0])\n                minutes = int(parts[1]) if len(parts) > 1 else 0\n                seconds = int(parts[2]) if len(parts) > 2 else 0\n                return hours + minutes/60 + seconds/3600\n            else:\n                return float(time_str)\n        except:\n            return 0.0\n    \n    def process_csv_file(self, uploaded_file):\n        \"\"\"Processa arquivo CSV completo\"\"\"\n        try:\n            # Detectar separador e encoding do arquivo\n            separator = ','\n            encoding = 'utf-8'\n            \n            # Tentar diferentes separadores e encodings\n            sample = uploaded_file.read(1024)\n            uploaded_file.seek(0)\n            \n            if isinstance(sample, bytes):\n                sample_str = sample.decode('utf-8', errors='ignore')\n            else:\n                sample_str = sample\n            \n            # Detectar separador (v√≠rgula vs ponto e v√≠rgula)\n            if sample_str.count(';') > sample_str.count(','):\n                separator = ';'\n            \n            # Tentar ler com diferentes encodings\n            df = None\n            encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'windows-1252', 'cp1252']\n            \n            for enc in encodings_to_try:\n                try:\n                    uploaded_file.seek(0)\n                    df = pd.read_csv(uploaded_file, sep=separator, encoding=enc)\n                    encoding = enc\n                    break\n                except:\n                    continue\n            \n            if df is None:\n                # Fall back para leitura padr√£o\n                uploaded_file.seek(0)\n                df = pd.read_csv(uploaded_file, sep=separator)\n            \n            st.info(f\"üìÑ Arquivo lido com separador '{separator}' e encoding '{encoding}'\")\n            \n            # Validar estrutura\n            if not self.validate_csv_structure(df):\n                return None, self.validation_errors\n            \n            # Limpar e padronizar dados\n            clean_df = self.clean_and_standardize_data(df)\n            \n            if clean_df is None:\n                return None, self.validation_errors\n            \n            # Converter hor√≠metros\n            if 'horimetro_periodo' in clean_df.columns:\n                clean_df['horimetro_periodo_horas'] = clean_df['horimetro_periodo'].apply(\n                    self.convert_time_to_hours\n                )\n            \n            # Salvar dados processados\n            filename = getattr(uploaded_file, 'name', 'uploaded_file.csv')\n            self.save_processed_data(clean_df, filename)\n            \n            self.data = clean_df\n            return clean_df, []\n            \n        except Exception as e:\n            error_msg = f\"Erro no processamento do arquivo: {str(e)}\"\n            return None, [error_msg]\n    \n    def save_processed_data(self, df, filename=None):\n        \"\"\"Salva dados processados na base de dados e como backup\"\"\"\n        try:\n            # Salvar na base de dados se habilitado\n            if self.use_database:\n                result = DatabaseManager.migrate_csv_to_database_from_df(df, filename)\n                if not result['success']:\n                    st.error(f\"Erro ao salvar na base de dados: {result['error']}\")\n                    # Fall back to file save\n                    self.use_database = False\n            \n            # Sempre manter backup em arquivo\n            os.makedirs('data', exist_ok=True)\n            df.to_csv('data/processed_data.csv', index=False)\n            \n            # Salvar backup com timestamp\n            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n            df.to_csv(f'data/backup_{timestamp}.csv', index=False)\n            \n        except Exception as e:\n            st.error(f\"Erro ao salvar dados: {str(e)}\")\n    \n    def get_data_summary(self, df):\n        \"\"\"Gera resumo dos dados processados\"\"\"\n        if df is None or df.empty:\n            return {}\n        \n        summary = {\n            'total_registros': len(df),\n            'total_veiculos': df['placa'].nunique(),\n            'total_clientes': df['cliente'].nunique(),\n            'periodo_inicio': df['data'].min(),\n            'periodo_fim': df['data'].max(),\n            'velocidade_media': df['velocidade_km'].mean(),\n            'velocidade_maxima': df['velocidade_km'].max(),\n            'total_km_periodo': df['odometro_periodo_km'].sum(),\n            'registros_com_gps': df['gps'].sum(),\n            'registros_sem_gps': (~df['gps']).sum(),\n            'veiculos_bloqueados': df['bloqueado'].sum()\n        }\n        \n        return summary\n","size_bytes":11356},"utils/data_analyzer.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport sys\nsys.path.append('.')\nfrom database.db_manager import DatabaseManager\n\nclass DataAnalyzer:\n    \"\"\"Classe para an√°lise de dados de frota\"\"\"\n    \n    def __init__(self, df):\n        \"\"\"Inicializa o analisador com DataFrame\"\"\"\n        self.df = df\n        self.filtered_df = df.copy()\n    \n    @classmethod\n    def from_database(cls, cliente=None, placa=None, data_inicio=None, data_fim=None):\n        \"\"\"Cria uma inst√¢ncia do analisador usando dados da base de dados\"\"\"\n        try:\n            # Buscar dados da base de dados com filtros\n            df = DatabaseManager.get_dashboard_data(\n                client_filter=cliente if cliente != \"Todos\" else None,\n                vehicle_filter=placa if placa != \"Todos\" else None,\n                start_date=data_inicio,\n                end_date=data_fim\n            )\n            \n            if not df.empty:\n                print(f\"‚úÖ DataAnalyzer: {len(df):,} registros carregados da base PostgreSQL\")\n            \n            return cls(df)\n        except Exception as e:\n            print(f\"‚ùå Erro ao carregar dados: {str(e)}\")\n            # Em caso de erro, retornar analisador com DataFrame vazio\n            empty_df = pd.DataFrame(columns=[\n                'cliente', 'placa', 'data', 'velocidade_km', 'odometro_periodo_km',\n                'gps', 'bloqueado', 'engine_hours_period'\n            ])\n            return cls(empty_df)\n    \n    def apply_filters(self, cliente=None, placa=None, data_inicio=None, data_fim=None):\n        \"\"\"Aplica filtros aos dados com tratamento robusto para 'TODOS'\"\"\"\n        try:\n            filtered = self.df.copy()\n            \n            # Filtro por cliente - garantir que \"Todos\" n√£o cause problemas\n            if cliente and cliente not in [\"Todos\", \"TODOS\", None]:\n                filtered = filtered[filtered['cliente'] == cliente]\n            \n            # Filtro por placa - garantir que \"Todos\" n√£o cause problemas  \n            if placa and placa not in [\"Todos\", \"TODOS\", None]:\n                filtered = filtered[filtered['placa'] == placa]\n        \n        except Exception as e:\n            # Em caso de erro, retornar dados originais sem filtros\n            print(f\"Erro ao aplicar filtros: {str(e)}\")\n            filtered = self.df.copy()\n        \n        # Verificar se h√° dados e se a coluna 'data' existe e √© datetime\n        if not filtered.empty and 'data' in filtered.columns:\n            try:\n                # Garantir que a coluna 'data' √© datetime\n                if not pd.api.types.is_datetime64_any_dtype(filtered['data']):\n                    filtered['data'] = pd.to_datetime(filtered['data'], errors='coerce')\n                \n                if data_inicio:\n                    # Converter data_inicio para o mesmo timezone dos dados\n                    try:\n                        if hasattr(filtered['data'].dtype, 'tz') and filtered['data'].dt.tz is not None:\n                            # Dados t√™m timezone, converter data_inicio\n                            data_inicio_tz = pd.Timestamp(data_inicio).tz_localize('UTC')\n                        else:\n                            # Dados sem timezone, usar timestamp simples\n                            data_inicio_tz = pd.Timestamp(data_inicio)\n                        filtered = filtered[filtered['data'] >= data_inicio_tz]\n                    except Exception:\n                        # Em caso de erro, usar compara√ß√£o simples\n                        data_inicio_tz = pd.Timestamp(data_inicio)\n                        filtered = filtered[filtered['data'] >= data_inicio_tz]\n                \n                if data_fim:\n                    # Converter data_fim para o mesmo timezone dos dados\n                    try:\n                        if hasattr(filtered['data'].dtype, 'tz') and filtered['data'].dt.tz is not None:\n                            # Dados t√™m timezone, converter data_fim\n                            data_fim_tz = pd.Timestamp(data_fim).tz_localize('UTC')\n                        else:\n                            # Dados sem timezone, usar timestamp simples\n                            data_fim_tz = pd.Timestamp(data_fim)\n                        \n                        # CORRE√á√ÉO: Se mesmo dia, incluir todo o dia at√© 23:59:59\n                        if data_inicio and pd.Timestamp(data_inicio).date() == pd.Timestamp(data_fim).date():\n                            data_fim_tz = data_fim_tz.replace(hour=23, minute=59, second=59, microsecond=999999)\n                        \n                        filtered = filtered[filtered['data'] <= data_fim_tz]\n                    except Exception:\n                        # Em caso de erro, usar compara√ß√£o simples\n                        data_fim_tz = pd.Timestamp(data_fim)\n                        \n                        # CORRE√á√ÉO: Se mesmo dia, incluir todo o dia at√© 23:59:59\n                        if data_inicio and pd.Timestamp(data_inicio).date() == pd.Timestamp(data_fim).date():\n                            data_fim_tz = data_fim_tz.replace(hour=23, minute=59, second=59, microsecond=999999)\n                        \n                        filtered = filtered[filtered['data'] <= data_fim_tz]\n                        \n            except Exception:\n                # Se houver erro com datetime, n√£o aplicar filtros de data\n                pass\n        \n        self.filtered_df = filtered\n        return filtered\n    \n    def _calculate_total_hours(self, time_series):\n        \"\"\"Converte s√©rie de tempo (HH:MM:SS) para total de horas\"\"\"\n        total_hours = 0.0\n        for time_str in time_series:\n            if pd.isna(time_str) or time_str == '' or time_str == '0':\n                continue\n            try:\n                # Parse formato HH:MM:SS\n                parts = str(time_str).split(':')\n                if len(parts) >= 3:\n                    hours = int(parts[0])\n                    minutes = int(parts[1])\n                    seconds = int(parts[2])\n                    total_hours += hours + (minutes / 60.0) + (seconds / 3600.0)\n                elif len(parts) == 2:  # HH:MM\n                    hours = int(parts[0])\n                    minutes = int(parts[1])\n                    total_hours += hours + (minutes / 60.0)\n                elif len(parts) == 1:  # Apenas horas\n                    total_hours += float(parts[0])\n            except (ValueError, IndexError):\n                # Ignorar valores inv√°lidos\n                continue\n        return float(total_hours)\n    \n    def get_kpis(self):\n        \"\"\"Calcula KPIs principais\"\"\"\n        if self.filtered_df.empty:\n            return {}\n        \n        df = self.filtered_df\n        \n        kpis = {\n            'total_veiculos': int(df['placa'].nunique()),\n            'total_registros': int(len(df)),\n            'velocidade_media': float(df['velocidade_km'].mean()),\n            'velocidade_maxima': float(df['velocidade_km'].max()),\n            'distancia_total': float(df['odometro_periodo_km'].sum()),\n            'tempo_ativo_horas': self._calculate_total_hours(df['engine_hours_period']) if 'engine_hours_period' in df.columns else 0.0,\n            'cobertura_gps': float((df['gps'].sum() / len(df)) * 100),\n            'veiculos_bloqueados': int(df['bloqueado'].sum()),\n            'periodo_dias': int((df['data'].max() - df['data'].min()).days + 1) if len(df) > 0 else 0\n        }\n        \n        return kpis\n    \n    def get_speed_analysis(self):\n        \"\"\"An√°lise de velocidade\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Definir faixas de velocidade\n        conditions = [\n            (df['velocidade_km'] == 0),\n            (df['velocidade_km'] > 0) & (df['velocidade_km'] <= 40),\n            (df['velocidade_km'] > 40) & (df['velocidade_km'] <= 60),\n            (df['velocidade_km'] > 60) & (df['velocidade_km'] <= 80),\n            (df['velocidade_km'] > 80)\n        ]\n        choices = ['Parado', 'Baixa (1-40)', 'Moderada (41-60)', 'Alta (61-80)', 'Muito Alta (80+)']\n        \n        df['faixa_velocidade'] = np.select(conditions, choices, default='Indefinido')\n        \n        speed_dist = df['faixa_velocidade'].value_counts()\n        \n        return {\n            'distribuicao': speed_dist,\n            'velocidade_media_por_veiculo': df.groupby('placa')['velocidade_km'].mean().sort_values(ascending=False),\n            'velocidade_maxima_por_veiculo': df.groupby('placa')['velocidade_km'].max().sort_values(ascending=False),\n            'velocidade_por_hora': df.groupby(df['data'].dt.hour)['velocidade_km'].mean()\n        }\n    \n    def get_operational_analysis(self):\n        \"\"\"An√°lise operacional\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # An√°lise por ve√≠culo\n        agg_dict = {\n            'velocidade_km': ['mean', 'max', 'count'],\n            'odometro_periodo_km': 'sum',\n            'gps': 'mean',\n            'bloqueado': 'any'\n        }\n        \n        # Adicionar engine_hours_period apenas se existir na coluna\n        if 'engine_hours_period' in df.columns:\n            agg_dict['engine_hours_period'] = 'sum'\n        \n        vehicle_stats = df.groupby('placa').agg(agg_dict).round(2)\n        \n        # Achatamento do MultiIndex\n        vehicle_stats.columns = ['_'.join(col) for col in vehicle_stats.columns]\n        vehicle_stats = vehicle_stats.reset_index()\n        \n        # An√°lise temporal\n        daily_stats = df.groupby(df['data'].dt.date).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean',\n            'odometro_periodo_km': 'sum'\n        }).reset_index()\n        \n        # An√°lise de utiliza√ß√£o por hora\n        hourly_usage = df.groupby(df['data'].dt.hour).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean'\n        }).reset_index()\n        \n        return {\n            'estatisticas_por_veiculo': vehicle_stats,\n            'estatisticas_diarias': daily_stats,\n            'utilizacao_por_hora': hourly_usage,\n            'total_km_por_veiculo': df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False)\n        }\n    \n    def get_compliance_analysis(self):\n        \"\"\"An√°lise de compliance/conformidade\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Definir limites de compliance\n        SPEED_LIMIT = 80  # km/h\n        MIN_GPS_COVERAGE = 95  # %\n        \n        # An√°lise de excesso de velocidade\n        speed_violations = df[df['velocidade_km'] > SPEED_LIMIT]\n        \n        # An√°lise de cobertura GPS\n        gps_coverage_by_vehicle = df.groupby('placa')['gps'].mean() * 100\n        low_gps_vehicles = gps_coverage_by_vehicle[gps_coverage_by_vehicle < MIN_GPS_COVERAGE]\n        \n        # Ve√≠culos com problemas\n        blocked_vehicles = df[df['bloqueado'] == True]['placa'].unique()\n        \n        # Score de compliance por ve√≠culo\n        compliance_scores = {}\n        for placa in df['placa'].unique():\n            vehicle_data = df[df['placa'] == placa]\n            \n            # Pontua√ß√£o baseada em crit√©rios\n            speed_score = 100 - (len(vehicle_data[vehicle_data['velocidade_km'] > SPEED_LIMIT]) / len(vehicle_data)) * 100\n            gps_score = (vehicle_data['gps'].mean() * 100)\n            block_score = 100 if not vehicle_data['bloqueado'].any() else 0\n            \n            overall_score = (speed_score * 0.4 + gps_score * 0.4 + block_score * 0.2)\n            compliance_scores[placa] = round(overall_score, 2)\n        \n        return {\n            'violacoes_velocidade': len(speed_violations),\n            'veiculos_baixo_gps': len(low_gps_vehicles),\n            'veiculos_bloqueados': len(blocked_vehicles),\n            'score_compliance': compliance_scores,\n            'detalhes_violacoes': speed_violations.groupby('placa').size().sort_values(ascending=False),\n            'cobertura_gps_por_veiculo': gps_coverage_by_vehicle.sort_values(ascending=True)\n        }\n    \n    def compare_vehicles(self, placas_list):\n        \"\"\"Compara m√∫ltiplos ve√≠culos\"\"\"\n        if not placas_list or len(placas_list) < 2:\n            return {}\n        \n        comparison_data = {}\n        \n        for placa in placas_list:\n            vehicle_data = self.filtered_df[self.filtered_df['placa'] == placa]\n            \n            if not vehicle_data.empty:\n                comparison_data[placa] = {\n                    'total_registros': len(vehicle_data),\n                    'velocidade_media': vehicle_data['velocidade_km'].mean(),\n                    'velocidade_maxima': vehicle_data['velocidade_km'].max(),\n                    'distancia_total': vehicle_data['odometro_periodo_km'].sum(),\n                    'tempo_ativo': vehicle_data['engine_hours_period'].sum() if 'engine_hours_period' in vehicle_data.columns else 0,\n                    'cobertura_gps': (vehicle_data['gps'].mean() * 100),\n                    'violacoes_velocidade': len(vehicle_data[vehicle_data['velocidade_km'] > 80]),\n                    'bloqueios': vehicle_data['bloqueado'].sum()\n                }\n        \n        return comparison_data\n    \n    def get_temporal_patterns(self):\n        \"\"\"An√°lise de padr√µes temporais\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Padr√µes por dia da semana\n        df['dia_semana'] = df['data'].dt.day_name()\n        weekly_patterns = df.groupby('dia_semana').agg({\n            'velocidade_km': 'mean',\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        })\n        \n        # Padr√µes por hora do dia\n        hourly_patterns = df.groupby(df['data'].dt.hour).agg({\n            'velocidade_km': 'mean',\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        })\n        \n        # Padr√µes mensais - usando year+month para evitar warning de timezone\n        monthly_patterns = df.groupby([df['data'].dt.year, df['data'].dt.month]).agg({\n            'velocidade_km': 'mean',\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum'\n        })\n        \n        return {\n            'padroes_semanais': weekly_patterns,\n            'padroes_por_hora': hourly_patterns,\n            'padroes_mensais': monthly_patterns\n        }\n    \n    def get_efficiency_metrics(self):\n        \"\"\"M√©tricas de efici√™ncia\"\"\"\n        df = self.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        # Efici√™ncia por ve√≠culo\n        vehicle_efficiency = df.groupby('placa').apply(\n            lambda x: {\n                'km_por_dia': x['odometro_periodo_km'].sum() / max(1, (x['data'].max() - x['data'].min()).days + 1),\n                'utilizacao_diaria': len(x) / max(1, (x['data'].max() - x['data'].min()).days + 1),\n                'velocidade_media': x['velocidade_km'].mean(),\n                'tempo_parado_pct': (len(x[x['velocidade_km'] == 0]) / len(x)) * 100\n            }\n        )\n        \n        return {\n            'eficiencia_por_veiculo': vehicle_efficiency,\n            'top_veiculos_km': df.groupby('placa')['odometro_periodo_km'].sum().sort_values(ascending=False).head(10),\n            'veiculos_mais_utilizados': df.groupby('placa').size().sort_values(ascending=False).head(10)\n        }\n","size_bytes":15500},"utils/insights_generator.py":{"content":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport streamlit as st\n\nclass InsightsGenerator:\n    \"\"\"Gerador de insights autom√°ticos para dados de frota\"\"\"\n    \n    def __init__(self, analyzer):\n        \"\"\"Inicializa com um analisador de dados\"\"\"\n        self.analyzer = analyzer\n        self.insights = []\n    \n    def generate_all_insights(self):\n        \"\"\"Gera todos os tipos de insights\"\"\"\n        self.insights = []\n        \n        # Gerar diferentes tipos de insights\n        self.generate_performance_insights()\n        self.generate_compliance_insights()\n        self.generate_efficiency_insights()\n        self.generate_operational_insights()\n        self.generate_predictive_insights()\n        \n        return self.insights\n    \n    def generate_performance_insights(self):\n        \"\"\"Gera insights de performance\"\"\"\n        kpis = self.analyzer.get_kpis()\n        speed_analysis = self.analyzer.get_speed_analysis()\n        \n        if not kpis:\n            return\n        \n        # Insight sobre velocidade m√©dia\n        if kpis['velocidade_media'] > 60:\n            self.add_insight(\n                \"‚ö†Ô∏è Velocidade Elevada\",\n                f\"A velocidade m√©dia da frota √© de {kpis['velocidade_media']:.1f} km/h, acima do recomendado.\",\n                \"Considere implementar pol√≠ticas de condu√ß√£o mais rigorosas.\",\n                \"warning\"\n            )\n        elif kpis['velocidade_media'] < 25:\n            self.add_insight(\n                \"üêå Velocidade Baixa\",\n                f\"A velocidade m√©dia da frota √© de {kpis['velocidade_media']:.1f} km/h, possivelmente indicando tr√¢nsito urbano intenso.\",\n                \"Avalie otimiza√ß√£o de rotas em hor√°rios de menor movimento.\",\n                \"info\"\n            )\n        \n        # Insight sobre cobertura GPS\n        if kpis['cobertura_gps'] < 90:\n            self.add_insight(\n                \"üì° Problemas de GPS\",\n                f\"Cobertura GPS de apenas {kpis['cobertura_gps']:.1f}%, abaixo do ideal (>95%).\",\n                \"Verifique equipamentos GPS e √°reas de cobertura de sinal.\",\n                \"error\"\n            )\n        else:\n            self.add_insight(\n                \"‚úÖ Excelente Cobertura GPS\",\n                f\"Cobertura GPS de {kpis['cobertura_gps']:.1f}%, dentro do padr√£o de qualidade.\",\n                \"Sistema de rastreamento funcionando adequadamente.\",\n                \"success\"\n            )\n    \n    def generate_compliance_insights(self):\n        \"\"\"Gera insights de compliance\"\"\"\n        compliance = self.analyzer.get_compliance_analysis()\n        \n        if not compliance:\n            return\n        \n        # Insights sobre viola√ß√µes de velocidade\n        if compliance['violacoes_velocidade'] > 0:\n            self.add_insight(\n                \"üö® Viola√ß√µes de Velocidade\",\n                f\"{compliance['violacoes_velocidade']} registros de excesso de velocidade detectados.\",\n                \"Implemente treinamento de condutores e monitoramento rigoroso.\",\n                \"error\"\n            )\n        \n        # Insights sobre ve√≠culos bloqueados\n        if compliance['veiculos_bloqueados'] > 0:\n            self.add_insight(\n                \"üîí Ve√≠culos Bloqueados\",\n                f\"{compliance['veiculos_bloqueados']} ve√≠culos com status de bloqueio ativo.\",\n                \"Verifique motivos do bloqueio e normalize situa√ß√£o se necess√°rio.\",\n                \"warning\"\n            )\n        \n        # Score de compliance geral\n        if compliance['score_compliance']:\n            avg_score = np.mean(list(compliance['score_compliance'].values()))\n            if avg_score >= 90:\n                self.add_insight(\n                    \"üèÜ Excelente Compliance\",\n                    f\"Score m√©dio de compliance de {avg_score:.1f}%, indicando √≥tima conformidade.\",\n                    \"Mantenha os padr√µes de opera√ß√£o atuais.\",\n                    \"success\"\n                )\n            elif avg_score < 70:\n                self.add_insight(\n                    \"‚ö†Ô∏è Compliance Baixo\",\n                    f\"Score m√©dio de compliance de {avg_score:.1f}%, requer aten√ß√£o.\",\n                    \"Implemente a√ß√µes corretivas urgentes para melhorar conformidade.\",\n                    \"error\"\n                )\n    \n    def generate_efficiency_insights(self):\n        \"\"\"Gera insights de efici√™ncia\"\"\"\n        kpis = self.analyzer.get_kpis()\n        efficiency = self.analyzer.get_efficiency_metrics()\n        \n        if not kpis or not efficiency:\n            return\n        \n        # Insight sobre utiliza√ß√£o da frota\n        if kpis['periodo_dias'] > 0:\n            utilizacao_media = kpis['total_registros'] / (kpis['total_veiculos'] * kpis['periodo_dias'])\n            \n            if utilizacao_media < 10:\n                self.add_insight(\n                    \"üìâ Baixa Utiliza√ß√£o\",\n                    f\"Utiliza√ß√£o m√©dia de {utilizacao_media:.1f} registros por ve√≠culo/dia.\",\n                    \"Considere otimizar a distribui√ß√£o e uso dos ve√≠culos.\",\n                    \"warning\"\n                )\n            elif utilizacao_media > 50:\n                self.add_insight(\n                    \"üìà Alta Utiliza√ß√£o\",\n                    f\"Utiliza√ß√£o m√©dia de {utilizacao_media:.1f} registros por ve√≠culo/dia.\",\n                    \"Frota sendo bem utilizada, monitore sobrecarga.\",\n                    \"success\"\n                )\n        \n        # Insight sobre dist√¢ncia percorrida\n        if kpis['distancia_total'] > 0 and kpis['tempo_ativo_horas'] > 0:\n            km_por_hora = kpis['distancia_total'] / kpis['tempo_ativo_horas']\n            if km_por_hora < 15:\n                self.add_insight(\n                    \"üê¢ Baixa Produtividade\",\n                    f\"M√©dia de {km_por_hora:.1f} km/hora de opera√ß√£o, indicando poss√≠vel inefici√™ncia.\",\n                    \"Analise rotas e otimize deslocamentos.\",\n                    \"warning\"\n                )\n    \n    def generate_operational_insights(self):\n        \"\"\"Gera insights operacionais\"\"\"\n        operational = self.analyzer.get_operational_analysis()\n        patterns = self.analyzer.get_temporal_patterns()\n        \n        if not operational or not patterns:\n            return\n        \n        # Insight sobre padr√µes de uso por hora\n        if 'padroes_por_hora' in patterns and patterns['padroes_por_hora'] is not None:\n            hourly_data = patterns['padroes_por_hora']\n            \n            # Se hourly_data √© um DataFrame com dados v√°lidos\n            if isinstance(hourly_data, pd.DataFrame) and not hourly_data.empty:\n                # Verificar se tem √≠ndice ou coluna de hora\n                if hourly_data.index.name == 'hora' or 'hora' in hourly_data.columns:\n                    # Usar a primeira coluna num√©rica dispon√≠vel como proxy para atividade\n                    numeric_cols = hourly_data.select_dtypes(include=[np.number]).columns\n                    if len(numeric_cols) > 0:\n                        activity_col = numeric_cols[0]\n                        peak_hour = hourly_data[activity_col].idxmax()\n                        peak_value = hourly_data.loc[peak_hour, activity_col]\n                        \n                        self.add_insight(\n                            \"‚è∞ Pico de Utiliza√ß√£o\",\n                            f\"Maior atividade registrada √†s {peak_hour}h.\",\n                            \"Considere balanceamento de carga em outros hor√°rios.\",\n                            \"info\"\n                        )\n                        return\n        \n        # Insight sobre ve√≠culos inativos\n        if 'estatisticas_por_veiculo' in operational:\n            vehicle_stats = operational['estatisticas_por_veiculo']\n            if len(vehicle_stats) > 0:\n                inactive_threshold = vehicle_stats['velocidade_km_count'].quantile(0.25)\n                inactive_vehicles = len(vehicle_stats[vehicle_stats['velocidade_km_count'] < inactive_threshold])\n                \n                if inactive_vehicles > 0:\n                    self.add_insight(\n                        \"üò¥ Ve√≠culos Pouco Ativos\",\n                        f\"{inactive_vehicles} ve√≠culos com baixa atividade registrada.\",\n                        \"Verifique se estes ve√≠culos est√£o sendo subutilizados.\",\n                        \"info\"\n                    )\n    \n    def generate_predictive_insights(self):\n        \"\"\"Gera insights preditivos\"\"\"\n        df = self.analyzer.filtered_df\n        \n        if df.empty:\n            return\n        \n        # An√°lise de tend√™ncias - usar per√≠odo din√¢mico baseado nos dados filtrados\n        if len(df) > 7:  # Pelo menos uma semana de dados\n            # Calcular per√≠odo de an√°lise baseado no range total dos dados filtrados\n            date_range = (df['data'].max() - df['data'].min()).days\n            \n            # Usar 25% do per√≠odo total para an√°lise recente, m√≠nimo 1 dia, m√°ximo 30 dias\n            analysis_days = max(1, min(30, round(date_range * 0.25)))\n            \n            recent_data = df[df['data'] >= df['data'].max() - timedelta(days=analysis_days)]\n            older_data = df[df['data'] < df['data'].max() - timedelta(days=analysis_days)]\n            \n            if not recent_data.empty and not older_data.empty:\n                recent_avg_speed = recent_data['velocidade_km'].mean()\n                older_avg_speed = older_data['velocidade_km'].mean()\n                \n                # Evitar divis√£o por zero e valores inv√°lidos\n                if pd.notna(older_avg_speed) and pd.notna(recent_avg_speed) and older_avg_speed > 0:\n                    speed_change = ((recent_avg_speed - older_avg_speed) / older_avg_speed) * 100\n                    \n                    if abs(speed_change) > 10:\n                        trend = \"aumento\" if speed_change > 0 else \"redu√ß√£o\"\n                        period_text = f\"√∫ltimos {analysis_days} dia{'s' if analysis_days > 1 else ''}\"\n                        self.add_insight(\n                            f\"üìà Tend√™ncia de Velocidade\",\n                            f\"Detectado {trend} de {abs(speed_change):.1f}% na velocidade m√©dia nos {period_text}.\",\n                            f\"Monitore esta tend√™ncia para identificar padr√µes sazonais ou operacionais.\",\n                            \"info\"\n                        )\n        \n        # Predi√ß√£o de manuten√ß√£o baseada em uso\n        agg_map = {'odometro_periodo_km': 'sum'}\n        if 'engine_hours_period' in df.columns:\n            agg_map['engine_hours_period'] = 'sum'\n        \n        vehicle_usage = df.groupby('placa').agg(agg_map)\n        \n        for placa, data in vehicle_usage.iterrows():\n            total_km = data['odometro_periodo_km']\n            if total_km > 1000:  # Ve√≠culos com alta quilometragem\n                self.add_insight(\n                    \"üîß Manuten√ß√£o Preventiva\",\n                    f\"Ve√≠culo {placa} percorreu {total_km:.0f}km no per√≠odo analisado.\",\n                    \"Agende revis√£o preventiva para manter performance e seguran√ßa.\",\n                    \"info\"\n                )\n    \n    def add_insight(self, title, description, recommendation, type=\"info\"):\n        \"\"\"Adiciona um insight √† lista\"\"\"\n        insight = {\n            'title': title,\n            'description': description,\n            'recommendation': recommendation,\n            'type': type,\n            'timestamp': datetime.now(),\n            'priority': self.get_priority_by_type(type)\n        }\n        self.insights.append(insight)\n    \n    def get_priority_by_type(self, type):\n        \"\"\"Retorna prioridade baseada no tipo\"\"\"\n        priority_map = {\n            'error': 1,\n            'warning': 2,\n            'info': 3,\n            'success': 4\n        }\n        return priority_map.get(type, 3)\n    \n    def get_insights_by_priority(self):\n        \"\"\"Retorna insights ordenados por prioridade\"\"\"\n        return sorted(self.insights, key=lambda x: x['priority'])\n    \n    def get_insights_by_type(self, type):\n        \"\"\"Retorna insights de um tipo espec√≠fico\"\"\"\n        return [insight for insight in self.insights if insight['type'] == type]\n    \n    def export_insights_to_text(self):\n        \"\"\"Exporta insights para texto\"\"\"\n        output = f\"# Relat√≥rio de Insights - {datetime.now().strftime('%d/%m/%Y %H:%M')}\\n\\n\"\n        \n        for insight in self.get_insights_by_priority():\n            output += f\"## {insight['title']}\\n\"\n            output += f\"**Descri√ß√£o:** {insight['description']}\\n\"\n            output += f\"**Recomenda√ß√£o:** {insight['recommendation']}\\n\"\n            output += f\"**Tipo:** {insight['type'].upper()}\\n\\n\"\n        \n        return output\n","size_bytes":12764},"utils/ml_predictive.py":{"content":"\"\"\"\nAn√°lise de Manuten√ß√£o Preditiva usando Machine Learning\nDetecta anomalias e prev√™ necessidades de manuten√ß√£o\n\"\"\"\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom typing import Dict, List, Any, Tuple\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nimport warnings\nwarnings.filterwarnings('ignore')\n\nclass PredictiveMaintenanceAnalyzer:\n    \"\"\"An√°lise de manuten√ß√£o preditiva para frota\"\"\"\n    \n    def __init__(self):\n        self.scaler = StandardScaler()\n        self.anomaly_detector = IsolationForest(\n            contamination=0.1,\n            random_state=42,\n            n_estimators=100\n        )\n        self.clusterer = DBSCAN(eps=0.5, min_samples=5)\n        \n    def analyze_vehicle_health(self, df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"An√°lise completa de sa√∫de do ve√≠culo\"\"\"\n        if df.empty:\n            return {'status': 'error', 'message': 'Sem dados dispon√≠veis'}\n            \n        # Preparar features para ML\n        features = self._prepare_features(df)\n        if features.empty:\n            return {'status': 'error', 'message': 'Dados insuficientes para an√°lise'}\n            \n        # Detectar anomalias\n        anomalies = self._detect_anomalies(features)\n        \n        # An√°lise de padr√µes\n        patterns = self._analyze_patterns(df)\n        \n        # Previs√µes de manuten√ß√£o\n        maintenance_alerts = self._predict_maintenance_needs(df, anomalies)\n        \n        # Scores de sa√∫de\n        health_scores = self._calculate_health_scores(df, anomalies)\n        \n        return {\n            'status': 'success',\n            'health_scores': health_scores,\n            'anomalies': anomalies,\n            'patterns': patterns,\n            'maintenance_alerts': maintenance_alerts,\n            'recommendations': self._generate_recommendations(health_scores, maintenance_alerts)\n        }\n    \n    def _prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Preparar features para an√°lise ML\"\"\"\n        try:\n            features_data = []\n            \n            # Converter campos num√©ricos primeiro\n            numeric_columns = ['velocidade_km', 'battery_level', 'tensao', 'odometro_periodo_km', 'engine_hours_period']\n            for col in numeric_columns:\n                if col in df.columns:\n                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n            \n            # Agrupar por hora para an√°lise temporal\n            df_hourly = df.groupby(df['data'].dt.floor('H')).agg({\n                'velocidade_km': ['mean', 'max', 'std'],\n                'battery_level': 'mean',\n                'tensao': 'mean',\n                'odometro_periodo_km': 'sum',\n                'engine_hours_period': 'sum',\n                'ignicao': lambda x: (x == 'Ligada').sum() / len(x) if len(x) > 0 else 0\n            }).reset_index()\n            \n            # Flatten column names\n            df_hourly.columns = ['timestamp', 'vel_media', 'vel_max', 'vel_std', \n                                'bateria_media', 'tensao_media', 'km_periodo', \n                                'engine_hours_periodo', 'ignicao_ratio']\n            \n            # Adicionar features temporais\n            df_hourly['hora'] = df_hourly['timestamp'].dt.hour\n            df_hourly['dia_semana'] = df_hourly['timestamp'].dt.dayofweek\n            \n            # Preencher NaNs\n            numeric_cols = ['vel_media', 'vel_max', 'vel_std', 'bateria_media', \n                           'tensao_media', 'km_periodo', 'engine_hours_periodo', 'ignicao_ratio']\n            for col in numeric_cols:\n                if col in df_hourly.columns:\n                    df_hourly[col] = pd.to_numeric(df_hourly[col], errors='coerce').fillna(0)\n            \n            return df_hourly[numeric_cols + ['hora', 'dia_semana']].dropna()\n            \n        except Exception as e:\n            print(f\"Erro ao preparar features: {e}\")\n            return pd.DataFrame()\n    \n    def _detect_anomalies(self, features: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"Detectar anomalias usando Isolation Forest\"\"\"\n        try:\n            if len(features) < 10:\n                return {'count': 0, 'indices': [], 'scores': []}\n            \n            # Normalizar dados\n            features_scaled = self.scaler.fit_transform(features)\n            \n            # Detectar anomalias\n            outliers = self.anomaly_detector.fit_predict(features_scaled)\n            anomaly_scores = self.anomaly_detector.score_samples(features_scaled)\n            \n            anomaly_indices = np.where(outliers == -1)[0]\n            \n            return {\n                'count': len(anomaly_indices),\n                'indices': anomaly_indices.tolist(),\n                'scores': anomaly_scores.tolist(),\n                'severity': self._classify_anomaly_severity(anomaly_scores[anomaly_indices])\n            }\n            \n        except Exception as e:\n            print(f\"Erro na detec√ß√£o de anomalias: {e}\")\n            return {'count': 0, 'indices': [], 'scores': []}\n    \n    def _analyze_patterns(self, df: pd.DataFrame) -> Dict[str, Any]:\n        \"\"\"An√°lise de padr√µes de uso\"\"\"\n        try:\n            patterns = {}\n            \n            # Padr√µes de velocidade\n            vel_stats = df['velocidade_km'].describe()\n            patterns['velocidade'] = {\n                'media': round(vel_stats['mean'], 1),\n                'maxima': round(vel_stats['max'], 1),\n                'excessos': len(df[df['velocidade_km'] > 80]),  # Velocidade excessiva\n                'paradas': len(df[df['velocidade_km'] == 0])\n            }\n            \n            # Padr√µes temporais\n            df['hora'] = df['data'].dt.hour\n            uso_por_hora = df.groupby('hora').size()\n            patterns['uso_temporal'] = {\n                'horario_pico': uso_por_hora.idxmax(),\n                'horario_baixo': uso_por_hora.idxmin(),\n                'uso_noturno': len(df[(df['hora'] >= 22) | (df['hora'] <= 6)])\n            }\n            \n            # Padr√µes de manuten√ß√£o\n            bateria_baixa = df[pd.to_numeric(df['bateria'], errors='coerce') < 12].shape[0] if 'bateria' in df.columns else 0\n            patterns['manutencao'] = {\n                'bateria_baixa_count': bateria_baixa,\n                'tensao_problemas': len(df[pd.to_numeric(df['tensao'], errors='coerce') < 11]) if 'tensao' in df.columns else 0\n            }\n            \n            return patterns\n            \n        except Exception as e:\n            print(f\"Erro na an√°lise de padr√µes: {e}\")\n            return {}\n    \n    def _predict_maintenance_needs(self, df: pd.DataFrame, anomalies: Dict) -> List[Dict]:\n        \"\"\"Prever necessidades de manuten√ß√£o\"\"\"\n        alerts = []\n        \n        try:\n            # An√°lise de bateria\n            if 'bateria' in df.columns:\n                bateria_values = pd.to_numeric(df['bateria'], errors='coerce').dropna()\n                if not bateria_values.empty:\n                    bateria_media = bateria_values.mean()\n                    if bateria_media < 12:\n                        alerts.append({\n                            'tipo': 'Bateria',\n                            'severidade': 'Alta' if bateria_media < 11 else 'M√©dia',\n                            'descricao': f'Bateria baixa (m√©dia: {bateria_media:.1f}V)',\n                            'prazo': '3-7 dias' if bateria_media < 11 else '1-2 semanas'\n                        })\n            \n            # An√°lise de velocidade excessiva\n            vel_excessiva = len(df[df['velocidade_km'] > 80])\n            if vel_excessiva > len(df) * 0.1:  # Mais de 10% do tempo em velocidade alta\n                alerts.append({\n                    'tipo': 'Desgaste',\n                    'severidade': 'M√©dia',\n                    'descricao': f'Uso intensivo detectado ({vel_excessiva} ocorr√™ncias)',\n                    'prazo': '2-4 semanas'\n                })\n            \n            # Anomalias frequentes\n            if anomalies.get('count', 0) > len(df) * 0.05:  # Mais de 5% anomalias\n                alerts.append({\n                    'tipo': 'Comportamento An√¥malo',\n                    'severidade': 'M√©dia',\n                    'descricao': f'{anomalies[\"count\"]} anomalias detectadas',\n                    'prazo': '1-3 semanas'\n                })\n                \n        except Exception as e:\n            print(f\"Erro na previs√£o de manuten√ß√£o: {e}\")\n        \n        return alerts\n    \n    def _calculate_health_scores(self, df: pd.DataFrame, anomalies: Dict) -> Dict[str, float]:\n        \"\"\"Calcular scores de sa√∫de do ve√≠culo\"\"\"\n        scores = {}\n        \n        try:\n            # Score da bateria (0-100)\n            if 'bateria' in df.columns:\n                bateria_values = pd.to_numeric(df['bateria'], errors='coerce').dropna()\n                if not bateria_values.empty:\n                    bateria_media = bateria_values.mean()\n                    scores['bateria'] = max(0, min(100, (bateria_media - 10) * 10))  # 10V = 0%, 20V = 100%\n                else:\n                    scores['bateria'] = 50\n            else:\n                scores['bateria'] = 50\n            \n            # Score do comportamento (baseado em anomalias)\n            total_registros = len(df)\n            anomalia_ratio = anomalies.get('count', 0) / total_registros if total_registros > 0 else 0\n            scores['comportamento'] = max(0, 100 - (anomalia_ratio * 500))  # Penaliza anomalias\n            \n            # Score de velocidade (uso respons√°vel)\n            vel_excessiva_ratio = len(df[df['velocidade_km'] > 80]) / total_registros if total_registros > 0 else 0\n            scores['velocidade'] = max(0, 100 - (vel_excessiva_ratio * 200))\n            \n            # Score geral (m√©dia ponderada)\n            scores['geral'] = (\n                scores['bateria'] * 0.4 + \n                scores['comportamento'] * 0.35 + \n                scores['velocidade'] * 0.25\n            )\n            \n        except Exception as e:\n            print(f\"Erro no c√°lculo de scores: {e}\")\n            scores = {'bateria': 50, 'comportamento': 50, 'velocidade': 50, 'geral': 50}\n        \n        return {k: round(v, 1) for k, v in scores.items()}\n    \n    def _classify_anomaly_severity(self, anomaly_scores: np.ndarray) -> str:\n        \"\"\"Classificar severidade das anomalias\"\"\"\n        if len(anomaly_scores) == 0:\n            return 'Baixa'\n        \n        mean_score = np.mean(anomaly_scores)\n        if mean_score < -0.3:\n            return 'Alta'\n        elif mean_score < -0.1:\n            return 'M√©dia'\n        else:\n            return 'Baixa'\n    \n    def _generate_recommendations(self, health_scores: Dict, alerts: List[Dict]) -> List[str]:\n        \"\"\"Gerar recomenda√ß√µes baseadas na an√°lise\"\"\"\n        recommendations = []\n        \n        try:\n            # Recomenda√ß√µes baseadas nos scores\n            if health_scores.get('bateria', 50) < 70:\n                recommendations.append(\"üîã Verificar sistema el√©trico e bateria\")\n            \n            if health_scores.get('comportamento', 50) < 70:\n                recommendations.append(\"‚ö†Ô∏è Revisar padr√µes de condu√ß√£o e treinamento\")\n            \n            if health_scores.get('velocidade', 50) < 70:\n                recommendations.append(\"üöó Implementar controle de velocidade\")\n            \n            # Recomenda√ß√µes baseadas nos alertas\n            if any(alert['severidade'] == 'Alta' for alert in alerts):\n                recommendations.append(\"üö® Manuten√ß√£o urgente necess√°ria\")\n            \n            if len(alerts) > 2:\n                recommendations.append(\"üìã Revisar plano de manuten√ß√£o preventiva\")\n            \n            if not recommendations:\n                recommendations.append(\"‚úÖ Ve√≠culo em bom estado - manter rotina\")\n                \n        except Exception as e:\n            print(f\"Erro ao gerar recomenda√ß√µes: {e}\")\n            recommendations = [\"üìã Manter rotina de manuten√ß√£o\"]\n        \n        return recommendations","size_bytes":12153},"utils/monthly_data_manager.py":{"content":"\"\"\"\nGerenciador de dados mensais - preparado para uploads recorrentes\n\"\"\"\n\nimport os\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\nfrom database.db_manager import DatabaseManager\n\nclass MonthlyDataManager:\n    \"\"\"Gerencia uploads de dados mensais com a mesma estrutura\"\"\"\n    \n    @staticmethod\n    def process_monthly_upload(csv_files: List[str]) -> Dict[str, Any]:\n        \"\"\"Processa upload mensal de m√∫ltiplos arquivos CSV\"\"\"\n        results = {\n            'success': True,\n            'processed_files': 0,\n            'total_records': 0,\n            'new_vehicles': 0,\n            'errors': []\n        }\n        \n        for csv_file in csv_files:\n            try:\n                if not os.path.exists(csv_file):\n                    results['errors'].append(f\"Arquivo n√£o encontrado: {csv_file}\")\n                    continue\n                \n                # Processar arquivo\n                result = DatabaseManager.migrate_csv_to_database(csv_file)\n                \n                if result['success']:\n                    results['processed_files'] += 1\n                    results['total_records'] += result.get('records_processed', 0)\n                    results['new_vehicles'] += result.get('unique_vehicles', 0)\n                else:\n                    results['errors'].append(f\"{os.path.basename(csv_file)}: {result.get('error', 'Erro desconhecido')}\")\n                    \n            except Exception as e:\n                results['errors'].append(f\"{os.path.basename(csv_file)}: {str(e)}\")\n        \n        # Se houve erros mas alguns sucessos, ainda √© parcialmente bem-sucedido\n        if results['errors'] and results['processed_files'] == 0:\n            results['success'] = False\n        \n        return results\n    \n    @staticmethod\n    def get_monthly_summary() -> Dict[str, Any]:\n        \"\"\"Resumo dos dados mensais na base\"\"\"\n        summary = DatabaseManager.get_fleet_summary()\n        \n        # Adicionar informa√ß√µes espec√≠ficas mensais\n        df = DatabaseManager.get_dashboard_data()\n        \n        if not df.empty:\n            # An√°lise temporal\n            df['mes'] = df['data'].dt.to_period('M')\n            monthly_data = df.groupby('mes').agg({\n                'placa': 'nunique',\n                'velocidade_km': 'mean',\n                'data': 'count'\n            }).reset_index()\n            \n            summary['monthly_breakdown'] = monthly_data.to_dict('records')\n            summary['data_period'] = {\n                'start': df['data'].min().strftime('%d/%m/%Y'),\n                'end': df['data'].max().strftime('%d/%m/%Y'),\n                'days': (df['data'].max() - df['data'].min()).days\n            }\n        \n        return summary\n    \n    @staticmethod\n    def prepare_for_next_month() -> Dict[str, Any]:\n        \"\"\"Prepara sistema para pr√≥ximo upload mensal\"\"\"\n        current_summary = MonthlyDataManager.get_monthly_summary()\n        \n        return {\n            'current_data': current_summary,\n            'ready_for_upload': True,\n            'upload_instructions': {\n                'format': 'CSV com separador ; (ponto e v√≠rgula)',\n                'encoding': 'Latin-1 (ISO 8859-1)',\n                'required_columns': [\n                    'Cliente', 'Placa', 'Ativo', 'Data', 'Data (GPRS)',\n                    'Velocidade (Km)', 'Igni√ß√£o', 'Motorista', 'GPS', 'Gprs',\n                    'Localiza√ß√£o', 'Endere√ßo', 'Tipo do Evento', 'Cerca',\n                    'Saida', 'Entrada', 'Pacote', 'Od√¥metro do per√≠odo  (Km)',\n                    'Hor√≠metro do per√≠odo', 'Hor√≠metro embarcado',\n                    'Od√¥metro embarcado (Km)', 'Bateria', 'Imagem', 'Tens√£o', 'Bloqueado'\n                ],\n                'date_format': 'DD/MM/AAAA HH:MM:SS (formato brasileiro)',\n                'notes': 'O sistema detecta automaticamente o formato e processa m√∫ltiplos arquivos'\n            }\n        }","size_bytes":3940},"utils/pdf_reports.py":{"content":"\"\"\"Gerador de Relat√≥rios PDF Avan√ßado\"\"\"\nfrom fpdf import FPDF\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nimport os\nimport numpy as np\n\nclass PDFReportGenerator:\n    def __init__(self):\n        self.pdf = FPDF()\n        self.pdf.set_auto_page_break(auto=True, margin=15)\n        self.pdf.add_page()\n        self.pdf.set_font('Arial', 'B', 16)\n    \n    def add_header(self, title):\n        \"\"\"Adiciona cabe√ßalho da se√ß√£o\"\"\"\n        self.pdf.set_fill_color(230, 230, 230)\n        self.pdf.set_font('Arial', 'B', 14)\n        self.pdf.cell(0, 10, title, 0, 1, 'L', True)\n        self.pdf.ln(5)\n    \n    def add_subsection(self, title):\n        \"\"\"Adiciona subt√≠tulo\"\"\"\n        self.pdf.set_font('Arial', 'B', 12)\n        self.pdf.cell(0, 8, title, 0, 1, 'L')\n        self.pdf.ln(3)\n    \n    def add_metric(self, label, value, unit=\"\"):\n        \"\"\"Adiciona m√©trica formatada\"\"\"\n        self.pdf.set_font('Arial', '', 10)\n        self.pdf.cell(100, 6, f'{label}:', 0, 0, 'L')\n        self.pdf.set_font('Arial', 'B', 10)\n        self.pdf.cell(0, 6, f'{value} {unit}', 0, 1, 'L')\n    \n    def add_table_header(self, headers):\n        \"\"\"Adiciona cabe√ßalho de tabela\"\"\"\n        self.pdf.set_font('Arial', 'B', 9)\n        self.pdf.set_fill_color(200, 200, 200)\n        col_width = 190 / len(headers)\n        for header in headers:\n            self.pdf.cell(col_width, 8, str(header), 1, 0, 'C', True)\n        self.pdf.ln()\n    \n    def add_table_row(self, data):\n        \"\"\"Adiciona linha de tabela\"\"\"\n        self.pdf.set_font('Arial', '', 8)\n        col_width = 190 / len(data)\n        for item in data:\n            self.pdf.cell(col_width, 6, str(item), 1, 0, 'C')\n        self.pdf.ln()\n    \n    def generate_comprehensive_report(self, output_path: str = \"relatorio_completo_frota.pdf\") -> str:\n        \"\"\"Gera relat√≥rio completo e detalhado da frota\"\"\"\n        try:\n            # Carregar dados\n            df = DatabaseManager.get_dashboard_data()\n            summary = DatabaseManager.get_fleet_summary()\n            \n            if df.empty:\n                self.pdf.cell(0, 10, 'ERRO: Nenhum dado dispon√≠vel para gerar relat√≥rio', 0, 1, 'C')\n                self.pdf.output(output_path)\n                return output_path\n            \n            # Cabe√ßalho principal\n            self.pdf.set_font('Arial', 'B', 18)\n            self.pdf.cell(0, 15, 'RELAT√ìRIO EXECUTIVO DE FROTA', 0, 1, 'C')\n            self.pdf.set_font('Arial', 'I', 12)\n            self.pdf.cell(0, 8, 'Insight Hub - Sistema de Monitoramento Municipal', 0, 1, 'C')\n            self.pdf.ln(10)\n            \n            # Informa√ß√µes do relat√≥rio\n            self.pdf.set_font('Arial', '', 10)\n            self.pdf.cell(0, 6, f'Data de Gera√ß√£o: {datetime.now().strftime(\"%d/%m/%Y √†s %H:%M:%S\")}', 0, 1)\n            self.pdf.cell(0, 6, f'Per√≠odo Analisado: {df[\"data\"].min().strftime(\"%d/%m/%Y\")} a {df[\"data\"].max().strftime(\"%d/%m/%Y\")}', 0, 1)\n            self.pdf.ln(10)\n            \n            # 1. RESUMO EXECUTIVO\n            self.add_header('1. RESUMO EXECUTIVO')\n            \n            total_records = len(df)\n            total_vehicles = df['placa'].nunique()\n            total_clients = df['cliente'].nunique()\n            avg_speed = df['velocidade_km'].mean()\n            max_speed = df['velocidade_km'].max()\n            total_distance = df['odometro_periodo_km'].sum()\n            \n            # M√©tricas principais\n            self.add_metric('Total de Registros Processados', f'{total_records:,}')\n            self.add_metric('Ve√≠culos Monitorados', f'{total_vehicles}')\n            self.add_metric('Clientes Atendidos', f'{total_clients}')\n            self.add_metric('Velocidade M√©dia da Frota', f'{avg_speed:.1f}', 'km/h')\n            self.add_metric('Velocidade M√°xima Registrada', f'{max_speed:.1f}', 'km/h')\n            self.add_metric('Dist√¢ncia Total Percorrida', f'{total_distance:.1f}', 'km')\n            \n            # Cobertura GPS\n            gps_quality = df['gps'].value_counts()\n            gps_coverage = (gps_quality.get('Ativo', 0) / total_records * 100) if total_records > 0 else 0\n            self.add_metric('Cobertura GPS Ativa', f'{gps_coverage:.1f}', '%')\n            self.pdf.ln(5)\n            \n            # 2. AN√ÅLISE POR CLIENTE\n            self.add_header('2. AN√ÅLISE POR CLIENTE')\n            \n            client_stats = df.groupby('cliente').agg({\n                'placa': 'nunique',\n                'velocidade_km': ['mean', 'max'],\n                'odometro_periodo_km': 'sum'\n            }).round(2)\n            \n            self.add_table_header(['Cliente', 'Ve√≠culos', 'Vel. M√©dia', 'Vel. M√°x', 'Dist. Total'])\n            for cliente in client_stats.index:\n                data = [\n                    cliente,\n                    client_stats.loc[cliente, ('placa', 'nunique')],\n                    f\"{client_stats.loc[cliente, ('velocidade_km', 'mean')]:.1f}\",\n                    f\"{client_stats.loc[cliente, ('velocidade_km', 'max')]:.1f}\",\n                    f\"{client_stats.loc[cliente, ('odometro_periodo_km', 'sum')]:.1f}\"\n                ]\n                self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 3. AN√ÅLISE DETALHADA POR VE√çCULO\n            self.add_header('3. AN√ÅLISE DETALHADA POR VE√çCULO')\n            \n            vehicle_stats = df.groupby(['cliente', 'placa']).agg({\n                'velocidade_km': ['mean', 'max', 'std'],\n                'odometro_periodo_km': 'sum',\n                'data': 'count'\n            }).round(2)\n            \n            self.add_table_header(['Cliente', 'Placa', 'Registros', 'Vel. M√©dia', 'Vel. M√°x', 'Dist√¢ncia'])\n            for (cliente, placa) in vehicle_stats.index:\n                data = [\n                    cliente[:15],  # Truncar se muito longo\n                    placa,\n                    vehicle_stats.loc[(cliente, placa), ('data', 'count')],\n                    f\"{vehicle_stats.loc[(cliente, placa), ('velocidade_km', 'mean')]:.1f}\",\n                    f\"{vehicle_stats.loc[(cliente, placa), ('velocidade_km', 'max')]:.1f}\",\n                    f\"{vehicle_stats.loc[(cliente, placa), ('odometro_periodo_km', 'sum')]:.1f}\"\n                ]\n                self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 4. AN√ÅLISE DE VELOCIDADE E SEGURAN√áA\n            self.add_header('4. AN√ÅLISE DE VELOCIDADE E SEGURAN√áA')\n            \n            # Faixas de velocidade\n            speed_ranges = {\n                '0-30 km/h': len(df[df['velocidade_km'] <= 30]),\n                '31-60 km/h': len(df[(df['velocidade_km'] > 30) & (df['velocidade_km'] <= 60)]),\n                '61-80 km/h': len(df[(df['velocidade_km'] > 60) & (df['velocidade_km'] <= 80)]),\n                '81-100 km/h': len(df[(df['velocidade_km'] > 80) & (df['velocidade_km'] <= 100)]),\n                'Acima de 100 km/h': len(df[df['velocidade_km'] > 100])\n            }\n            \n            self.add_subsection('Distribui√ß√£o de Velocidades:')\n            for faixa, count in speed_ranges.items():\n                percentage = (count / total_records * 100) if total_records > 0 else 0\n                self.add_metric(faixa, f'{count:,} ({percentage:.1f}%)')\n            \n            # Viola√ß√µes de velocidade (acima de 80 km/h)\n            violations = df[df['velocidade_km'] > 80]\n            self.add_subsection('Alertas de Velocidade (acima de 80 km/h):')\n            self.add_metric('Total de Viola√ß√µes', f'{len(violations):,}')\n            self.add_metric('Percentual da Frota', f'{(len(violations)/total_records*100):.1f}%')\n            \n            if not violations.empty:\n                worst_violations = violations.nlargest(5, 'velocidade_km')\n                self.add_subsection('Top 5 Maiores Velocidades:')\n                self.add_table_header(['Placa', 'Cliente', 'Velocidade', 'Data/Hora'])\n                for _, row in worst_violations.iterrows():\n                    data = [\n                        row['placa'],\n                        row['cliente'][:15],\n                        f\"{row['velocidade_km']:.1f} km/h\",\n                        row['data'].strftime('%d/%m %H:%M')\n                    ]\n                    self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 5. AN√ÅLISE OPERACIONAL\n            self.add_header('5. AN√ÅLISE OPERACIONAL')\n            \n            # Status operacional\n            ignition_stats = df['ignicao'].value_counts() if 'ignicao' in df.columns else {}\n            blocked_stats = df['bloqueado'].value_counts() if 'bloqueado' in df.columns else {}\n            \n            self.add_subsection('Status dos Ve√≠culos:')\n            if 'ligado' in ignition_stats.index or 'desligado' in ignition_stats.index:\n                self.add_metric('Ve√≠culos com Igni√ß√£o Ligada', f'{ignition_stats.get(\"ligado\", 0):,}')\n                self.add_metric('Ve√≠culos com Igni√ß√£o Desligada', f'{ignition_stats.get(\"desligado\", 0):,}')\n            \n            if 'sim' in blocked_stats.index or 'n√£o' in blocked_stats.index:\n                self.add_metric('Ve√≠culos Bloqueados', f'{blocked_stats.get(\"sim\", 0):,}')\n                self.add_metric('Ve√≠culos Desbloqueados', f'{blocked_stats.get(\"n√£o\", 0):,}')\n            \n            # An√°lise temporal\n            df['hora'] = df['data'].dt.hour\n            peak_hour = df.groupby('hora').size().idxmax()\n            peak_count = df.groupby('hora').size().max()\n            \n            self.add_subsection('Padr√µes de Uso:')\n            self.add_metric('Hor√°rio de Pico', f'{peak_hour}:00 - {peak_hour+1}:00')\n            self.add_metric('Atividade no Pico', f'{peak_count:,} registros')\n            self.pdf.ln(5)\n            \n            # 6. AN√ÅLISE DE EFICI√äNCIA\n            self.add_header('6. AN√ÅLISE DE EFICI√äNCIA ENERG√âTICA')\n            \n            if 'bateria' in df.columns:\n                battery_avg = df['bateria'].mean()\n                battery_low = len(df[df['bateria'] < 12.0])\n                self.add_metric('N√≠vel M√©dio de Bateria', f'{battery_avg:.1f}V')\n                self.add_metric('Alertas de Bateria Baixa', f'{battery_low:,}')\n            \n            # Efici√™ncia por ve√≠culo\n            efficiency_stats = df.groupby('placa').agg({\n                'velocidade_km': 'mean',\n                'odometro_periodo_km': 'sum'\n            }).round(2)\n            \n            # Top 5 mais eficientes (maior dist√¢ncia, menor velocidade m√©dia)\n            efficiency_stats['score'] = efficiency_stats['odometro_periodo_km'] / (efficiency_stats['velocidade_km'] + 1)\n            top_efficient = efficiency_stats.nlargest(5, 'score')\n            \n            self.add_subsection('Top 5 Ve√≠culos Mais Eficientes:')\n            self.add_table_header(['Placa', 'Dist√¢ncia Total', 'Vel. M√©dia', 'Score'])\n            for placa in top_efficient.index:\n                data = [\n                    placa,\n                    f\"{top_efficient.loc[placa, 'odometro_periodo_km']:.1f} km\",\n                    f\"{top_efficient.loc[placa, 'velocidade_km']:.1f} km/h\",\n                    f\"{top_efficient.loc[placa, 'score']:.2f}\"\n                ]\n                self.add_table_row(data)\n            self.pdf.ln(5)\n            \n            # 7. RECOMENDA√á√ïES\n            self.add_header('7. RECOMENDA√á√ïES E ALERTAS')\n            \n            self.pdf.set_font('Arial', '', 10)\n            recommendations = []\n            \n            if len(violations) > total_records * 0.1:\n                recommendations.append(\"‚Ä¢ Implementar treinamento de condu√ß√£o defensiva para reduzir viola√ß√µes de velocidade\")\n            \n            if gps_coverage < 95:\n                recommendations.append(\"‚Ä¢ Verificar sistema GPS dos ve√≠culos com baixa cobertura\")\n            \n            low_activity_vehicles = vehicle_stats[vehicle_stats[('data', 'count')] < 50].index\n            if len(low_activity_vehicles) > 0:\n                recommendations.append(f\"‚Ä¢ Investigar {len(low_activity_vehicles)} ve√≠culos com baixa atividade\")\n            \n            if not recommendations:\n                recommendations.append(\"‚Ä¢ Frota operando dentro dos par√¢metros normais\")\n                recommendations.append(\"‚Ä¢ Continuar monitoramento regular\")\n            \n            for rec in recommendations:\n                self.pdf.cell(0, 6, rec.encode('latin-1', 'replace').decode('latin-1'), 0, 1)\n            \n            self.pdf.ln(10)\n            \n            # Rodap√©\n            self.pdf.set_font('Arial', 'I', 8)\n            self.pdf.cell(0, 5, f'Relat√≥rio gerado automaticamente pelo Insight Hub em {datetime.now().strftime(\"%d/%m/%Y √†s %H:%M:%S\")}', 0, 1, 'C')\n            \n            # Salvar PDF\n            self.pdf.output(output_path)\n            return output_path\n            \n        except Exception as e:\n            # Em caso de erro, criar relat√≥rio b√°sico\n            self.pdf.cell(0, 10, f'Erro ao gerar relat√≥rio detalhado: {str(e)}', 0, 1)\n            self.pdf.output(output_path)\n            return output_path\n    \n    def generate_fleet_report(self, output_path: str = \"relatorio_frota.pdf\") -> str:\n        \"\"\"Mant√©m compatibilidade - chama relat√≥rio completo\"\"\"\n        return self.generate_comprehensive_report(output_path)","size_bytes":13365},"utils/visualizations.py":{"content":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport pandas as pd\nimport numpy as np\n\nclass FleetVisualizations:\n    \"\"\"Classe para criar visualiza√ß√µes de dados de frota\"\"\"\n    \n    def __init__(self, analyzer):\n        \"\"\"Inicializa com um analisador de dados\"\"\"\n        self.analyzer = analyzer\n        self.colors = {\n            'primary': '#1f77b4',\n            'secondary': '#ff7f0e',\n            'success': '#2ca02c',\n            'warning': '#d62728',\n            'info': '#9467bd'\n        }\n    \n    def create_kpi_charts(self):\n        \"\"\"Cria gr√°ficos de KPIs\"\"\"\n        kpis = self.analyzer.get_kpis()\n        \n        if not kpis:\n            return {}\n        \n        charts = {}\n        \n        # Gr√°fico de velocidade m√©dia por ve√≠culo\n        speed_by_vehicle = self.analyzer.filtered_df.groupby('placa')['velocidade_km'].mean().sort_values(ascending=False)\n        \n        speed_chart = px.bar(\n            x=speed_by_vehicle.values[:15],  # Top 15\n            y=speed_by_vehicle.index[:15],\n            orientation='h',\n            title='Velocidade M√©dia por Ve√≠culo (Top 15)',\n            labels={'x': 'Velocidade M√©dia (km/h)', 'y': 'Placa'}\n        )\n        speed_chart.update_traces(hovertemplate='<b>Ve√≠culo:</b> %{y}<br><b>Velocidade M√©dia:</b> %{x:.1f} km/h<extra></extra>')\n        charts['speed_by_vehicle'] = speed_chart\n        \n        # Gr√°fico de distribui√ß√£o de velocidade\n        dist_chart = px.histogram(\n            self.analyzer.filtered_df,\n            x='velocidade_km',\n            nbins=30,\n            title='Distribui√ß√£o de Velocidade',\n            labels={'x': 'Velocidade (km/h)', 'y': 'Frequ√™ncia'}\n        )\n        dist_chart.update_traces(hovertemplate='<b>Velocidade:</b> %{x:.1f} km/h<br><b>Frequ√™ncia:</b> %{y}<extra></extra>')\n        charts['speed_distribution'] = dist_chart\n        \n        return charts\n    \n    def create_temporal_charts(self):\n        \"\"\"Cria gr√°ficos temporais\"\"\"\n        df = self.analyzer.filtered_df\n        \n        if df.empty:\n            return {}\n        \n        charts = {}\n        \n        # Atividade por hora do dia\n        hourly_activity = df.groupby(df['data'].dt.hour).agg({\n            'placa': 'nunique',\n            'velocidade_km': 'mean'\n        }).reset_index()\n        \n        activity_chart = px.line(\n            hourly_activity,\n            x='data',\n            y='placa',\n            title='Ve√≠culos Ativos por Hora do Dia',\n            labels={'data': 'Hora', 'placa': 'N√∫mero de Ve√≠culos Ativos'}\n        )\n        activity_chart.update_traces(hovertemplate='<b>Hora:</b> %{x}h<br><b>Ve√≠culos Ativos:</b> %{y}<extra></extra>')\n        charts['hourly_activity'] = activity_chart\n        \n        # Velocidade m√©dia por hora\n        speed_hourly_chart = px.line(\n            hourly_activity,\n            x='data',\n            y='velocidade_km',\n            title='Velocidade M√©dia por Hora do Dia',\n            labels={'data': 'Hora', 'velocidade_km': 'Velocidade M√©dia (km/h)'}\n        )\n        speed_hourly_chart.update_traces(hovertemplate='<b>Hora:</b> %{x}h<br><b>Velocidade M√©dia:</b> %{y:.1f} km/h<extra></extra>')\n        charts['hourly_speed'] = speed_hourly_chart\n        \n        # Atividade di√°ria\n        daily_activity = df.groupby(df['data'].dt.date).agg({\n            'placa': 'nunique',\n            'odometro_periodo_km': 'sum',\n            'velocidade_km': 'mean'\n        }).reset_index()\n        \n        charts['daily_activity'] = px.line(\n            daily_activity,\n            x='data',\n            y='placa',\n            title='Ve√≠culos Ativos por Dia',\n            labels={'data': 'Data', 'placa': 'N√∫mero de Ve√≠culos Ativos'}\n        )\n        \n        return charts\n    \n    def create_compliance_charts(self):\n        \"\"\"Cria gr√°ficos de compliance\"\"\"\n        compliance = self.analyzer.get_compliance_analysis()\n        \n        if not compliance:\n            return {}\n        \n        charts = {}\n        \n        # Score de compliance por ve√≠culo\n        if compliance['score_compliance']:\n            scores_df = pd.DataFrame(list(compliance['score_compliance'].items()), \n                                   columns=['Placa', 'Score'])\n            scores_df = scores_df.sort_values('Score', ascending=False)\n            \n            # Definir cores baseadas no score\n            colors = ['#2ca02c' if score >= 90 else '#ff7f0e' if score >= 70 else '#d62728' \n                     for score in scores_df['Score']]\n            \n            charts['compliance_scores'] = go.Figure(data=[\n                go.Bar(\n                    x=scores_df['Placa'],\n                    y=scores_df['Score'],\n                    marker_color=colors,\n                    text=scores_df['Score'].round(1),\n                    textposition='outside'\n                )\n            ])\n            charts['compliance_scores'].update_layout(\n                title='Score de Compliance por Ve√≠culo',\n                xaxis_title='Placa',\n                yaxis_title='Score (%)',\n                yaxis=dict(range=[0, 100])\n            )\n        \n        # Viola√ß√µes de velocidade\n        if 'detalhes_violacoes' in compliance and not compliance['detalhes_violacoes'].empty:\n            violations_df = compliance['detalhes_violacoes'].head(10).reset_index()\n            \n            charts['speed_violations'] = px.bar(\n                violations_df,\n                x='placa',\n                y=0,  # A coluna de contagem\n                title='Top 10 Ve√≠culos com Viola√ß√µes de Velocidade',\n                labels={'placa': 'Placa', '0': 'N√∫mero de Viola√ß√µes'}\n            )\n        \n        return charts\n    \n    def create_comparison_chart(self, comparison_data):\n        \"\"\"Cria gr√°fico de compara√ß√£o entre ve√≠culos\"\"\"\n        if not comparison_data:\n            return None\n        \n        # Converter dados para DataFrame\n        df_comparison = pd.DataFrame(comparison_data).T.reset_index()\n        df_comparison.columns = ['Placa'] + list(df_comparison.columns[1:])\n        \n        # Criar gr√°fico de radar/spider\n        categories = ['Velocidade M√©dia', 'Dist√¢ncia Total', 'Tempo Ativo', 'Cobertura GPS']\n        \n        fig = go.Figure()\n        \n        for _, row in df_comparison.iterrows():\n            fig.add_trace(go.Scatterpolar(\n                r=[\n                    row['velocidade_media'],\n                    row['distancia_total'] / 100,  # Normalizar\n                    row['tempo_ativo'] / 10,  # Normalizar\n                    row['cobertura_gps']\n                ],\n                theta=categories,\n                fill='toself',\n                name=row['Placa']\n            ))\n        \n        fig.update_layout(\n            polar=dict(\n                radialaxis=dict(\n                    visible=True,\n                    range=[0, 100]\n                )),\n            showlegend=True,\n            title=\"Compara√ß√£o de Performance entre Ve√≠culos\"\n        )\n        \n        return fig\n    \n    def create_efficiency_charts(self):\n        \"\"\"Cria gr√°ficos de efici√™ncia\"\"\"\n        efficiency = self.analyzer.get_efficiency_metrics()\n        \n        if not efficiency:\n            return {}\n        \n        charts = {}\n        \n        # Top ve√≠culos por quilometragem\n        if 'top_veiculos_km' in efficiency:\n            top_km = efficiency['top_veiculos_km'].head(10)\n            \n            charts['top_km'] = px.bar(\n                x=top_km.values,\n                y=top_km.index,\n                orientation='h',\n                title='Top 10 Ve√≠culos por Quilometragem Total',\n                labels={'x': 'Quilometragem Total (km)', 'y': 'Placa'}\n            )\n        \n        # Ve√≠culos mais utilizados\n        if 'veiculos_mais_utilizados' in efficiency:\n            most_used = efficiency['veiculos_mais_utilizados'].head(10)\n            \n            charts['most_used'] = px.bar(\n                x=most_used.values,\n                y=most_used.index,\n                orientation='h',\n                title='Top 10 Ve√≠culos Mais Utilizados',\n                labels={'x': 'N√∫mero de Registros', 'y': 'Placa'}\n            )\n        \n        return charts\n    \n    def create_speed_analysis_charts(self):\n        \"\"\"Cria gr√°ficos de an√°lise de velocidade\"\"\"\n        speed_analysis = self.analyzer.get_speed_analysis()\n        \n        if not speed_analysis:\n            return {}\n        \n        charts = {}\n        \n        # Distribui√ß√£o por faixas de velocidade\n        if 'distribuicao' in speed_analysis:\n            dist_data = speed_analysis['distribuicao']\n            \n            charts['speed_ranges'] = px.pie(\n                values=dist_data.values,\n                names=dist_data.index,\n                title='Distribui√ß√£o por Faixas de Velocidade'\n            )\n        \n        # Velocidade por hora do dia\n        if 'velocidade_por_hora' in speed_analysis:\n            hourly_speed = speed_analysis['velocidade_por_hora']\n            \n            charts['speed_by_hour'] = px.line(\n                x=hourly_speed.index,\n                y=hourly_speed.values,\n                title='Velocidade M√©dia por Hora do Dia',\n                labels={'x': 'Hora', 'y': 'Velocidade M√©dia (km/h)'}\n            )\n        \n        return charts\n    \n    def create_map_visualization(self, sample_size=1000):\n        \"\"\"Cria visualiza√ß√£o de mapa com as localiza√ß√µes\"\"\"\n        df = self.analyzer.filtered_df\n        \n        if df.empty or 'localizacao' not in df.columns:\n            return None\n        \n        # Filtrar apenas uma amostra para performance\n        if len(df) > sample_size:\n            df_sample = df.sample(n=sample_size)\n        else:\n            df_sample = df\n        \n        # Extrair coordenadas (assumindo formato \"lat,lng\")\n        coords_data = []\n        for _, row in df_sample.iterrows():\n            try:\n                if pd.notna(row['localizacao']) and ',' in str(row['localizacao']):\n                    lat, lng = map(float, str(row['localizacao']).split(','))\n                    coords_data.append({\n                        'lat': lat,\n                        'lng': lng,\n                        'placa': row['placa'],\n                        'velocidade': row['velocidade_km'],\n                        'data': row['data']\n                    })\n            except:\n                continue\n        \n        if not coords_data:\n            return None\n        \n        coords_df = pd.DataFrame(coords_data)\n        \n        # Criar mapa\n        fig = px.scatter_mapbox(\n            coords_df,\n            lat='lat',\n            lon='lng',\n            color='velocidade',\n            size='velocidade',\n            hover_data=['placa', 'data'],\n            color_continuous_scale='Viridis',\n            mapbox_style='open-street-map',\n            title='Localiza√ß√£o dos Ve√≠culos (Amostra)',\n            zoom=10\n        )\n        \n        fig.update_layout(height=500)\n        \n        return fig\n    \n    def create_dashboard_summary(self):\n        \"\"\"Cria resumo visual para dashboard\"\"\"\n        kpis = self.analyzer.get_kpis()\n        \n        if not kpis:\n            return None\n        \n        # Criar gr√°fico de gauge para m√©tricas principais\n        fig = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=('Cobertura GPS (%)', 'Utiliza√ß√£o da Frota', \n                          'Velocidade M√©dia', 'Efici√™ncia Operacional'),\n            specs=[[{\"type\": \"indicator\"}, {\"type\": \"indicator\"}],\n                   [{\"type\": \"indicator\"}, {\"type\": \"indicator\"}]]\n        )\n        \n        # Gauge para cobertura GPS\n        fig.add_trace(go.Indicator(\n            mode = \"gauge+number\",\n            value = kpis['cobertura_gps'],\n            domain = {'x': [0, 1], 'y': [0, 1]},\n            title = {'text': \"GPS\"},\n            gauge = {\n                'axis': {'range': [None, 100]},\n                'bar': {'color': \"darkblue\"},\n                'steps': [\n                    {'range': [0, 50], 'color': \"lightgray\"},\n                    {'range': [50, 80], 'color': \"gray\"}],\n                'threshold': {\n                    'line': {'color': \"red\", 'width': 4},\n                    'thickness': 0.75,\n                    'value': 90}}\n        ), row=1, col=1)\n        \n        # Mais indicadores podem ser adicionados aqui...\n        \n        fig.update_layout(height=400)\n        \n        return fig\n","size_bytes":12494},".streamlit/config.toml":{"content":"[server]\nheadless = true\naddress = \"0.0.0.0\"\nport = 5000\n\n[theme]\nbase = \"light\"\nprimaryColor = \"#1f77b4\"\nbackgroundColor = \"#ffffff\"\nsecondaryBackgroundColor = \"#f0f2f6\"\n","size_bytes":171},"pages/8_üö®_Controle_Operacional.py":{"content":"import streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom datetime import datetime, time, timedelta\nimport pytz\nfrom database.db_manager import DatabaseManager\nfrom utils.data_analyzer import DataAnalyzer\n\ndef main():\n    st.title(\"üö® Controle Operacional\")\n    st.markdown(\"**Monitoramento de conformidade operacional das vans da prefeitura**\")\n    \n    # Carregar dados diretamente\n    df_inicial = DatabaseManager.get_dashboard_data()\n    if df_inicial.empty:\n        st.warning(\"‚ö†Ô∏è N√£o h√° dados carregados. Fa√ßa upload de arquivos CSV primeiro.\")\n        return\n    else:\n        st.success(f\"‚úÖ Dados carregados: {len(df_inicial):,} registros para controle operacional\")\n    \n    # Sidebar com filtros\n    with st.sidebar:\n        st.header(\"üîç Filtros\")\n        \n        # Filtro de cliente\n        clients = get_client_list()\n        selected_client = st.selectbox(\n            \"Cliente:\",\n            [\"Todos\"] + clients,\n            index=0\n        )\n        \n        # Filtro de ve√≠culo\n        vehicles = get_vehicle_list(selected_client if selected_client != \"Todos\" else None)\n        selected_vehicle = st.selectbox(\n            \"Ve√≠culo:\",\n            [\"Todos\"] + vehicles,\n            index=0\n        )\n        \n        # Filtro de per√≠odo\n        st.subheader(\"üìÖ Per√≠odo de An√°lise\")\n        col1, col2 = st.columns(2)\n        with col1:\n            start_date = st.date_input(\n                \"Data In√≠cio:\",\n                value=datetime(2025, 8, 6).date()  # In√≠cio dos dados dispon√≠veis\n            )\n        with col2:\n            end_date = st.date_input(\n                \"Data Fim:\",\n                value=datetime(2025, 8, 31).date()  # Fim dos dados dispon√≠veis\n            )\n        \n        # Filtros de hor√°rio autorizado\n        st.subheader(\"‚è∞ Filtros de Hor√°rio\")\n        \n        # Bot√µes de preset para per√≠odos autorizados\n        st.markdown(\"**Per√≠odos Autorizados:**\")\n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if st.button(\"üåÖ Manh√£\\n04:00-07:00\", key=\"morning\"):\n                st.session_state.time_filter = \"morning\"\n        with col2:\n            if st.button(\"üçΩÔ∏è Almo√ßo\\n10:50-13:00\", key=\"lunch\"):\n                st.session_state.time_filter = \"lunch\"\n        with col3:\n            if st.button(\"üåá Tarde\\n16:50-19:00\", key=\"afternoon\"):\n                st.session_state.time_filter = \"afternoon\"\n        \n        # Filtro personalizado de hor√°rio\n        time_filter_mode = st.selectbox(\n            \"Filtrar por per√≠odo:\",\n            [\"Todos os hor√°rios\", \"Apenas hor√°rios autorizados\", \"Apenas viola√ß√µes de hor√°rio\", \"Per√≠odo personalizado\"],\n            index=0\n        )\n        \n        custom_start_time = None\n        custom_end_time = None\n        if time_filter_mode == \"Per√≠odo personalizado\":\n            col1, col2 = st.columns(2)\n            with col1:\n                custom_start_time = st.time_input(\"Hora in√≠cio:\", value=time(0, 0))\n            with col2:\n                custom_end_time = st.time_input(\"Hora fim:\", value=time(23, 59))\n        \n        # Crit√©rios de movimento\n        st.subheader(\"üöó Crit√©rios de Movimento\")\n        include_stationary = st.checkbox(\n            \"Incluir ve√≠culos parados na an√°lise de viola√ß√µes\",\n            value=False,\n            help=\"Quando desmarcado, apenas ve√≠culos em movimento (velocidade > 0 ou igni√ß√£o ligada) ser√£o considerados para viola√ß√µes\"\n        )\n        \n        # Limites de velocidade\n        st.subheader(\"‚ö° Configura√ß√µes\")\n        speed_violation_threshold = st.slider(\n            \"Limite de velocidade (km/h):\",\n            min_value=10,\n            max_value=120,\n            value=80,\n            step=5,\n            help=\"Velocidades acima deste valor ser√£o destacadas como picos\"\n        )\n    \n    # Aplicar filtros aos dados j√° carregados\n    # Usar m√©todo centralizado de filtros do DataAnalyzer\n    analyzer = DataAnalyzer(df_inicial)\n    \n    # Aplicar filtros com m√©todo centralizado do DataAnalyzer\n    df = analyzer.apply_filters(\n        cliente=selected_client,\n        placa=selected_vehicle,\n        data_inicio=start_date,\n        data_fim=end_date\n    )\n    \n    # Aplicar filtros de hor√°rio se especificado\n    if time_filter_mode != \"Todos os hor√°rios\" and not df.empty:\n        df = apply_time_filters(df, time_filter_mode, custom_start_time, custom_end_time)\n    \n    if df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado encontrado para os filtros selecionados.\")\n        return\n    \n    # Processar dados para an√°lise operacional\n    df = process_operational_data(df, include_stationary=include_stationary)\n    \n    # Abas principais\n    tab1, tab2, tab3, tab4 = st.tabs([\n        \"üìä Resumo Operacional\", \n        \"‚ö†Ô∏è Viola√ß√µes Detectadas\", \n        \"üó∫Ô∏è Mapa de Trajetos\", \n        \"üìã Relat√≥rio Detalhado\"\n    ])\n    \n    with tab1:\n        show_operational_summary(df)\n    \n    with tab2:\n        show_violations(df)\n    \n    with tab3:\n        show_trajectory_map(df)\n    \n    with tab4:\n        show_detailed_report(df)\n\n@st.cache_data(ttl=300)\ndef get_client_list():\n    \"\"\"Busca lista de clientes com cache para melhor performance\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty and 'cliente' in df.columns:\n            return sorted(df['cliente'].unique().tolist())\n        return []\n    except Exception as e:\n        st.error(f\"Erro ao carregar clientes: {str(e)}\")\n        return []\n\n@st.cache_data(ttl=300)\ndef get_vehicle_list(client_filter=None):\n    \"\"\"Busca lista de ve√≠culos com cache para melhor performance\"\"\"\n    try:\n        df = DatabaseManager.get_dashboard_data()\n        if not df.empty and 'placa' in df.columns:\n            # Filtrar por cliente se especificado\n            if client_filter and client_filter != \"Todos\":\n                df = df[df['cliente'] == client_filter]\n            return sorted(df['placa'].unique().tolist())\n        return []\n    except Exception as e:\n        st.error(f\"Erro ao carregar ve√≠culos: {str(e)}\")\n        return []\n\n# Fun√ß√£o removida - agora usando m√©todo centralizado DataAnalyzer.apply_filters()\n\ndef load_filtered_data(client_filter, vehicle_filter, start_date, end_date):\n    \"\"\"Fun√ß√£o mantida para compatibilidade - Carrega dados filtrados\"\"\"\n    try:\n        client_f = None if client_filter == \"Todos\" else client_filter\n        vehicle_f = None if vehicle_filter == \"Todos\" else vehicle_filter\n        \n        start_datetime = datetime.combine(start_date, datetime.min.time())\n        end_datetime = datetime.combine(end_date, datetime.max.time())\n        \n        df = DatabaseManager.get_dashboard_data(\n            client_filter=client_f,\n            vehicle_filter=vehicle_f,\n            start_date=start_datetime,\n            end_date=end_datetime\n        )\n        \n        return df\n    except Exception as e:\n        st.error(f\"Erro ao carregar dados: {str(e)}\")\n        return pd.DataFrame()\n\ndef apply_time_filters(df, time_filter_mode, custom_start_time=None, custom_end_time=None):\n    \"\"\"Aplica filtros de hor√°rio aos dados\"\"\"\n    if df.empty:\n        return df\n        \n    df_filtered = df.copy()\n    \n    # Verificar se colunas necess√°rias existem\n    if 'data' not in df_filtered.columns:\n        return df_filtered\n    \n    # Converter para datetime se necess√°rio\n    if not pd.api.types.is_datetime64_any_dtype(df_filtered['data']):\n        df_filtered['data'] = pd.to_datetime(df_filtered['data'], errors='coerce')\n    \n    # Filtrar por per√≠odo autorizado ou viola√ß√µes\n    if time_filter_mode == \"Apenas hor√°rios autorizados\":\n        # Criar l√≥gica de hor√°rio autorizado se n√£o existir\n        if 'operacao_autorizada' not in df_filtered.columns:\n            df_filtered = create_authorization_logic(df_filtered)\n        df_filtered = df_filtered[df_filtered['operacao_autorizada'] == True]\n    elif time_filter_mode == \"Apenas viola√ß√µes de hor√°rio\":\n        # Criar l√≥gica de hor√°rio autorizado se n√£o existir\n        if 'operacao_autorizada' not in df_filtered.columns:\n            df_filtered = create_authorization_logic(df_filtered)\n        df_filtered = df_filtered[df_filtered['operacao_autorizada'] == False]\n    elif time_filter_mode == \"Per√≠odo personalizado\" and custom_start_time and custom_end_time:\n        # Filtrar por hor√°rio personalizado\n        hour_filter = (\n            (df_filtered['data'].dt.time >= custom_start_time) & \n            (df_filtered['data'].dt.time <= custom_end_time)\n        )\n        df_filtered = df_filtered[hour_filter]\n    \n    return df_filtered\n\ndef create_authorization_logic(df):\n    \"\"\"Cria a l√≥gica de autoriza√ß√£o para o DataFrame\"\"\"\n    if df.empty:\n        return df\n    \n    df = df.copy()\n    \n    # Extrair informa√ß√µes de tempo\n    df['hora'] = df['data'].dt.hour\n    df['minuto'] = df['data'].dt.minute\n    df['dia_semana'] = df['data'].dt.dayofweek  # 0=Segunda, 6=Domingo\n    \n    # Determinar se o hor√°rio est√° dentro dos per√≠odos autorizados\n    df['horario_permitido'] = df.apply(lambda row: is_authorized_time_simple(row['hora'], row['minuto']), axis=1)\n    df['dia_util'] = df['dia_semana'] < 5  # Segunda a Sexta (0-4)\n    df['operacao_autorizada'] = df['horario_permitido'] & df['dia_util']\n    \n    return df\n\ndef is_authorized_time_simple(hora, minuto):\n    \"\"\"Verifica se o hor√°rio est√° dentro dos per√≠odos autorizados - vers√£o simplificada\"\"\"\n    time_current = time(hora, minuto)\n    \n    # Hor√°rios permitidos pela prefeitura\n    morning_start = time(4, 0)   # 04:00\n    morning_end = time(7, 0)     # 07:00\n    \n    lunch_start = time(10, 50)   # 10:50\n    lunch_end = time(13, 0)      # 13:00\n    \n    afternoon_start = time(16, 50)  # 16:50\n    afternoon_end = time(19, 0)     # 19:00\n    \n    # Verificar se est√° em algum per√≠odo permitido\n    is_morning = morning_start <= time_current <= morning_end\n    is_lunch = lunch_start <= time_current <= lunch_end\n    is_afternoon = afternoon_start <= time_current <= afternoon_end\n    \n    return is_morning or is_lunch or is_afternoon\n\ndef safe_column_access(df, column, default_value=None, numeric=False):\n    \"\"\"Acesso seguro a colunas do DataFrame com valores padr√£o\"\"\"\n    if column not in df.columns:\n        if numeric:\n            return pd.Series([0] * len(df), index=df.index)\n        else:\n            return pd.Series([default_value] * len(df), index=df.index)\n    \n    series = df[column]\n    if numeric:\n        return pd.to_numeric(series, errors='coerce').fillna(0)\n    else:\n        return series.fillna(default_value if default_value is not None else '')\n\ndef process_operational_data(df, include_stationary=False):\n    \"\"\"Processa dados para an√°lise operacional com crit√©rios de movimento\"\"\"\n    if df.empty:\n        return df\n    \n    df = df.copy()\n    \n    # Verificar e converter colunas essenciais com acesso seguro\n    if 'data' in df.columns:\n        df['data'] = pd.to_datetime(df['data'], errors='coerce')\n    else:\n        # Se n√£o h√° coluna de data, criar uma padr√£o\n        df['data'] = pd.Timestamp.now()\n    \n    # Acessar colunas de forma segura\n    df['velocidade_km'] = safe_column_access(df, 'velocidade_km', 0, numeric=True)\n    df['ignicao'] = safe_column_access(df, 'ignicao', 'D')\n    df['latitude'] = safe_column_access(df, 'latitude', 0, numeric=True)\n    df['longitude'] = safe_column_access(df, 'longitude', 0, numeric=True)\n    df['endereco'] = safe_column_access(df, 'endereco', 'Local n√£o informado')\n    df['placa'] = safe_column_access(df, 'placa', 'Placa n√£o informada')\n    \n    # Extrair informa√ß√µes de tempo\n    df['hora'] = df['data'].dt.hour\n    df['minuto'] = df['data'].dt.minute\n    df['dia_semana'] = df['data'].dt.dayofweek  # 0=Segunda, 6=Domingo\n    df['dia_semana_nome'] = df['data'].dt.strftime('%A')\n    df['data_date'] = df['data'].dt.date\n    df['hora_minuto'] = df['data'].dt.time\n    \n    # Determinar se o ve√≠culo est√° em movimento\n    df['em_movimento'] = (\n        (df['velocidade_km'] > 0) | \n        (df['ignicao'].isin(['D', 'L', 'Dirigindo', 'Ligado']))\n    )\n    \n    # Definir hor√°rios permitidos pela prefeitura\n    df['horario_permitido'] = df.apply(is_authorized_time, axis=1)\n    df['dia_util'] = df['dia_semana'] < 5  # Segunda a Sexta (0-4)\n    df['operacao_autorizada'] = df['horario_permitido'] & df['dia_util']\n    \n    # Aplicar crit√©rio de movimento para viola√ß√µes (se configurado)\n    if not include_stationary:\n        # Apenas considerar viola√ß√µes quando o ve√≠culo est√° em movimento\n        df['violacao_considerada'] = ~df['operacao_autorizada'] & df['em_movimento']\n    else:\n        # Considerar todas as viola√ß√µes, independente do movimento\n        df['violacao_considerada'] = ~df['operacao_autorizada']\n    \n    # Classificar viola√ß√µes considerando movimento\n    df['tipo_violacao'] = df.apply(lambda row: classify_violation(row, include_stationary), axis=1)\n    \n    return df\n\ndef is_authorized_time(row):\n    \"\"\"Verifica se o hor√°rio est√° dentro dos per√≠odos autorizados\"\"\"\n    hora = row['hora']\n    minuto = row['minuto']\n    time_current = time(hora, minuto)\n    \n    # Hor√°rios permitidos pela prefeitura\n    morning_start = time(4, 0)   # 04:00\n    morning_end = time(7, 0)     # 07:00\n    \n    lunch_start = time(10, 50)   # 10:50\n    lunch_end = time(13, 0)      # 13:00\n    \n    afternoon_start = time(16, 50)  # 16:50\n    afternoon_end = time(19, 0)     # 19:00\n    \n    # Verificar se est√° em algum per√≠odo permitido\n    is_morning = morning_start <= time_current <= morning_end\n    is_lunch = lunch_start <= time_current <= lunch_end\n    is_afternoon = afternoon_start <= time_current <= afternoon_end\n    \n    return is_morning or is_lunch or is_afternoon\n\ndef classify_violation(row, include_stationary=False):\n    \"\"\"Classifica o tipo de viola√ß√£o considerando crit√©rios de movimento\"\"\"\n    # Se a opera√ß√£o √© autorizada, sempre √© v√°lida\n    if row['operacao_autorizada']:\n        return \"‚úÖ Autorizada\"\n    \n    # Se n√£o devemos incluir ve√≠culos parados e o ve√≠culo n√£o est√° em movimento\n    if not include_stationary and not row.get('em_movimento', True):\n        return \"üöô Parado (N√£o Analisado)\"\n    \n    # Classificar tipos de viola√ß√£o\n    if not row['dia_util']:\n        return \"üö´ Final de Semana\"\n    elif not row['horario_permitido']:\n        return \"‚è∞ Hor√°rio N√£o Autorizado\"\n    else:\n        return \"‚ùì Outros\"\n\ndef show_operational_summary(df):\n    \"\"\"Mostra resumo operacional\"\"\"\n    st.markdown(\"### üìä Resumo Operacional\")\n    \n    # M√©tricas principais\n    col1, col2, col3, col4 = st.columns(4)\n    \n    total_records = len(df)\n    authorized_records = len(df[df['operacao_autorizada']])\n    violation_records = len(df[~df['operacao_autorizada']])\n    compliance_rate = (authorized_records / total_records * 100) if total_records > 0 else 0\n    \n    with col1:\n        st.metric(\n            \"üìä Total de Registros\",\n            f\"{total_records:,}\",\n            help=\"Total de registros no per√≠odo selecionado\"\n        )\n    \n    with col2:\n        st.metric(\n            \"‚úÖ Opera√ß√µes Autorizadas\",\n            f\"{authorized_records:,}\",\n            delta=f\"{(authorized_records/total_records*100):.1f}%\" if total_records > 0 else \"0%\"\n        )\n    \n    with col3:\n        st.metric(\n            \"‚ö†Ô∏è Viola√ß√µes Detectadas\",\n            f\"{violation_records:,}\",\n            delta=f\"-{(violation_records/total_records*100):.1f}%\" if total_records > 0 else \"0%\",\n            delta_color=\"inverse\"\n        )\n    \n    with col4:\n        st.metric(\n            \"üìà Taxa de Conformidade\",\n            f\"{compliance_rate:.1f}%\",\n            delta=\"Meta: 100%\",\n            delta_color=\"normal\" if compliance_rate >= 95 else \"inverse\"\n        )\n    \n    # Gr√°fico de distribui√ß√£o por tipo de opera√ß√£o\n    st.markdown(\"### üìà Distribui√ß√£o de Opera√ß√µes\")\n    \n    col1, col2 = st.columns(2)\n    \n    with col1:\n        # Gr√°fico de pizza - Autorizada vs Viola√ß√£o\n        violation_counts = df['operacao_autorizada'].value_counts()\n        \n        fig_pie = px.pie(\n            values=violation_counts.values,\n            names=['‚úÖ Autorizadas' if x else '‚ö†Ô∏è Viola√ß√µes' for x in violation_counts.index],\n            title=\"Propor√ß√£o de Opera√ß√µes\",\n            color_discrete_map={\n                '‚úÖ Autorizadas': '#2E8B57',\n                '‚ö†Ô∏è Viola√ß√µes': '#DC143C'\n            }\n        )\n        st.plotly_chart(fig_pie, use_container_width=True)\n    \n    with col2:\n        # Gr√°fico de barras - Tipos de viola√ß√£o\n        violation_types = df['tipo_violacao'].value_counts()\n        \n        fig_bar = px.bar(\n            x=violation_types.index,\n            y=violation_types.values,\n            title=\"Tipos de Viola√ß√£o\",\n            color=violation_types.index,\n            color_discrete_map={\n                '‚úÖ Autorizada': '#2E8B57',\n                'üö´ Final de Semana': '#FF4500',\n                '‚è∞ Hor√°rio N√£o Autorizado': '#DC143C',\n                '‚ùì Outros': '#696969'\n            }\n        )\n        fig_bar.update_layout(showlegend=False)\n        st.plotly_chart(fig_bar, use_container_width=True)\n\ndef show_violations(df):\n    \"\"\"Mostra viola√ß√µes detectadas\"\"\"\n    st.markdown(\"### ‚ö†Ô∏è Viola√ß√µes Operacionais Detectadas\")\n    \n    # Filtrar apenas viola√ß√µes\n    violations_df = df[~df['operacao_autorizada']].copy()\n    \n    if violations_df.empty:\n        st.success(\"üéâ Nenhuma viola√ß√£o detectada no per√≠odo selecionado!\")\n        return\n    \n    # Resumo das viola√ß√µes\n    col1, col2, col3 = st.columns(3)\n    \n    weekend_violations = len(violations_df[violations_df['tipo_violacao'] == 'üö´ Final de Semana'])\n    time_violations = len(violations_df[violations_df['tipo_violacao'] == '‚è∞ Hor√°rio N√£o Autorizado'])\n    \n    with col1:\n        st.metric(\"üö´ Final de Semana\", f\"{weekend_violations:,}\")\n    with col2:\n        st.metric(\"‚è∞ Hor√°rio Irregular\", f\"{time_violations:,}\")\n    with col3:\n        unique_vehicles = violations_df['placa'].nunique()\n        st.metric(\"üöó Ve√≠culos Envolvidos\", unique_vehicles)\n    \n    # An√°lise temporal das viola√ß√µes\n    st.markdown(\"#### üìÖ Viola√ß√µes por Dia\")\n    \n    daily_violations = violations_df.groupby(['data_date', 'tipo_violacao']).size().reset_index(name='count')\n    daily_violations['data_date'] = pd.to_datetime(daily_violations['data_date'])\n    \n    fig_daily = px.bar(\n        daily_violations,\n        x='data_date',\n        y='count',\n        color='tipo_violacao',\n        title=\"Viola√ß√µes Di√°rias por Tipo\",\n        color_discrete_map={\n            'üö´ Final de Semana': '#FF4500',\n            '‚è∞ Hor√°rio N√£o Autorizado': '#DC143C',\n            '‚ùì Outros': '#696969'\n        }\n    )\n    st.plotly_chart(fig_daily, use_container_width=True)\n    \n    # Tabela detalhada das viola√ß√µes\n    st.markdown(\"#### üìã Detalhes das Viola√ß√µes\")\n    \n    # Preparar dados para tabela\n    display_df = violations_df[[\n        'data', 'placa', 'tipo_violacao', 'velocidade_km', 'endereco'\n    ]].copy()\n    \n    display_df['data'] = display_df['data'].dt.strftime('%d/%m/%Y %H:%M:%S')\n    display_df = display_df.rename(columns={\n        'data': 'Data/Hora',\n        'placa': 'Ve√≠culo',\n        'tipo_violacao': 'Tipo de Viola√ß√£o',\n        'velocidade_km': 'Velocidade (km/h)',\n        'endereco': 'Local'\n    })\n    \n    # Ordenar por data mais recente\n    display_df = display_df.sort_values('Data/Hora', ascending=False)\n    \n    st.dataframe(\n        display_df.head(100),  # Limitar a 100 registros para performance\n        use_container_width=True,\n        hide_index=True\n    )\n    \n    if len(display_df) > 100:\n        st.info(f\"üìã Mostrando os 100 registros mais recentes de {len(display_df):,} viola√ß√µes totais.\")\n\ndef show_trajectory_map(df):\n    \"\"\"Mostra mapa de trajetos e picos de velocidade\"\"\"\n    st.markdown(\"### üó∫Ô∏è Mapa de Trajetos e Velocidade\")\n    \n    # Verificar se h√° coordenadas\n    map_df = df[(df['latitude'].notna()) & (df['longitude'].notna()) & \n                (df['latitude'] != 0) & (df['longitude'] != 0)].copy()\n    \n    if map_df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado com coordenadas GPS v√°lidas encontrado.\")\n        return\n    \n    # Filtros para o mapa\n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        map_filter = st.selectbox(\n            \"Mostrar no mapa:\",\n            [\"Todas as opera√ß√µes\", \"Apenas viola√ß√µes\", \"Apenas autorizadas\"]\n        )\n    \n    with col2:\n        speed_threshold = st.number_input(\n            \"Pico de velocidade acima de (km/h):\",\n            min_value=0,\n            max_value=200,\n            value=80,\n            step=5\n        )\n    \n    with col3:\n        if st.button(\"üîÑ Atualizar Mapa\"):\n            st.rerun()\n    \n    # Filtrar dados do mapa\n    if map_filter == \"Apenas viola√ß√µes\":\n        map_df = map_df[~map_df['operacao_autorizada']]\n    elif map_filter == \"Apenas autorizadas\":\n        map_df = map_df[map_df['operacao_autorizada']]\n    \n    if map_df.empty:\n        st.warning(\"‚ö†Ô∏è Nenhum dado para exibir no mapa com os filtros selecionados.\")\n        return\n    \n    # Identificar picos de velocidade\n    map_df['pico_velocidade'] = map_df['velocidade_km'] > speed_threshold\n    \n    # Preparar dados para o mapa com cores por status\n    map_df['cor_status'] = map_df.apply(lambda row: \n        '#FF0000' if row['pico_velocidade'] else  # Vermelho para picos de velocidade\n        '#DC143C' if row['tipo_violacao'] == '‚è∞ Hor√°rio N√£o Autorizado' else  # Vermelho escuro para viola√ß√µes de hor√°rio\n        '#FF4500' if row['tipo_violacao'] == 'üö´ Final de Semana' else  # Laranja para final de semana\n        '#2E8B57' if row['tipo_violacao'] == '‚úÖ Autorizada' else  # Verde para autorizadas\n        '#696969', axis=1  # Cinza para outros\n    )\n    \n    map_df['tamanho_ponto'] = map_df['pico_velocidade'].apply(lambda x: 12 if x else 6)\n    map_df['hover_info'] = map_df.apply(lambda row: \n        f\"<b>{row['placa']}</b><br>\" +\n        f\"Data: {row['data'].strftime('%d/%m/%Y %H:%M')}<br>\" +\n        f\"Velocidade: {row['velocidade_km']:.1f} km/h<br>\" +\n        f\"Status: {row['tipo_violacao']}<br>\" +\n        f\"Local: {row['endereco'][:50]}...\", axis=1\n    )\n    \n    # Calcular centro do mapa\n    center_lat = map_df['latitude'].mean()\n    center_lon = map_df['longitude'].mean()\n    \n    # Criar mapa real com Mapbox e OpenStreetMap\n    try:\n        # Usar scatter_mapbox para mapa real\n        fig_map = px.scatter_mapbox(\n            map_df,\n            lat='latitude',\n            lon='longitude',\n            color='tipo_violacao',\n            size='tamanho_ponto',\n            hover_data={\n                'placa': True,\n                'velocidade_km': ':.1f',\n                'data': True,\n                'endereco': True,\n                'latitude': False,\n                'longitude': False,\n                'tamanho_ponto': False\n            },\n            color_discrete_map={\n                '‚úÖ Autorizada': '#2E8B57',\n                'üö´ Final de Semana': '#FF4500', \n                '‚è∞ Hor√°rio N√£o Autorizado': '#DC143C',\n                'üöó Parado (N√£o Analisado)': '#808080',\n                '‚ùì Outros': '#696969'\n            },\n            title=\"üó∫Ô∏è Mapa Real de Trajetos e Opera√ß√µes\",\n            mapbox_style=\"open-street-map\",  # Usar OpenStreetMap\n            height=600,\n            zoom=12\n        )\n        \n        # Configurar layout do mapa\n        fig_map.update_layout(\n            mapbox=dict(\n                center=dict(lat=center_lat, lon=center_lon),\n                zoom=12\n            ),\n            title={\n                'text': \"üó∫Ô∏è Mapa Real de Trajetos e Opera√ß√µes\",\n                'x': 0.5,\n                'xanchor': 'center'\n            },\n            showlegend=True,\n            height=600\n        )\n        \n        # Adicionar linhas de trajeto por ve√≠culo\n        vehicles = map_df['placa'].unique()\n        colors = px.colors.qualitative.Set1\n        \n        for i, vehicle in enumerate(vehicles[:5]):  # Limitar a 5 ve√≠culos para performance\n            vehicle_df = map_df[map_df['placa'] == vehicle].sort_values('data')\n            if len(vehicle_df) > 1:  # S√≥ adicionar linha se houver m√∫ltiplos pontos\n                color = colors[i % len(colors)]\n                \n                # Adicionar linha de trajeto\n                fig_map.add_trace(\n                    go.Scattermapbox(\n                        lat=vehicle_df['latitude'],\n                        lon=vehicle_df['longitude'],\n                        mode='lines',\n                        line=dict(width=2, color=color),\n                        name=f'Trajeto {vehicle}',\n                        showlegend=True,\n                        opacity=0.7\n                    )\n                )\n        \n        st.plotly_chart(fig_map, use_container_width=True)\n        \n    except Exception as e:\n        st.error(f\"Erro ao carregar mapa: {str(e)}\")\n        st.info(\"üó∫Ô∏è Usando visualiza√ß√£o alternativa...\")\n        \n        # Fallback: usar pydeck se mapbox falhar\n        try:\n            import pydeck as pdk\n            \n            # Configurar layer do pydeck\n            layer = pdk.Layer(\n                'ScatterplotLayer',\n                data=map_df,\n                get_position='[longitude, latitude]',\n                get_color='[255, 0, 0, 160]',  # Vermelho semi-transparente\n                get_radius=50,\n                pickable=True\n            )\n            \n            # Configurar viewport\n            view_state = pdk.ViewState(\n                latitude=center_lat,\n                longitude=center_lon,\n                zoom=12,\n                pitch=0\n            )\n            \n            # Criar deck\n            deck = pdk.Deck(\n                layers=[layer],\n                initial_view_state=view_state,\n                tooltip={\n                    'text': 'Ve√≠culo: {placa}\\nVelocidade: {velocidade_km} km/h\\nStatus: {tipo_violacao}'\n                }\n            )\n            \n            st.pydeck_chart(deck)\n            \n        except Exception as e2:\n            st.error(f\"Erro ao carregar visualiza√ß√£o alternativa: {str(e2)}\")\n            st.warning(\"‚ö†Ô∏è Mapa n√£o p√¥de ser carregado. Verifique os dados de coordenadas.\")\n    \n    # Estat√≠sticas do mapa\n    col1, col2, col3, col4 = st.columns(4)\n    \n    with col1:\n        st.metric(\"üìç Pontos no Mapa\", f\"{len(map_df):,}\")\n    with col2:\n        st.metric(\"‚≠ê Picos de Velocidade\", f\"{len(map_df[map_df['pico_velocidade']]):,}\")\n    with col3:\n        st.metric(\"üöó Ve√≠culos √önicos\", f\"{map_df['placa'].nunique()}\")\n    with col4:\n        avg_speed = map_df['velocidade_km'].mean()\n        st.metric(\"‚ö° Velocidade M√©dia\", f\"{avg_speed:.1f} km/h\")\n\ndef show_detailed_report(df):\n    \"\"\"Mostra relat√≥rio detalhado\"\"\"\n    st.markdown(\"### üìã Relat√≥rio Detalhado de Conformidade\")\n    \n    # Relat√≥rio por ve√≠culo\n    st.markdown(\"#### üöó An√°lise por Ve√≠culo\")\n    \n    vehicle_summary = df.groupby('placa').agg({\n        'data': 'count',\n        'operacao_autorizada': ['sum', 'mean'],\n        'velocidade_km': ['mean', 'max'],\n        'tipo_violacao': lambda x: (x != '‚úÖ Autorizada').sum()\n    }).round(2)\n    \n    vehicle_summary.columns = ['Total Registros', 'Opera√ß√µes Autorizadas', 'Taxa Conformidade (%)', \n                              'Velocidade M√©dia', 'Velocidade M√°xima', 'Total Viola√ß√µes']\n    vehicle_summary['Taxa Conformidade (%)'] = (vehicle_summary['Taxa Conformidade (%)'] * 100).round(1)\n    \n    # Colorir c√©lulas baseado na conformidade\n    def color_compliance(val):\n        if isinstance(val, (int, float)):\n            if val >= 95:\n                return 'color: green'\n            elif val >= 80:\n                return 'color: orange'\n            else:\n                return 'color: red'\n        return ''\n    \n    styled_df = vehicle_summary.style.applymap(\n        color_compliance, \n        subset=['Taxa Conformidade (%)']\n    )\n    \n    st.dataframe(styled_df, use_container_width=True)\n    \n    # Relat√≥rio por dia da semana\n    st.markdown(\"#### üìÖ An√°lise por Dia da Semana\")\n    \n    weekday_names = {\n        0: 'Segunda', 1: 'Ter√ßa', 2: 'Quarta', 3: 'Quinta', \n        4: 'Sexta', 5: 'S√°bado', 6: 'Domingo'\n    }\n    \n    df['dia_semana_nome'] = df['dia_semana'].map(weekday_names)\n    \n    weekday_summary = df.groupby('dia_semana_nome').agg({\n        'data': 'count',\n        'operacao_autorizada': ['sum', 'mean'],\n        'tipo_violacao': lambda x: (x != '‚úÖ Autorizada').sum()\n    }).round(2)\n    \n    weekday_summary.columns = ['Total Registros', 'Opera√ß√µes Autorizadas', 'Taxa Conformidade', 'Total Viola√ß√µes']\n    weekday_summary['Taxa Conformidade'] = (weekday_summary['Taxa Conformidade'] * 100).round(1)\n    \n    # Reordenar dias da semana\n    day_order = ['Segunda', 'Ter√ßa', 'Quarta', 'Quinta', 'Sexta', 'S√°bado', 'Domingo']\n    weekday_summary = weekday_summary.reindex([day for day in day_order if day in weekday_summary.index])\n    \n    fig_weekday = px.bar(\n        weekday_summary.reset_index(),\n        x='dia_semana_nome',\n        y='Taxa Conformidade',\n        title=\"Taxa de Conformidade por Dia da Semana\",\n        color='Taxa Conformidade',\n        color_continuous_scale='RdYlGn'\n    )\n    fig_weekday.update_layout(showlegend=False)\n    st.plotly_chart(fig_weekday, use_container_width=True)\n    \n    # Exportar relat√≥rio\n    if st.button(\"üìÑ Exportar Relat√≥rio Completo\"):\n        # Aqui voc√™ pode implementar exporta√ß√£o para PDF ou Excel\n        st.success(\"üéâ Funcionalidade de exporta√ß√£o ser√° implementada em breve!\")\n\nif __name__ == \"__main__\":\n    main()","size_bytes":29902},"pages/5_üó∫Ô∏è_Mapa_de_Rotas.py":{"content":"\"\"\"Painel de Mapa de Rotas - An√°lise de Trajetos e Velocidade\"\"\"\nimport streamlit as st\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport numpy as np\nfrom datetime import datetime, timedelta\nfrom database.db_manager import DatabaseManager\nfrom utils.data_analyzer import DataAnalyzer\nimport folium\nfrom streamlit_folium import st_folium\n\nst.set_page_config(page_title=\"Mapa de Rotas\", page_icon=\"üó∫Ô∏è\", layout=\"wide\")\nst.title(\"üó∫Ô∏è Mapa de Rotas\")\nst.markdown(\"**An√°lise de trajetos, frequ√™ncia de rotas, desvios e picos de velocidade**\")\n\n# Carregar dados\ndf = DatabaseManager.get_dashboard_data()\nif df.empty:\n    st.warning(\"‚ö†Ô∏è Nenhum dado encontrado. Fa√ßa o upload de um arquivo CSV primeiro.\")\n    st.stop()\nelse:\n    st.success(f\"‚úÖ Dados carregados: {len(df):,} registros para an√°lise de rotas\")\n\n# Filtros na sidebar\nwith st.sidebar:\n    st.header(\"üîç Filtros de An√°lise\")\n    \n    # Filtro de cliente\n    clients = [\"Todos\"] + sorted(df['cliente'].unique().tolist())\n    selected_client = st.selectbox(\"Cliente:\", clients, index=0)\n    \n    # Filtrar ve√≠culos baseado no cliente\n    if selected_client != \"Todos\":\n        vehicles_df = df[df['cliente'] == selected_client]\n    else:\n        vehicles_df = df\n    \n    vehicles = [\"Todos\"] + sorted(vehicles_df['placa'].unique().tolist())\n    selected_vehicle = st.selectbox(\"Ve√≠culo:\", vehicles, index=0)\n    \n    # Filtro de per√≠odo\n    st.subheader(\"üìÖ Per√≠odo\")\n    col1, col2 = st.columns(2)\n    with col1:\n        start_date = st.date_input(\n            \"Data In√≠cio:\",\n            value=datetime.now().date() - timedelta(days=7)\n        )\n    with col2:\n        end_date = st.date_input(\n            \"Data Fim:\",\n            value=datetime.now().date()\n        )\n    \n    # Filtros de an√°lise\n    st.subheader(\"‚ö° Par√¢metros de An√°lise\")\n    speed_threshold = st.slider(\n        \"Limite de velocidade para picos (km/h):\",\n        min_value=50,\n        max_value=120,\n        value=80,\n        step=5\n    )\n    \n    deviation_radius = st.slider(\n        \"Raio para detec√ß√£o de desvios (km):\",\n        min_value=0.5,\n        max_value=5.0,\n        value=2.0,\n        step=0.5\n    )\n\n# Aplicar filtros usando m√©todo centralizado do DataAnalyzer\nanalyzer = DataAnalyzer(df)\nfiltered_df = analyzer.apply_filters(\n    cliente=selected_client,\n    placa=selected_vehicle,\n    data_inicio=start_date,\n    data_fim=end_date\n)\n\nif filtered_df.empty:\n    st.warning(\"‚ö†Ô∏è Nenhum dado encontrado para os filtros selecionados.\")\n    \n    # Ajudar o usu√°rio a entender o problema\n    st.info(\"üí° **Dicas para encontrar dados:**\")\n    st.markdown(\"\"\"\n    - ‚úÖ Verifique se o **per√≠odo de datas** corresponde aos dados dispon√≠veis\n    - ‚úÖ Experimente ampliar o **intervalo de datas** \n    - ‚úÖ Tente selecionar **\"Todos\"** para cliente ou ve√≠culo\n    - ‚úÖ Use filtros mais amplos para ver dados dispon√≠veis\n    \"\"\")\n    st.stop()\n\n# Preparar dados para an√°lise de rotas\ndef prepare_route_data(df):\n    \"\"\"Prepara dados para an√°lise de rotas\"\"\"\n    route_df = df.copy()\n    \n    # Garantir colunas necess√°rias\n    required_cols = ['latitude', 'longitude', 'velocidade_km', 'data', 'placa']\n    available_cols = [col for col in required_cols if col in route_df.columns]\n    \n    if len(available_cols) < 3:\n        return pd.DataFrame()\n    \n    # Remover registros sem coordenadas v√°lidas\n    route_df = route_df.dropna(subset=['latitude', 'longitude'])\n    route_df = route_df[(route_df['latitude'] != 0) & (route_df['longitude'] != 0)]\n    \n    # Converter velocidade para num√©rico\n    if 'velocidade_km' in route_df.columns:\n        route_df['velocidade_km'] = pd.to_numeric(route_df['velocidade_km'], errors='coerce')\n        route_df['velocidade_km'] = route_df['velocidade_km'].fillna(0)\n    \n    return route_df\n\nroute_data = prepare_route_data(filtered_df)\n\nif route_data.empty:\n    st.warning(\"‚ö†Ô∏è Dados insuficientes para an√°lise de rotas (faltam coordenadas v√°lidas).\")\n    st.stop()\n\n# Tabs principais\ntab1, tab2, tab3, tab4, tab5 = st.tabs([\n    \"üó∫Ô∏è Mapa Interativo\", \n    \"üìä An√°lise de Velocidade\", \n    \"üõ£Ô∏è Rotas Frequentes\",\n    \"‚ö†Ô∏è Desvios Detectados\",\n    \"üìà Padr√µes Temporais\"\n])\n\nwith tab1:\n    st.header(\"üó∫Ô∏è Mapa Interativo de Trajetos\")\n    \n    col1, col2 = st.columns([3, 1])\n    \n    with col1:\n        if not route_data.empty and len(route_data) > 0:\n            # Criar mapa Folium\n            center_lat = route_data['latitude'].mean()\n            center_lon = route_data['longitude'].mean()\n            \n            m = folium.Map(\n                location=[center_lat, center_lon],\n                zoom_start=12,\n                tiles='OpenStreetMap'\n            )\n            \n            # Adicionar pontos coloridos por velocidade\n            for idx, row in route_data.iterrows():\n                if pd.notna(row['latitude']) and pd.notna(row['longitude']):\n                    color = 'red' if row.get('velocidade_km', 0) > speed_threshold else 'green'\n                    popup_text = f\"\"\"\n                    Ve√≠culo: {row.get('placa', 'N/A')}<br>\n                    Velocidade: {row.get('velocidade_km', 0):.1f} km/h<br>\n                    Data: {row.get('data', 'N/A')}<br>\n                    Coordenadas: {row['latitude']:.6f}, {row['longitude']:.6f}\n                    \"\"\"\n                    \n                    folium.CircleMarker(\n                        location=[row['latitude'], row['longitude']],\n                        radius=3,\n                        popup=popup_text,\n                        color=color,\n                        fill=True,\n                        fillOpacity=0.7\n                    ).add_to(m)\n            \n            # Exibir mapa\n            map_data = st_folium(m, width=700, height=400)\n        else:\n            st.warning(\"N√£o h√° dados de coordenadas suficientes para exibir o mapa.\")\n    \n    with col2:\n        st.subheader(\"üìä Resumo da Rota\")\n        \n        if not route_data.empty:\n            total_points = len(route_data)\n            speed_violations = len(route_data[route_data['velocidade_km'] > speed_threshold])\n            avg_speed = route_data['velocidade_km'].mean()\n            max_speed = route_data['velocidade_km'].max()\n            \n            st.metric(\"Total de Pontos\", f\"{total_points:,}\")\n            st.metric(\"Velocidade M√©dia\", f\"{avg_speed:.1f} km/h\")\n            st.metric(\"Velocidade M√°xima\", f\"{max_speed:.1f} km/h\")\n            st.metric(\"Picos de Velocidade\", f\"{speed_violations:,}\", \n                     delta=f\"{(speed_violations/total_points*100):.1f}% do total\")\n\nwith tab2:\n    st.header(\"üìä An√°lise de Velocidade\")\n    \n    if not route_data.empty and 'velocidade_km' in route_data.columns:\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Histograma de velocidades\n            fig_hist = px.histogram(\n                route_data, \n                x='velocidade_km',\n                nbins=30,\n                title=\"Distribui√ß√£o de Velocidades\",\n                labels={'velocidade_km': 'Velocidade (km/h)', 'count': 'Frequ√™ncia'}\n            )\n            fig_hist.add_vline(x=speed_threshold, line_dash=\"dash\", line_color=\"red\", \n                              annotation_text=f\"Limite: {speed_threshold} km/h\")\n            st.plotly_chart(fig_hist, use_container_width=True)\n        \n        with col2:\n            # Velocidade ao longo do tempo\n            if 'data' in route_data.columns:\n                route_data_sorted = route_data.sort_values('data')\n                fig_time = px.line(\n                    route_data_sorted,\n                    x='data',\n                    y='velocidade_km',\n                    title=\"Velocidade ao Longo do Tempo\",\n                    labels={'data': 'Data/Hora', 'velocidade_km': 'Velocidade (km/h)'}\n                )\n                fig_time.add_hline(y=speed_threshold, line_dash=\"dash\", line_color=\"red\")\n                st.plotly_chart(fig_time, use_container_width=True)\n        \n        # Tabela de picos de velocidade\n        speed_violations_df = route_data[route_data['velocidade_km'] > speed_threshold]\n        if not speed_violations_df.empty:\n            st.subheader(\"‚ö†Ô∏è Picos de Velocidade Detectados\")\n            speed_violations_display = speed_violations_df[['data', 'placa', 'velocidade_km', 'latitude', 'longitude']].copy()\n            speed_violations_display = speed_violations_display.sort_values('velocidade_km', ascending=False)\n            st.dataframe(speed_violations_display.head(20), use_container_width=True)\n\nwith tab3:\n    st.header(\"üõ£Ô∏è An√°lise de Rotas Frequentes\")\n    \n    if not route_data.empty:\n        # Agrupar pontos pr√≥ximos para identificar rotas frequentes\n        st.subheader(\"üìç Pontos Mais Visitados\")\n        \n        # Criar grid de coordenadas para agrupar pontos pr√≥ximos\n        precision = 0.01  # Precis√£o do grid (aproximadamente 1km)\n        route_data['lat_grid'] = (route_data['latitude'] / precision).round() * precision\n        route_data['lon_grid'] = (route_data['longitude'] / precision).round() * precision\n        \n        # Contar frequ√™ncia de cada ponto\n        point_frequency = route_data.groupby(['lat_grid', 'lon_grid']).size().reset_index(name='frequencia')\n        point_frequency = point_frequency.sort_values('frequencia', ascending=False)\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Top 10 pontos mais frequentes\n            top_points = point_frequency.head(10)\n            fig_freq = px.bar(\n                top_points,\n                x='frequencia',\n                y=range(len(top_points)),\n                orientation='h',\n                title=\"Top 10 Locais Mais Visitados\",\n                labels={'frequencia': 'N√∫mero de Visitas', 'y': 'Ranking'}\n            )\n            st.plotly_chart(fig_freq, use_container_width=True)\n        \n        with col2:\n            # An√°lise por hor√°rio\n            if 'data' in route_data.columns:\n                route_data['hora'] = route_data['data'].dt.hour\n                hourly_activity = route_data.groupby('hora').size().reset_index(name='atividade')\n                \n                fig_hourly = px.bar(\n                    hourly_activity,\n                    x='hora',\n                    y='atividade',\n                    title=\"Atividade por Hor√°rio do Dia\",\n                    labels={'hora': 'Hora do Dia', 'atividade': 'N√∫mero de Registros'}\n                )\n                st.plotly_chart(fig_hourly, use_container_width=True)\n\nwith tab4:\n    st.header(\"‚ö†Ô∏è An√°lise de Desvios de Rota\")\n    \n    if not route_data.empty:\n        st.subheader(\"üîç Detec√ß√£o de Desvios\")\n        \n        # Calcular centro geogr√°fico das rotas\n        center_lat = route_data['latitude'].mean()\n        center_lon = route_data['longitude'].mean()\n        \n        # Calcular dist√¢ncia do centro para cada ponto\n        def haversine_distance(lat1, lon1, lat2, lon2):\n            from math import radians, cos, sin, asin, sqrt\n            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n            dlon = lon2 - lon1\n            dlat = lat2 - lat1\n            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n            return 2 * asin(sqrt(a)) * 6371  # Raio da Terra em km\n        \n        route_data['distancia_centro'] = route_data.apply(\n            lambda row: haversine_distance(center_lat, center_lon, row['latitude'], row['longitude']),\n            axis=1\n        )\n        \n        # Identificar desvios (pontos muito distantes do centro)\n        desvios = route_data[route_data['distancia_centro'] > deviation_radius]\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.metric(\"Total de Desvios Detectados\", len(desvios))\n            st.metric(\"Raio de An√°lise\", f\"{deviation_radius} km\")\n            \n            if not desvios.empty:\n                st.subheader(\"üìã Lista de Desvios\")\n                desvios_display = desvios[['data', 'placa', 'distancia_centro', 'velocidade_km']].copy()\n                desvios_display['distancia_centro'] = desvios_display['distancia_centro'].round(2)\n                st.dataframe(desvios_display.head(10), use_container_width=True)\n        \n        with col2:\n            if not route_data.empty:\n                # Gr√°fico de dispers√£o das dist√¢ncias\n                fig_scatter = px.scatter(\n                    route_data,\n                    x='distancia_centro',\n                    y='velocidade_km',\n                    color='placa' if 'placa' in route_data.columns else None,\n                    title=\"Desvios vs Velocidade\",\n                    labels={'distancia_centro': 'Dist√¢ncia do Centro (km)', 'velocidade_km': 'Velocidade (km/h)'}\n                )\n                fig_scatter.add_vline(x=deviation_radius, line_dash=\"dash\", line_color=\"red\",\n                                    annotation_text=f\"Limite: {deviation_radius} km\")\n                st.plotly_chart(fig_scatter, use_container_width=True)\n\nwith tab5:\n    st.header(\"üìà Padr√µes Temporais de Movimento\")\n    \n    if not route_data.empty and 'data' in route_data.columns:\n        # An√°lise por dia da semana\n        route_data['dia_semana'] = route_data['data'].dt.day_name()\n        route_data['dia_semana_num'] = route_data['data'].dt.dayofweek\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # Atividade por dia da semana\n            daily_activity = route_data.groupby('dia_semana').size().reset_index(name='atividade')\n            # Ordenar por dia da semana\n            day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n            daily_activity['dia_semana'] = pd.Categorical(daily_activity['dia_semana'], categories=day_order, ordered=True)\n            daily_activity = daily_activity.sort_values('dia_semana')\n            \n            fig_daily = px.bar(\n                daily_activity,\n                x='dia_semana',\n                y='atividade',\n                title=\"Atividade por Dia da Semana\",\n                labels={'dia_semana': 'Dia da Semana', 'atividade': 'N√∫mero de Registros'}\n            )\n            st.plotly_chart(fig_daily, use_container_width=True)\n        \n        with col2:\n            # Heatmap de atividade por hora e dia\n            route_data['hora'] = route_data['data'].dt.hour\n            heatmap_data = route_data.groupby(['dia_semana_num', 'hora']).size().reset_index(name='atividade')\n            \n            # Criar pivot table para heatmap\n            pivot_data = heatmap_data.pivot(index='dia_semana_num', columns='hora', values='atividade').fillna(0)\n            \n            fig_heatmap = px.imshow(\n                pivot_data,\n                labels=dict(x=\"Hora do Dia\", y=\"Dia da Semana\", color=\"Atividade\"),\n                title=\"Padr√£o de Atividade (Heatmap)\",\n                aspect=\"auto\"\n            )\n            st.plotly_chart(fig_heatmap, use_container_width=True)\n        \n        # An√°lise de velocidade m√©dia por per√≠odo\n        st.subheader(\"üöó Velocidade M√©dia por Per√≠odo\")\n        route_data['periodo'] = route_data['hora'].apply(\n            lambda x: 'Madrugada (00-06)' if x < 6 \n            else 'Manh√£ (06-12)' if x < 12 \n            else 'Tarde (12-18)' if x < 18 \n            else 'Noite (18-24)'\n        )\n        \n        period_stats = route_data.groupby('periodo')['velocidade_km'].agg(['mean', 'max', 'count']).reset_index()\n        period_stats.columns = ['periodo', 'velocidade_media', 'velocidade_maxima', 'total_registros']\n        \n        fig_period = px.bar(\n            period_stats,\n            x='periodo',\n            y='velocidade_media',\n            title=\"Velocidade M√©dia por Per√≠odo do Dia\",\n            labels={'periodo': 'Per√≠odo', 'velocidade_media': 'Velocidade M√©dia (km/h)'}\n        )\n        st.plotly_chart(fig_period, use_container_width=True)\n        \n        # Tabela resumo\n        st.subheader(\"üìä Resumo Estat√≠stico por Per√≠odo\")\n        st.dataframe(period_stats, use_container_width=True)\n\n# Footer com informa√ß√µes t√©cnicas\nst.markdown(\"---\")\nst.markdown(\"\"\"\n<div style='text-align: center; color: gray; font-size: 0.8em;'>\n    <p>üó∫Ô∏è Mapa de Rotas - Sistema de An√°lise de Trajetos</p>\n    <p>Dados processados: {total_records:,} registros | √öltima atualiza√ß√£o: {timestamp}</p>\n</div>\n\"\"\".format(\n    total_records=len(route_data),\n    timestamp=datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")\n), unsafe_allow_html=True)","size_bytes":16674}},"version":1}